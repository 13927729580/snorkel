{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro. to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: Training our End Extraction Model\n",
    "\n",
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last tutorial part to train our end extraction model.\n",
    "\n",
    "For this tutorial, we will be training a simple - but fairly effective - logistic regression model.  More generally, however, Snorkel plugs in with many ML libraries including [TensorFlow](https://www.tensorflow.org/), making it easy to use almost any state-of-the-art model as the end extractor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass, and load the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 Training a `SparseLogReg` Discriminative Model\n",
    "We use the training marginals to train a discriminative model that classifies each `Candidate` as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Instead of using a deep learning approach to start, let's look at a standard sparse logistic regression model. First, we need to extract out features. This can take a while, but we only have to do it once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n",
      "CPU times: user 10min 54s, sys: 3.19 s, total: 10min 57s\n",
      "Wall time: 11min\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n",
      "CPU times: user 22 s, sys: 253 ms, total: 22.3 s\n",
      "Wall time: 22.4 s\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n",
      "CPU times: user 36.4 s, sys: 435 ms, total: 36.9 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev = featurizer.load_matrix(session, split=1)\n",
    "F_test = featurizer.load_matrix(session, split=2)\n",
    "\n",
    "if F_train.size == 0:    \n",
    "    %time F_train = featurizer.apply(split=0)\n",
    "if F_dev.size == 0:     \n",
    "    %time F_dev  = featurizer.apply_existing(split=1)\n",
    "if F_test == 0:\n",
    "    %time F_test = featurizer.apply_existing(split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, reload the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, F_train, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs model selection by tuning our learning algorithm's hyperparamters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 20. Search space size = 125.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-02, l1_penalty = 1.00e-03, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.001 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.31s)\tAvg. loss=0.825656\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.26s)\tAvg. loss=0.562187\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.08s)\tAvg. loss=0.560432\tNNZ=136398\n",
      "[SparseLR] Training done (8.08s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_0\n",
      "============================================================\n",
      "[2] Testing lr = 1.00e-04, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.31s)\tAvg. loss=0.784314\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.16s)\tAvg. loss=0.708896\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (7.89s)\tAvg. loss=0.663600\tNNZ=136398\n",
      "[SparseLR] Training done (7.89s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_1\n",
      "============================================================\n",
      "[3] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.31s)\tAvg. loss=0.805405\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.08s)\tAvg. loss=0.566027\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (7.98s)\tAvg. loss=0.554212\tNNZ=136398\n",
      "[SparseLR] Training done (7.98s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_2\n",
      "============================================================\n",
      "[4] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.35s)\tAvg. loss=0.732849\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.30s)\tAvg. loss=0.561326\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.17s)\tAvg. loss=0.552531\tNNZ=136398\n",
      "[SparseLR] Training done (8.17s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_3\n",
      "============================================================\n",
      "[5] Testing lr = 1.00e-02, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.34s)\tAvg. loss=0.754657\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.59s)\tAvg. loss=0.552806\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.39s)\tAvg. loss=0.554132\tNNZ=136398\n",
      "[SparseLR] Training done (8.39s)\n",
      "============================================================\n",
      "[6] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.42s)\tAvg. loss=0.926739\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.86s)\tAvg. loss=0.925460\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (9.15s)\tAvg. loss=0.924241\tNNZ=136398\n",
      "[SparseLR] Training done (9.15s)\n",
      "============================================================\n",
      "[7] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.36s)\tAvg. loss=0.968414\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.37s)\tAvg. loss=0.967148\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.08s)\tAvg. loss=0.965941\tNNZ=136398\n",
      "[SparseLR] Training done (8.08s)\n",
      "============================================================\n",
      "[8] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.36s)\tAvg. loss=0.796288\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.23s)\tAvg. loss=0.556565\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (7.85s)\tAvg. loss=0.556387\tNNZ=136398\n",
      "[SparseLR] Training done (7.85s)\n",
      "============================================================\n",
      "[9] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.34s)\tAvg. loss=1.866872\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.10s)\tAvg. loss=1.673380\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (7.94s)\tAvg. loss=1.527168\tNNZ=136398\n",
      "[SparseLR] Training done (7.94s)\n",
      "============================================================\n",
      "[10] Testing lr = 1.00e-06, l1_penalty = 1.00e-05, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=1e-05 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.43s)\tAvg. loss=0.768110\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.65s)\tAvg. loss=0.767163\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.56s)\tAvg. loss=0.766261\tNNZ=136398\n",
      "[SparseLR] Training done (8.56s)\n",
      "============================================================\n",
      "[11] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.42s)\tAvg. loss=1.928707\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.61s)\tAvg. loss=1.715411\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.64s)\tAvg. loss=1.557905\tNNZ=136398\n",
      "[SparseLR] Training done (8.64s)\n",
      "============================================================\n",
      "[12] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.41s)\tAvg. loss=0.755256\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.51s)\tAvg. loss=0.564411\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.54s)\tAvg. loss=0.553547\tNNZ=136398\n",
      "[SparseLR] Training done (8.54s)\n",
      "============================================================\n",
      "[13] Testing lr = 1.00e-04, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.45s)\tAvg. loss=0.862352\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (5.28s)\tAvg. loss=0.765429\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (10.08s)\tAvg. loss=0.709212\tNNZ=136398\n",
      "[SparseLR] Training done (10.08s)\n",
      "============================================================\n",
      "[14] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.47s)\tAvg. loss=0.732708\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.72s)\tAvg. loss=0.561492\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.43s)\tAvg. loss=0.552571\tNNZ=136398\n",
      "[SparseLR] Training done (8.43s)\n",
      "============================================================\n",
      "[15] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.42s)\tAvg. loss=1.836546\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.35s)\tAvg. loss=1.656250\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.07s)\tAvg. loss=1.515510\tNNZ=136398\n",
      "[SparseLR] Training done (8.07s)\n",
      "============================================================\n",
      "[16] Testing lr = 1.00e-05, l1_penalty = 1.00e-06, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=1e-06 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.44s)\tAvg. loss=0.759374\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.65s)\tAvg. loss=0.751609\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (9.15s)\tAvg. loss=0.744552\tNNZ=136398\n",
      "[SparseLR] Training done (9.15s)\n",
      "============================================================\n",
      "[17] Testing lr = 1.00e-06, l1_penalty = 1.00e-02, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.01 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.52s)\tAvg. loss=1.887773\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (5.05s)\tAvg. loss=1.885597\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (9.16s)\tAvg. loss=1.883517\tNNZ=136398\n",
      "[SparseLR] Training done (9.16s)\n",
      "============================================================\n",
      "[18] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.49s)\tAvg. loss=0.731655\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.39s)\tAvg. loss=0.553791\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (8.11s)\tAvg. loss=0.555701\tNNZ=136398\n",
      "[SparseLR] Training done (8.11s)\n",
      "============================================================\n",
      "[19] Testing lr = 1.00e-05, l1_penalty = 1.00e-02, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=0.01 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.45s)\tAvg. loss=1.875243\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.17s)\tAvg. loss=1.854061\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (7.72s)\tAvg. loss=1.834247\tNNZ=136398\n",
      "[SparseLR] Training done (7.72s)\n",
      "============================================================\n",
      "[20] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=392  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (0.49s)\tAvg. loss=0.717133\tNNZ=136398\n",
      "[SparseLR] Epoch 25 (4.58s)\tAvg. loss=0.551223\tNNZ=136398\n",
      "[SparseLR] Epoch 49 (9.29s)\tAvg. loss=0.554026\tNNZ=136398\n",
      "[SparseLR] Training done (9.29s)\n",
      "INFO:tensorflow:Restoring parameters from ./SparseLR_3-0\n",
      "[SparseLR] Loaded model <SparseLR_3>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>l1_penalty</th>\n",
       "      <th>l2_penalty</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.078740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.029851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.025316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lr  l1_penalty  l2_penalty     Prec.      Rec.        F1\n",
       "3   0.001000    0.000001    0.001000  0.400000  0.285714  0.333333\n",
       "4   0.010000    0.000100    0.000010  0.400000  0.285714  0.333333\n",
       "19  0.010000    0.000010    0.000100  0.500000  0.142857  0.222222\n",
       "2   0.001000    0.000010    0.000010  0.166667  0.285714  0.210526\n",
       "17  0.010000    0.000010    0.000001  0.250000  0.142857  0.181818\n",
       "7   0.010000    0.000010    0.010000  0.200000  0.142857  0.166667\n",
       "5   0.000001    0.001000    0.000010  0.048276  1.000000  0.092105\n",
       "12  0.000100    0.000100    0.000010  0.045752  1.000000  0.087500\n",
       "6   0.000001    0.001000    0.010000  0.043796  0.857143  0.083333\n",
       "16  0.000001    0.010000    0.001000  0.041667  0.714286  0.078740\n",
       "9   0.000001    0.000010    0.001000  0.032609  0.428571  0.060606\n",
       "15  0.000010    0.000001    0.000100  0.027778  0.428571  0.052174\n",
       "14  0.000100    0.010000    0.000010  0.023810  0.142857  0.040816\n",
       "8   0.000100    0.010000    0.000001  0.018182  0.142857  0.032258\n",
       "18  0.000010    0.010000    0.000001  0.016667  0.142857  0.029851\n",
       "1   0.000100    0.000001    0.001000  0.013889  0.142857  0.025316\n",
       "10  0.000100    0.010000    0.000100  0.010417  0.142857  0.019417\n",
       "13  0.001000    0.000010    0.000100  0.000000  0.000000  0.000000\n",
       "11  0.001000    0.000001    0.001000  0.000000  0.000000  0.000000\n",
       "0   0.010000    0.001000    0.000100  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Features\n",
    "Extracting features allows us to inspect and interperet our learned weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[\\ nmontano replace]                         Weight: -0.398191\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[as if]                                          Weight: -0.395683\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[predict]                                        Weight: -0.392996\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[visit at the]                                   Weight: -0.388405\n",
      "Feature: TDL_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[nsubjpass|Bobby]      Weight: -0.380656\n"
     ]
    }
   ],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = reversed(np.argsort(np.abs(w))[-5:])\n",
    "for i in largest_idxs:\n",
    "    print('Feature: {0: <70}Weight: {1:.6f}'.format(F_train.get_key(session, i).name, w[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 0.971\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 8 | TN: 264 | FN: 6\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Training an LSTM Discriminative Model\n",
    "Deep learning allows us to train models without manually definining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\t4781 candidates\n",
      "Dev set:\t228 candidates\n",
      "Test set:\t278 candidates\n"
     ]
    }
   ],
   "source": [
    "train = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "\n",
    "print('Training set:\\t{0} candidates'.format(len(train)))\n",
    "print('Dev set:\\t{0} candidates'.format(len(dev)))\n",
    "print('Test set:\\t{0} candidates'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dev labels and convert to [0, 1] range\n",
    "dev_labels = (np.ravel(L_gold_dev.todense()) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reRNN] Dimension=100  LR=0.01\n",
      "[reRNN] Begin preprocessing\n",
      "[reRNN] Loaded 228 candidates for evaluation\n",
      "[reRNN] Preprocessing done (15.75s)\n",
      "[reRNN] Training model\n",
      "[reRNN] #examples=784  #epochs=50  batch size=256\n",
      "[reRNN] Epoch 0 (11.13s)\tAverage loss=0.680978\tDev F1=0.00\n",
      "[reRNN] Epoch 5 (56.46s)\tAverage loss=0.505550\tDev F1=0.00\n",
      "[reRNN] Epoch 10 (100.32s)\tAverage loss=0.496826\tDev F1=0.00\n",
      "[reRNN] Epoch 15 (138.23s)\tAverage loss=0.495360\tDev F1=0.00\n",
      "[reRNN] Epoch 20 (175.94s)\tAverage loss=0.495219\tDev F1=0.00\n",
      "[reRNN] Epoch 25 (215.71s)\tAverage loss=0.494242\tDev F1=0.00\n",
      "[reRNN] Epoch 30 (261.43s)\tAverage loss=0.494273\tDev F1=0.00\n",
      "[reRNN] Epoch 35 (309.35s)\tAverage loss=0.493773\tDev F1=0.00\n",
      "[reRNN] Epoch 40 (352.63s)\tAverage loss=0.493954\tDev F1=0.00\n",
      "[reRNN] Epoch 45 (396.19s)\tAverage loss=0.493807\tDev F1=0.00\n",
      "[reRNN] Epoch 49 (434.40s)\tAverage loss=0.493831\tDev F1=0.00\n",
      "[reRNN] Training done (435.17s)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train, train_marginals, dev_candidates=dev, dev_labels=dev_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Evaluating on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (`split` 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 0.963\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 10 | TN: 262 | FN: 6\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _  = lstm.score(session, test, L_gold_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
