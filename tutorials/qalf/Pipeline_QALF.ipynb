{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'project': 'qalf',\n",
    "    'domain': 'tacred',\n",
    "    'relation': 'org_top_members_employees',\n",
    "    'splits': [0,1],\n",
    "    'supervision': 'generative',\n",
    "#     'max_train': 2000,\n",
    "#     'max_lfs': 1,\n",
    "#     'learn_deps': True,\n",
    "    'gen_model_search_space': 10,\n",
    "    'gen_init_params': {\n",
    "        'lf_propensity'         : True,\n",
    "        'class_prior'           : False,\n",
    "        'lf_class_propensity'   : False,\n",
    "        'seed'                  : False,\n",
    "    },\n",
    "#     'gen_params_default': {\n",
    "#         'step_size'     : 0.0001,\n",
    "#         'decay'         : 0.90,\n",
    "#         'reg_param'     : 0.50,\n",
    "#         'epochs'        : 5,\n",
    "#     },\n",
    "    'disc_params_range': {\n",
    "        'lr'        : [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "        'dim'       : [32, 64, 128],\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'l1_penalty': [0, 1e-5, 1e-4, 1e-3, 1e-2], \n",
    "        'l2_penalty': [0, 1e-5, 1e-4, 1e-3, 1e-2], \n",
    "    },\n",
    "    'disc_params_default': {\n",
    "#         'l1_penalty': 1.0,\n",
    "#         'l2_penalty': 1.0,\n",
    "#         'lr':         0.01,\n",
    "        'dim':        50,\n",
    "#         'n_epochs':   20,\n",
    "#         'dropout':    0.5,\n",
    "#         'rebalance':  False,\n",
    "#         'batch_size': 128,\n",
    "#         'max_sentence_length': 100,\n",
    "#         'print_freq': 1,\n",
    "    },\n",
    "    'disc_model_class': 'logreg',    \n",
    "    'disc_model_search_space': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$SNORKELDB = sqlite:///qalf_tacred_org_top_members_employees.db\n"
     ]
    }
   ],
   "source": [
    "# Get DB connection string and add to globals\n",
    "# NOTE: $SNORKELDB must be set before any snorkel imports\n",
    "import os\n",
    "\n",
    "default_db_name = (config['project'] + '_' + config['domain'] + \n",
    "                   ('_' + config['relation'] if config['relation'] else '') +\n",
    "                   ('_debug' if config.get('debug', False) else ''))\n",
    "DB_NAME = config.get('db_name', default_db_name)\n",
    "if 'postgres' in config and config['postgres']:\n",
    "    DB_TYPE = 'postgres'\n",
    "else:\n",
    "    DB_TYPE = 'sqlite'\n",
    "    DB_NAME += '.db'\n",
    "DB_ADDR = \"localhost:{0}\".format(config['db_port']) if 'db_port' in config else \"\"\n",
    "os.environ['SNORKELDB'] = '{0}://{1}/{2}'.format(DB_TYPE, DB_ADDR, DB_NAME)\n",
    "print(\"$SNORKELDB = {0}\".format(os.environ['SNORKELDB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting domain=None to domain=tacred\n",
      "Overwriting batch_size=128 to batch_size=64\n",
      "Overwriting n_epochs=20 to n_epochs=25\n",
      "Overwriting l2_penalty=1.0 to l2_penalty=0\n",
      "Overwriting l1_penalty=1.0 to l1_penalty=0\n",
      "Overwriting seed=123 to seed=False\n",
      "Overwriting disc_model_class=lstm to disc_model_class=logreg\n",
      "Overwriting project=babble to project=qalf\n",
      "Overwriting splits=[0, 1, 2] to splits=[0, 1]\n",
      "Using TacredQalfPipeline object.\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Resolve config conflicts (nb_config > local_config > global_config)\n",
    "from snorkel.contrib.pipelines import merge_configs, get_local_pipeline\n",
    "config = merge_configs(config)\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "candidate_class = candidate_subclass(config['candidate_name'], config['candidate_entities'])\n",
    "\n",
    "pipeline = get_local_pipeline(config['domain'], config['project'])\n",
    "pipe = pipeline(session, candidate_class, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0: 7824 Candidates\n",
      "Split 1: 2039 Candidates\n"
     ]
    }
   ],
   "source": [
    "for split in config['splits']:\n",
    "    num_cands = pipe.session.query(pipe.candidate_class).filter(\n",
    "        pipe.candidate_class.split == split).count()\n",
    "    print(\"Split {}: {} Candidates\".format(split, num_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time pipe.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time pipe.load_gold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time pipe.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Who is the president of ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>0</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>0.101866</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>650</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.736961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the head of ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.117459</td>\n",
       "      <td>0.110046</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>654</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.711643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the CEO of ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.113625</td>\n",
       "      <td>0.109407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>661</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.743532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is an executive at ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>3</td>\n",
       "      <td>0.163216</td>\n",
       "      <td>0.150179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>925</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is a manager at ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>4</td>\n",
       "      <td>0.126789</td>\n",
       "      <td>0.113625</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>645</td>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.650202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the president of ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>5</td>\n",
       "      <td>0.340619</td>\n",
       "      <td>0.332950</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>2485</td>\n",
       "      <td>0.932458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the head of ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>6</td>\n",
       "      <td>0.329499</td>\n",
       "      <td>0.324131</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>2399</td>\n",
       "      <td>0.930566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the CEO of ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>7</td>\n",
       "      <td>0.347904</td>\n",
       "      <td>0.340746</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>2550</td>\n",
       "      <td>0.936811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is an executive at ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>8</td>\n",
       "      <td>0.342791</td>\n",
       "      <td>0.336273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>2523</td>\n",
       "      <td>0.940716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is a manager at ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>9</td>\n",
       "      <td>0.334867</td>\n",
       "      <td>0.326943</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.930534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 j  Coverage  Overlaps  \\\n",
       "Who is the president of ENT:subj? (0.95 -> +1)   0  0.112730  0.101866   \n",
       "Who is the head of ENT:subj? (0.95 -> +1)        1  0.117459  0.110046   \n",
       "Who is the CEO of ENT:subj? (0.95 -> +1)         2  0.113625  0.109407   \n",
       "Who is an executive at ENT:subj? (0.95 -> +1)    3  0.163216  0.150179   \n",
       "Who is a manager at ENT:subj? (0.95 -> +1)       4  0.126789  0.113625   \n",
       "Who is the president of ENT:subj? (<0.10 -> -1)  5  0.340619  0.332950   \n",
       "Who is the head of ENT:subj? (<0.10 -> -1)       6  0.329499  0.324131   \n",
       "Who is the CEO of ENT:subj? (<0.10 -> -1)        7  0.347904  0.340746   \n",
       "Who is an executive at ENT:subj? (<0.10 -> -1)   8  0.342791  0.336273   \n",
       "Who is a manager at ENT:subj? (<0.10 -> -1)      9  0.334867  0.326943   \n",
       "\n",
       "                                                 Conflicts   TP   FP   FN  \\\n",
       "Who is the president of ENT:subj? (0.95 -> +1)    0.000383  650  232    0   \n",
       "Who is the head of ENT:subj? (0.95 -> +1)         0.000128  654  265    0   \n",
       "Who is the CEO of ENT:subj? (0.95 -> +1)          0.000000  661  228    0   \n",
       "Who is an executive at ENT:subj? (0.95 -> +1)     0.000000  925  352    0   \n",
       "Who is a manager at ENT:subj? (0.95 -> +1)        0.000256  645  347    0   \n",
       "Who is the president of ENT:subj? (<0.10 -> -1)   0.000256    0    0  180   \n",
       "Who is the head of ENT:subj? (<0.10 -> -1)        0.000383    0    0  179   \n",
       "Who is the CEO of ENT:subj? (<0.10 -> -1)         0.000383    0    0  172   \n",
       "Who is an executive at ENT:subj? (<0.10 -> -1)    0.000000    0    0  159   \n",
       "Who is a manager at ENT:subj? (<0.10 -> -1)       0.000383    0    0  182   \n",
       "\n",
       "                                                   TN  Empirical Acc.  \n",
       "Who is the president of ENT:subj? (0.95 -> +1)      0        0.736961  \n",
       "Who is the head of ENT:subj? (0.95 -> +1)           0        0.711643  \n",
       "Who is the CEO of ENT:subj? (0.95 -> +1)            0        0.743532  \n",
       "Who is an executive at ENT:subj? (0.95 -> +1)       0        0.724354  \n",
       "Who is a manager at ENT:subj? (0.95 -> +1)          0        0.650202  \n",
       "Who is the president of ENT:subj? (<0.10 -> -1)  2485        0.932458  \n",
       "Who is the head of ENT:subj? (<0.10 -> -1)       2399        0.930566  \n",
       "Who is the CEO of ENT:subj? (<0.10 -> -1)        2550        0.936811  \n",
       "Who is an executive at ENT:subj? (<0.10 -> -1)   2523        0.940716  \n",
       "Who is a manager at ENT:subj? (<0.10 -> -1)      2438        0.930534  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels, load_label_matrix\n",
    "L_train = load_label_matrix(pipe.session, split=0)\n",
    "L_gold_train = load_gold_labels(session, annotator_name='gold', split=0)\n",
    "L_train.lf_stats(session, labels=L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Who is the president of ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>0</td>\n",
       "      <td>0.170672</td>\n",
       "      <td>0.163806</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>266</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the head of ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.212359</td>\n",
       "      <td>0.203531</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>317</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the CEO of ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.202060</td>\n",
       "      <td>0.191270</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>306</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.742718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is an executive at ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>3</td>\n",
       "      <td>0.251594</td>\n",
       "      <td>0.234919</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>388</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is a manager at ENT:subj? (0.95 -&gt; +1)</th>\n",
       "      <td>4</td>\n",
       "      <td>0.190780</td>\n",
       "      <td>0.182442</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>285</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the president of ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>5</td>\n",
       "      <td>0.273173</td>\n",
       "      <td>0.269250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>515</td>\n",
       "      <td>0.924596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the head of ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>6</td>\n",
       "      <td>0.266307</td>\n",
       "      <td>0.261403</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>502</td>\n",
       "      <td>0.924494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is the CEO of ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>7</td>\n",
       "      <td>0.280039</td>\n",
       "      <td>0.276116</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>526</td>\n",
       "      <td>0.921191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is an executive at ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>8</td>\n",
       "      <td>0.283472</td>\n",
       "      <td>0.274154</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>534</td>\n",
       "      <td>0.923875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who is a manager at ENT:subj? (&lt;0.10 -&gt; -1)</th>\n",
       "      <td>9</td>\n",
       "      <td>0.271211</td>\n",
       "      <td>0.261403</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>506</td>\n",
       "      <td>0.915009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 j  Coverage  Overlaps  \\\n",
       "Who is the president of ENT:subj? (0.95 -> +1)   0  0.170672  0.163806   \n",
       "Who is the head of ENT:subj? (0.95 -> +1)        1  0.212359  0.203531   \n",
       "Who is the CEO of ENT:subj? (0.95 -> +1)         2  0.202060  0.191270   \n",
       "Who is an executive at ENT:subj? (0.95 -> +1)    3  0.251594  0.234919   \n",
       "Who is a manager at ENT:subj? (0.95 -> +1)       4  0.190780  0.182442   \n",
       "Who is the president of ENT:subj? (<0.10 -> -1)  5  0.273173  0.269250   \n",
       "Who is the head of ENT:subj? (<0.10 -> -1)       6  0.266307  0.261403   \n",
       "Who is the CEO of ENT:subj? (<0.10 -> -1)        7  0.280039  0.276116   \n",
       "Who is an executive at ENT:subj? (<0.10 -> -1)   8  0.283472  0.274154   \n",
       "Who is a manager at ENT:subj? (<0.10 -> -1)      9  0.271211  0.261403   \n",
       "\n",
       "                                                 Conflicts   TP   FP  FN   TN  \\\n",
       "Who is the president of ENT:subj? (0.95 -> +1)     0.00000  266   82   0    0   \n",
       "Who is the head of ENT:subj? (0.95 -> +1)          0.00000  317  116   0    0   \n",
       "Who is the CEO of ENT:subj? (0.95 -> +1)           0.00049  306  106   0    0   \n",
       "Who is an executive at ENT:subj? (0.95 -> +1)      0.00000  388  125   0    0   \n",
       "Who is a manager at ENT:subj? (0.95 -> +1)         0.00049  285  104   0    0   \n",
       "Who is the president of ENT:subj? (<0.10 -> -1)    0.00000    0    0  42  515   \n",
       "Who is the head of ENT:subj? (<0.10 -> -1)         0.00049    0    0  41  502   \n",
       "Who is the CEO of ENT:subj? (<0.10 -> -1)          0.00049    0    0  45  526   \n",
       "Who is an executive at ENT:subj? (<0.10 -> -1)     0.00000    0    0  44  534   \n",
       "Who is a manager at ENT:subj? (<0.10 -> -1)        0.00000    0    0  47  506   \n",
       "\n",
       "                                                 Empirical Acc.  \n",
       "Who is the president of ENT:subj? (0.95 -> +1)         0.764368  \n",
       "Who is the head of ENT:subj? (0.95 -> +1)              0.732102  \n",
       "Who is the CEO of ENT:subj? (0.95 -> +1)               0.742718  \n",
       "Who is an executive at ENT:subj? (0.95 -> +1)          0.756335  \n",
       "Who is a manager at ENT:subj? (0.95 -> +1)             0.732648  \n",
       "Who is the president of ENT:subj? (<0.10 -> -1)        0.924596  \n",
       "Who is the head of ENT:subj? (<0.10 -> -1)             0.924494  \n",
       "Who is the CEO of ENT:subj? (<0.10 -> -1)              0.921191  \n",
       "Who is an executive at ENT:subj? (<0.10 -> -1)         0.923875  \n",
       "Who is a manager at ENT:subj? (<0.10 -> -1)            0.915009  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels, load_label_matrix\n",
    "L_dev = load_label_matrix(pipe.session, split=1)\n",
    "L_gold_dev   = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_dev.lf_stats(session, labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L_train: <7824x10 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 18226 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_train: <7824x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 7824 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 27.3%\n",
      "\n",
      "Using L_dev: <2039x10 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 4897 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_dev: <2039x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 2039 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 31.1%\n",
      "\n",
      "============================================================\n",
      "[1] Testing epochs = 25, step_size = 1.00e-02, reg_param = 1.00e-02, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.0\n",
      "[GenerativeModel] Model saved as <GenerativeModel_0>.\n",
      "============================================================\n",
      "[2] Testing epochs = 50, step_size = 1.00e-04, reg_param = 0.00e+00, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "[GenerativeModel] Model saved as <GenerativeModel_1>.\n",
      "============================================================\n",
      "[3] Testing epochs = 25, step_size = 1.00e-03, reg_param = 1.00e-02, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.0\n",
      "============================================================\n",
      "[4] Testing epochs = 50, step_size = 1.00e-04, reg_param = 5.00e-01, decay = 9.90e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "============================================================\n",
      "[5] Testing epochs = 50, step_size = 1.00e-05, reg_param = 2.50e-01, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "============================================================\n",
      "[6] Testing epochs = 75, step_size = 1.00e-05, reg_param = 0.00e+00, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "============================================================\n",
      "[7] Testing epochs = 25, step_size = 1.00e-04, reg_param = 2.50e-01, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "============================================================\n",
      "[8] Testing epochs = 75, step_size = 1.00e-02, reg_param = 0.00e+00, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "============================================================\n",
      "[9] Testing epochs = 25, step_size = 1.00e-03, reg_param = 1.00e-01, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "============================================================\n",
      "[10] Testing epochs = 25, step_size = 1.00e-03, reg_param = 2.50e-01, decay = 9.90e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1.0 Score: 0.686495176849\n",
      "[GenerativeModel] Model <GenerativeModel_1> loaded.\n",
      "   epochs  step_size  reg_param  decay     Prec.      Rec.     F-1.0\n",
      "1      50    0.00010       0.00   0.95  0.701149  0.672441  0.686495\n",
      "3      50    0.00010       0.50   0.99  0.701149  0.672441  0.686495\n",
      "4      50    0.00001       0.25   0.90  0.701149  0.672441  0.686495\n",
      "5      75    0.00001       0.00   0.95  0.701149  0.672441  0.686495\n",
      "6      25    0.00010       0.25   0.95  0.701149  0.672441  0.686495\n",
      "7      75    0.01000       0.00   0.95  0.701149  0.672441  0.686495\n",
      "8      25    0.00100       0.10   0.90  0.701149  0.672441  0.686495\n",
      "9      25    0.00100       0.25   0.99  0.701149  0.672441  0.686495\n",
      "0      25    0.01000       0.01   0.90  0.000000  0.000000  0.000000\n",
      "2      25    0.00100       0.01   0.95  0.000000  0.000000  0.000000\n",
      "[GenerativeModel] Model saved as <generative_tacred>.\n",
      "\n",
      "Gen. model (DP) score on dev set (b=0.5):\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.672\n",
      "Neg. class accuracy: 0.87\n",
      "Precision            0.701\n",
      "Recall               0.672\n",
      "F1                   0.686\n",
      "----------------------------------------\n",
      "TP: 427 | FP: 182 | TN: 1222 | FN: 208\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVOReB+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ac91850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7824 marginals\n",
      "CPU times: user 2min 37s, sys: 945 ms, total: 2min 38s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.supervise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.672\n",
      "Neg. class accuracy: 0.87\n",
      "Precision            0.701\n",
      "Recall               0.672\n",
      "F1                   0.686\n",
      "----------------------------------------\n",
      "TP: 427 | FP: 182 | TN: 1222 | FN: 208\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = pipe.gen_model.error_analysis(\n",
    "    pipe.session, L_dev, L_gold_dev, b=0.5, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7824,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVOReB+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102e20250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [7.1] Begin training discriminative model\n",
      "============================================================\n",
      "[1] Testing dim = 32, l2_penalty = 1.00e-02, lr = 1.00e-01, l1_penalty = 1.00e-05, batch_size = 32\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=32\n",
      "[SparseLogi...] Epoch 0 (0.29s) \tTrain Loss=7.579\tDev Loss=17.420\tDev F1=64.42\n",
      "[SparseLogi...] Epoch 1 (0.71s) \tTrain Loss=23.093\tDev Loss=29.771\tDev F1=63.31\n",
      "[SparseLogi...] Epoch 2 (1.11s) \tTrain Loss=34.310\tDev Loss=41.294\tDev F1=62.43\n",
      "[SparseLogi...] Epoch 3 (1.53s) \tTrain Loss=45.588\tDev Loss=49.840\tDev F1=63.03\n",
      "[SparseLogi...] Epoch 4 (1.95s) \tTrain Loss=52.497\tDev Loss=57.482\tDev F1=62.43\n",
      "[SparseLogi...] Epoch 5 (2.35s) \tTrain Loss=60.592\tDev Loss=64.858\tDev F1=63.78\n",
      "[SparseLogi...] Epoch 6 (2.75s) \tTrain Loss=68.904\tDev Loss=73.373\tDev F1=63.85\n",
      "[SparseLogi...] Epoch 7 (3.17s) \tTrain Loss=77.751\tDev Loss=82.397\tDev F1=60.71\n",
      "[SparseLogi...] Epoch 8 (3.59s) \tTrain Loss=85.375\tDev Loss=90.210\tDev F1=62.72\n",
      "[SparseLogi...] Epoch 9 (4.00s) \tTrain Loss=93.739\tDev Loss=98.994\tDev F1=63.23\n",
      "[SparseLogi...] Epoch 10 (4.41s) \tTrain Loss=101.435\tDev Loss=107.503\tDev F1=63.63\n",
      "[SparseLogi...] Epoch 11 (4.81s) \tTrain Loss=110.356\tDev Loss=112.144\tDev F1=62.01\n",
      "[SparseLogi...] Epoch 12 (5.22s) \tTrain Loss=113.810\tDev Loss=117.769\tDev F1=65.11\n",
      "[SparseLogi...] Epoch 13 (5.63s) \tTrain Loss=121.421\tDev Loss=127.077\tDev F1=63.04\n",
      "[SparseLogi...] Epoch 14 (6.05s) \tTrain Loss=128.572\tDev Loss=132.027\tDev F1=63.10\n",
      "[SparseLogi...] Epoch 15 (6.46s) \tTrain Loss=133.285\tDev Loss=138.317\tDev F1=64.97\n",
      "[SparseLogi...] Epoch 16 (6.90s) \tTrain Loss=140.927\tDev Loss=144.782\tDev F1=63.88\n",
      "[SparseLogi...] Epoch 17 (7.30s) \tTrain Loss=145.235\tDev Loss=148.811\tDev F1=62.40\n",
      "[SparseLogi...] Epoch 18 (7.71s) \tTrain Loss=150.118\tDev Loss=152.835\tDev F1=62.39\n",
      "[SparseLogi...] Epoch 19 (8.24s) \tTrain Loss=153.190\tDev Loss=155.385\tDev F1=63.57\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (8.96s) \tTrain Loss=159.126\tDev Loss=164.315\tDev F1=63.17\n",
      "[SparseLogi...] Epoch 21 (9.38s) \tTrain Loss=166.868\tDev Loss=169.865\tDev F1=64.18\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 22 (10.27s) \tTrain Loss=172.219\tDev Loss=176.669\tDev F1=61.52\n",
      "[SparseLogi...] Epoch 23 (10.69s) \tTrain Loss=176.874\tDev Loss=178.664\tDev F1=64.39\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 24 (11.46s) \tTrain Loss=180.821\tDev Loss=186.440\tDev F1=63.90\n",
      "[SparseLogisticRegression] Training done (11.58s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.643902439024\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "============================================================\n",
      "[2] Testing dim = 64, l2_penalty = 1.00e-04, lr = 1.00e-04, l1_penalty = 1.00e-03, batch_size = 128\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=128\n",
      "[SparseLogi...] Epoch 0 (0.16s) \tTrain Loss=1.084\tDev Loss=1.080\tDev F1=48.19\n",
      "[SparseLogi...] Epoch 1 (0.42s) \tTrain Loss=1.064\tDev Loss=1.066\tDev F1=49.08\n",
      "[SparseLogi...] Epoch 2 (0.68s) \tTrain Loss=1.048\tDev Loss=1.053\tDev F1=49.96\n",
      "[SparseLogi...] Epoch 3 (0.93s) \tTrain Loss=1.032\tDev Loss=1.041\tDev F1=50.45\n",
      "[SparseLogi...] Epoch 4 (1.19s) \tTrain Loss=1.017\tDev Loss=1.030\tDev F1=51.69\n",
      "[SparseLogi...] Epoch 5 (1.44s) \tTrain Loss=1.004\tDev Loss=1.021\tDev F1=52.29\n",
      "[SparseLogi...] Epoch 6 (1.70s) \tTrain Loss=0.992\tDev Loss=1.012\tDev F1=53.13\n",
      "[SparseLogi...] Epoch 7 (2.09s) \tTrain Loss=0.981\tDev Loss=1.004\tDev F1=54.06\n",
      "[SparseLogi...] Epoch 8 (2.35s) \tTrain Loss=0.969\tDev Loss=0.997\tDev F1=54.72\n",
      "[SparseLogi...] Epoch 9 (2.60s) \tTrain Loss=0.962\tDev Loss=0.991\tDev F1=55.02\n",
      "[SparseLogi...] Epoch 10 (2.87s) \tTrain Loss=0.953\tDev Loss=0.985\tDev F1=55.41\n",
      "[SparseLogi...] Epoch 11 (3.12s) \tTrain Loss=0.944\tDev Loss=0.980\tDev F1=55.60\n",
      "[SparseLogi...] Epoch 12 (3.39s) \tTrain Loss=0.937\tDev Loss=0.975\tDev F1=56.10\n",
      "[SparseLogi...] Epoch 13 (3.65s) \tTrain Loss=0.929\tDev Loss=0.971\tDev F1=56.10\n",
      "[SparseLogi...] Epoch 14 (3.91s) \tTrain Loss=0.922\tDev Loss=0.967\tDev F1=56.15\n",
      "[SparseLogi...] Epoch 15 (4.17s) \tTrain Loss=0.916\tDev Loss=0.963\tDev F1=56.35\n",
      "[SparseLogi...] Epoch 16 (4.55s) \tTrain Loss=0.911\tDev Loss=0.960\tDev F1=56.53\n",
      "[SparseLogi...] Epoch 17 (4.80s) \tTrain Loss=0.903\tDev Loss=0.957\tDev F1=56.74\n",
      "[SparseLogi...] Epoch 18 (5.07s) \tTrain Loss=0.900\tDev Loss=0.954\tDev F1=56.91\n",
      "[SparseLogi...] Epoch 19 (5.33s) \tTrain Loss=0.894\tDev Loss=0.951\tDev F1=57.04\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (5.87s) \tTrain Loss=0.890\tDev Loss=0.949\tDev F1=56.99\n",
      "[SparseLogi...] Epoch 21 (6.12s) \tTrain Loss=0.886\tDev Loss=0.947\tDev F1=56.99\n",
      "[SparseLogi...] Epoch 22 (6.50s) \tTrain Loss=0.880\tDev Loss=0.945\tDev F1=56.99\n",
      "[SparseLogi...] Epoch 23 (6.76s) \tTrain Loss=0.876\tDev Loss=0.943\tDev F1=57.12\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 24 (7.34s) \tTrain Loss=0.877\tDev Loss=0.942\tDev F1=57.70\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (7.90s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.576991150442\n",
      "============================================================\n",
      "[3] Testing dim = 32, l2_penalty = 1.00e-05, lr = 1.00e-02, l1_penalty = 1.00e-03, batch_size = 128\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=128\n",
      "[SparseLogi...] Epoch 0 (0.17s) \tTrain Loss=0.877\tDev Loss=1.012\tDev F1=65.51\n",
      "[SparseLogi...] Epoch 1 (0.42s) \tTrain Loss=0.853\tDev Loss=1.115\tDev F1=65.32\n",
      "[SparseLogi...] Epoch 2 (0.69s) \tTrain Loss=0.918\tDev Loss=1.217\tDev F1=64.98\n",
      "[SparseLogi...] Epoch 3 (1.08s) \tTrain Loss=0.997\tDev Loss=1.307\tDev F1=64.79\n",
      "[SparseLogi...] Epoch 4 (1.34s) \tTrain Loss=1.083\tDev Loss=1.411\tDev F1=64.44\n",
      "[SparseLogi...] Epoch 5 (1.60s) \tTrain Loss=1.170\tDev Loss=1.509\tDev F1=64.69\n",
      "[SparseLogi...] Epoch 6 (1.87s) \tTrain Loss=1.254\tDev Loss=1.591\tDev F1=64.97\n",
      "[SparseLogi...] Epoch 7 (2.15s) \tTrain Loss=1.330\tDev Loss=1.672\tDev F1=64.68\n",
      "[SparseLogi...] Epoch 8 (2.47s) \tTrain Loss=1.405\tDev Loss=1.764\tDev F1=64.75\n",
      "[SparseLogi...] Epoch 9 (2.77s) \tTrain Loss=1.481\tDev Loss=1.831\tDev F1=65.08\n",
      "[SparseLogi...] Epoch 10 (3.05s) \tTrain Loss=1.542\tDev Loss=1.901\tDev F1=64.88\n",
      "[SparseLogi...] Epoch 11 (3.33s) \tTrain Loss=1.603\tDev Loss=1.965\tDev F1=64.84\n",
      "[SparseLogi...] Epoch 12 (3.72s) \tTrain Loss=1.665\tDev Loss=2.026\tDev F1=64.84\n",
      "[SparseLogi...] Epoch 13 (3.98s) \tTrain Loss=1.725\tDev Loss=2.100\tDev F1=64.99\n",
      "[SparseLogi...] Epoch 14 (4.25s) \tTrain Loss=1.779\tDev Loss=2.161\tDev F1=64.45\n",
      "[SparseLogi...] Epoch 15 (4.57s) \tTrain Loss=1.830\tDev Loss=2.206\tDev F1=64.56\n",
      "[SparseLogi...] Epoch 16 (4.85s) \tTrain Loss=1.881\tDev Loss=2.280\tDev F1=64.90\n",
      "[SparseLogi...] Epoch 17 (5.11s) \tTrain Loss=1.932\tDev Loss=2.316\tDev F1=64.80\n",
      "[SparseLogi...] Epoch 18 (5.38s) \tTrain Loss=1.980\tDev Loss=2.364\tDev F1=64.64\n",
      "[SparseLogi...] Epoch 19 (5.65s) \tTrain Loss=2.029\tDev Loss=2.418\tDev F1=64.88\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (6.42s) \tTrain Loss=2.071\tDev Loss=2.460\tDev F1=64.54\n",
      "[SparseLogi...] Epoch 21 (6.72s) \tTrain Loss=2.118\tDev Loss=2.519\tDev F1=64.87\n",
      "[SparseLogi...] Epoch 22 (7.00s) \tTrain Loss=2.159\tDev Loss=2.566\tDev F1=64.82\n",
      "[SparseLogi...] Epoch 23 (7.27s) \tTrain Loss=2.197\tDev Loss=2.603\tDev F1=64.51\n",
      "[SparseLogi...] Epoch 24 (7.56s) \tTrain Loss=2.240\tDev Loss=2.644\tDev F1=64.79\n",
      "[SparseLogisticRegression] Training done (7.67s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.648849294729\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_2>\n",
      "============================================================\n",
      "[4] Testing dim = 64, l2_penalty = 0.00e+00, lr = 1.00e-03, l1_penalty = 1.00e-03, batch_size = 32\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=32\n",
      "[SparseLogi...] Epoch 0 (0.32s) \tTrain Loss=0.940\tDev Loss=0.916\tDev F1=59.13\n",
      "[SparseLogi...] Epoch 1 (0.72s) \tTrain Loss=0.815\tDev Loss=0.908\tDev F1=62.88\n",
      "[SparseLogi...] Epoch 2 (1.12s) \tTrain Loss=0.782\tDev Loss=0.924\tDev F1=65.93\n",
      "[SparseLogi...] Epoch 3 (1.52s) \tTrain Loss=0.771\tDev Loss=0.939\tDev F1=66.16\n",
      "[SparseLogi...] Epoch 4 (2.07s) \tTrain Loss=0.770\tDev Loss=0.958\tDev F1=65.55\n",
      "[SparseLogi...] Epoch 5 (2.47s) \tTrain Loss=0.774\tDev Loss=0.978\tDev F1=65.90\n",
      "[SparseLogi...] Epoch 6 (2.88s) \tTrain Loss=0.781\tDev Loss=0.993\tDev F1=65.90\n",
      "[SparseLogi...] Epoch 7 (3.27s) \tTrain Loss=0.789\tDev Loss=1.011\tDev F1=65.71\n",
      "[SparseLogi...] Epoch 8 (3.67s) \tTrain Loss=0.800\tDev Loss=1.030\tDev F1=65.90\n",
      "[SparseLogi...] Epoch 9 (4.21s) \tTrain Loss=0.811\tDev Loss=1.049\tDev F1=65.80\n",
      "[SparseLogi...] Epoch 10 (4.63s) \tTrain Loss=0.824\tDev Loss=1.070\tDev F1=65.90\n",
      "[SparseLogi...] Epoch 11 (5.07s) \tTrain Loss=0.838\tDev Loss=1.088\tDev F1=66.06\n",
      "[SparseLogi...] Epoch 12 (5.47s) \tTrain Loss=0.854\tDev Loss=1.107\tDev F1=65.55\n",
      "[SparseLogi...] Epoch 13 (6.01s) \tTrain Loss=0.871\tDev Loss=1.131\tDev F1=65.66\n",
      "[SparseLogi...] Epoch 14 (6.41s) \tTrain Loss=0.889\tDev Loss=1.156\tDev F1=65.46\n",
      "[SparseLogi...] Epoch 15 (6.82s) \tTrain Loss=0.907\tDev Loss=1.176\tDev F1=65.56\n",
      "[SparseLogi...] Epoch 16 (7.24s) \tTrain Loss=0.926\tDev Loss=1.205\tDev F1=65.52\n",
      "[SparseLogi...] Epoch 17 (7.63s) \tTrain Loss=0.946\tDev Loss=1.224\tDev F1=65.31\n",
      "[SparseLogi...] Epoch 18 (8.03s) \tTrain Loss=0.966\tDev Loss=1.248\tDev F1=65.41\n",
      "[SparseLogi...] Epoch 19 (8.44s) \tTrain Loss=0.988\tDev Loss=1.276\tDev F1=65.47\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (9.25s) \tTrain Loss=1.009\tDev Loss=1.298\tDev F1=65.47\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 21 (10.10s) \tTrain Loss=1.032\tDev Loss=1.326\tDev F1=65.18\n",
      "[SparseLogi...] Epoch 22 (10.50s) \tTrain Loss=1.054\tDev Loss=1.354\tDev F1=65.03\n",
      "[SparseLogi...] Epoch 23 (10.92s) \tTrain Loss=1.077\tDev Loss=1.378\tDev F1=64.79\n",
      "[SparseLogi...] Epoch 24 (11.36s) \tTrain Loss=1.100\tDev Loss=1.403\tDev F1=64.84\n",
      "[SparseLogisticRegression] Training done (11.47s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.654735272185\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_3>\n",
      "============================================================\n",
      "[5] Testing dim = 64, l2_penalty = 1.00e-05, lr = 1.00e-04, l1_penalty = 0.00e+00, batch_size = 128\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=128\n",
      "[SparseLogi...] Epoch 0 (0.18s) \tTrain Loss=0.646\tDev Loss=0.653\tDev F1=50.32\n",
      "[SparseLogi...] Epoch 1 (0.43s) \tTrain Loss=0.628\tDev Loss=0.639\tDev F1=51.65\n",
      "[SparseLogi...] Epoch 2 (0.69s) \tTrain Loss=0.611\tDev Loss=0.626\tDev F1=52.59\n",
      "[SparseLogi...] Epoch 3 (1.09s) \tTrain Loss=0.594\tDev Loss=0.615\tDev F1=53.55\n",
      "[SparseLogi...] Epoch 4 (1.35s) \tTrain Loss=0.581\tDev Loss=0.604\tDev F1=54.63\n",
      "[SparseLogi...] Epoch 5 (1.61s) \tTrain Loss=0.568\tDev Loss=0.595\tDev F1=55.79\n",
      "[SparseLogi...] Epoch 6 (1.88s) \tTrain Loss=0.557\tDev Loss=0.586\tDev F1=56.30\n",
      "[SparseLogi...] Epoch 7 (2.14s) \tTrain Loss=0.545\tDev Loss=0.578\tDev F1=56.83\n",
      "[SparseLogi...] Epoch 8 (2.40s) \tTrain Loss=0.534\tDev Loss=0.571\tDev F1=57.76\n",
      "[SparseLogi...] Epoch 9 (2.66s) \tTrain Loss=0.527\tDev Loss=0.565\tDev F1=58.11\n",
      "[SparseLogi...] Epoch 10 (2.92s) \tTrain Loss=0.517\tDev Loss=0.559\tDev F1=57.95\n",
      "[SparseLogi...] Epoch 11 (3.19s) \tTrain Loss=0.508\tDev Loss=0.554\tDev F1=58.15\n",
      "[SparseLogi...] Epoch 12 (3.45s) \tTrain Loss=0.501\tDev Loss=0.549\tDev F1=58.46\n",
      "[SparseLogi...] Epoch 13 (3.86s) \tTrain Loss=0.493\tDev Loss=0.544\tDev F1=58.72\n",
      "[SparseLogi...] Epoch 14 (4.15s) \tTrain Loss=0.486\tDev Loss=0.540\tDev F1=58.72\n",
      "[SparseLogi...] Epoch 15 (4.41s) \tTrain Loss=0.480\tDev Loss=0.536\tDev F1=58.56\n",
      "[SparseLogi...] Epoch 16 (4.68s) \tTrain Loss=0.475\tDev Loss=0.532\tDev F1=58.83\n",
      "[SparseLogi...] Epoch 17 (4.97s) \tTrain Loss=0.468\tDev Loss=0.529\tDev F1=59.23\n",
      "[SparseLogi...] Epoch 18 (5.26s) \tTrain Loss=0.463\tDev Loss=0.526\tDev F1=59.33\n",
      "[SparseLogi...] Epoch 19 (5.55s) \tTrain Loss=0.459\tDev Loss=0.524\tDev F1=59.26\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (6.31s) \tTrain Loss=0.454\tDev Loss=0.521\tDev F1=59.51\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 21 (6.94s) \tTrain Loss=0.449\tDev Loss=0.519\tDev F1=59.56\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 22 (7.74s) \tTrain Loss=0.444\tDev Loss=0.516\tDev F1=59.68\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 23 (8.57s) \tTrain Loss=0.439\tDev Loss=0.514\tDev F1=59.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 24 (9.30s) \tTrain Loss=0.440\tDev Loss=0.513\tDev F1=59.97\n",
      "[SparseLogisticRegression] Training done (9.42s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.59965034965\n",
      "============================================================\n",
      "[6] Testing dim = 128, l2_penalty = 1.00e-05, lr = 1.00e-04, l1_penalty = 1.00e-05, batch_size = 32\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=32\n",
      "[SparseLogi...] Epoch 0 (0.31s) \tTrain Loss=0.636\tDev Loss=0.622\tDev F1=5.35\n",
      "[SparseLogi...] Epoch 1 (0.77s) \tTrain Loss=0.592\tDev Loss=0.594\tDev F1=10.68\n",
      "[SparseLogi...] Epoch 2 (1.20s) \tTrain Loss=0.557\tDev Loss=0.573\tDev F1=36.59\n",
      "[SparseLogi...] Epoch 3 (1.68s) \tTrain Loss=0.528\tDev Loss=0.557\tDev F1=54.49\n",
      "[SparseLogi...] Epoch 4 (2.27s) \tTrain Loss=0.505\tDev Loss=0.544\tDev F1=56.48\n",
      "[SparseLogi...] Epoch 5 (2.74s) \tTrain Loss=0.485\tDev Loss=0.534\tDev F1=57.79\n",
      "[SparseLogi...] Epoch 6 (3.21s) \tTrain Loss=0.469\tDev Loss=0.526\tDev F1=58.23\n",
      "[SparseLogi...] Epoch 7 (3.68s) \tTrain Loss=0.454\tDev Loss=0.519\tDev F1=58.50\n",
      "[SparseLogi...] Epoch 8 (4.15s) \tTrain Loss=0.442\tDev Loss=0.514\tDev F1=58.39\n",
      "[SparseLogi...] Epoch 9 (4.60s) \tTrain Loss=0.431\tDev Loss=0.510\tDev F1=57.97\n",
      "[SparseLogi...] Epoch 10 (5.16s) \tTrain Loss=0.421\tDev Loss=0.507\tDev F1=58.29\n",
      "[SparseLogi...] Epoch 11 (5.60s) \tTrain Loss=0.412\tDev Loss=0.504\tDev F1=58.39\n",
      "[SparseLogi...] Epoch 12 (6.17s) \tTrain Loss=0.404\tDev Loss=0.502\tDev F1=58.58\n",
      "[SparseLogi...] Epoch 13 (6.62s) \tTrain Loss=0.397\tDev Loss=0.500\tDev F1=58.68\n",
      "[SparseLogi...] Epoch 14 (7.08s) \tTrain Loss=0.390\tDev Loss=0.499\tDev F1=58.65\n",
      "[SparseLogi...] Epoch 15 (7.51s) \tTrain Loss=0.384\tDev Loss=0.498\tDev F1=60.10\n",
      "[SparseLogi...] Epoch 16 (7.92s) \tTrain Loss=0.379\tDev Loss=0.498\tDev F1=62.24\n",
      "[SparseLogi...] Epoch 17 (8.32s) \tTrain Loss=0.374\tDev Loss=0.497\tDev F1=62.41\n",
      "[SparseLogi...] Epoch 18 (8.74s) \tTrain Loss=0.369\tDev Loss=0.497\tDev F1=62.64\n",
      "[SparseLogi...] Epoch 19 (9.15s) \tTrain Loss=0.365\tDev Loss=0.497\tDev F1=63.61\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (10.00s) \tTrain Loss=0.362\tDev Loss=0.498\tDev F1=64.23\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 21 (10.76s) \tTrain Loss=0.358\tDev Loss=0.498\tDev F1=64.18\n",
      "[SparseLogi...] Epoch 22 (11.19s) \tTrain Loss=0.355\tDev Loss=0.499\tDev F1=64.40\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 23 (12.11s) \tTrain Loss=0.352\tDev Loss=0.500\tDev F1=64.26\n",
      "[SparseLogi...] Epoch 24 (12.54s) \tTrain Loss=0.349\tDev Loss=0.500\tDev F1=65.03\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (13.19s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.650277557494\n",
      "============================================================\n",
      "[7] Testing dim = 32, l2_penalty = 0.00e+00, lr = 1.00e-01, l1_penalty = 1.00e-05, batch_size = 32\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=32\n",
      "[SparseLogi...] Epoch 0 (0.32s) \tTrain Loss=0.427\tDev Loss=0.799\tDev F1=64.36\n",
      "[SparseLogi...] Epoch 1 (0.73s) \tTrain Loss=0.386\tDev Loss=0.922\tDev F1=63.65\n",
      "[SparseLogi...] Epoch 2 (1.13s) \tTrain Loss=0.369\tDev Loss=0.980\tDev F1=61.94\n",
      "[SparseLogi...] Epoch 3 (1.52s) \tTrain Loss=0.362\tDev Loss=0.910\tDev F1=62.65\n",
      "[SparseLogi...] Epoch 4 (1.92s) \tTrain Loss=0.355\tDev Loss=0.967\tDev F1=62.69\n",
      "[SparseLogi...] Epoch 5 (2.32s) \tTrain Loss=0.367\tDev Loss=0.984\tDev F1=63.51\n",
      "[SparseLogi...] Epoch 6 (2.76s) \tTrain Loss=0.382\tDev Loss=0.991\tDev F1=63.85\n",
      "[SparseLogi...] Epoch 7 (3.29s) \tTrain Loss=0.366\tDev Loss=1.089\tDev F1=60.60\n",
      "[SparseLogi...] Epoch 8 (3.69s) \tTrain Loss=0.369\tDev Loss=1.201\tDev F1=62.69\n",
      "[SparseLogi...] Epoch 9 (4.08s) \tTrain Loss=0.389\tDev Loss=1.058\tDev F1=64.05\n",
      "[SparseLogi...] Epoch 10 (4.61s) \tTrain Loss=0.384\tDev Loss=1.242\tDev F1=63.40\n",
      "[SparseLogi...] Epoch 11 (5.02s) \tTrain Loss=0.382\tDev Loss=1.086\tDev F1=63.03\n",
      "[SparseLogi...] Epoch 12 (5.43s) \tTrain Loss=0.395\tDev Loss=1.192\tDev F1=64.22\n",
      "[SparseLogi...] Epoch 13 (5.85s) \tTrain Loss=0.411\tDev Loss=1.275\tDev F1=62.97\n",
      "[SparseLogi...] Epoch 14 (6.27s) \tTrain Loss=0.400\tDev Loss=1.332\tDev F1=63.48\n",
      "[SparseLogi...] Epoch 15 (6.69s) \tTrain Loss=0.414\tDev Loss=1.443\tDev F1=64.15\n",
      "[SparseLogi...] Epoch 16 (7.09s) \tTrain Loss=0.417\tDev Loss=1.270\tDev F1=63.97\n",
      "[SparseLogi...] Epoch 17 (7.48s) \tTrain Loss=0.410\tDev Loss=1.185\tDev F1=61.01\n",
      "[SparseLogi...] Epoch 18 (8.02s) \tTrain Loss=0.387\tDev Loss=1.181\tDev F1=61.98\n",
      "[SparseLogi...] Epoch 19 (8.43s) \tTrain Loss=0.384\tDev Loss=1.245\tDev F1=63.92\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (9.24s) \tTrain Loss=0.414\tDev Loss=1.326\tDev F1=62.62\n",
      "[SparseLogi...] Epoch 21 (9.63s) \tTrain Loss=0.403\tDev Loss=1.275\tDev F1=64.03\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 22 (10.59s) \tTrain Loss=0.408\tDev Loss=1.511\tDev F1=61.53\n",
      "[SparseLogi...] Epoch 23 (10.99s) \tTrain Loss=0.406\tDev Loss=1.276\tDev F1=62.27\n",
      "[SparseLogi...] Epoch 24 (11.57s) \tTrain Loss=0.407\tDev Loss=1.375\tDev F1=63.96\n",
      "[SparseLogisticRegression] Training done (11.70s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.640332640333\n",
      "============================================================\n",
      "[8] Testing dim = 128, l2_penalty = 1.00e-05, lr = 1.00e-03, l1_penalty = 1.00e-05, batch_size = 32\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=32\n",
      "[SparseLogi...] Epoch 0 (0.31s) \tTrain Loss=0.527\tDev Loss=0.515\tDev F1=60.36\n",
      "[SparseLogi...] Epoch 1 (0.75s) \tTrain Loss=0.404\tDev Loss=0.499\tDev F1=62.28\n",
      "[SparseLogi...] Epoch 2 (1.15s) \tTrain Loss=0.362\tDev Loss=0.503\tDev F1=65.85\n",
      "[SparseLogi...] Epoch 3 (1.56s) \tTrain Loss=0.341\tDev Loss=0.506\tDev F1=66.41\n",
      "[SparseLogi...] Epoch 4 (2.10s) \tTrain Loss=0.328\tDev Loss=0.512\tDev F1=66.06\n",
      "[SparseLogi...] Epoch 5 (2.51s) \tTrain Loss=0.320\tDev Loss=0.520\tDev F1=66.06\n",
      "[SparseLogi...] Epoch 6 (2.94s) \tTrain Loss=0.313\tDev Loss=0.520\tDev F1=65.95\n",
      "[SparseLogi...] Epoch 7 (3.37s) \tTrain Loss=0.308\tDev Loss=0.525\tDev F1=65.66\n",
      "[SparseLogi...] Epoch 8 (3.77s) \tTrain Loss=0.304\tDev Loss=0.529\tDev F1=65.61\n",
      "[SparseLogi...] Epoch 9 (4.22s) \tTrain Loss=0.300\tDev Loss=0.532\tDev F1=65.76\n",
      "[SparseLogi...] Epoch 10 (4.63s) \tTrain Loss=0.297\tDev Loss=0.536\tDev F1=65.67\n",
      "[SparseLogi...] Epoch 11 (5.03s) \tTrain Loss=0.294\tDev Loss=0.536\tDev F1=65.87\n",
      "[SparseLogi...] Epoch 12 (5.42s) \tTrain Loss=0.291\tDev Loss=0.537\tDev F1=65.22\n",
      "[SparseLogi...] Epoch 13 (5.84s) \tTrain Loss=0.289\tDev Loss=0.541\tDev F1=64.92\n",
      "[SparseLogi...] Epoch 14 (6.24s) \tTrain Loss=0.287\tDev Loss=0.546\tDev F1=65.07\n",
      "[SparseLogi...] Epoch 15 (6.63s) \tTrain Loss=0.285\tDev Loss=0.546\tDev F1=64.63\n",
      "[SparseLogi...] Epoch 16 (7.03s) \tTrain Loss=0.283\tDev Loss=0.553\tDev F1=64.98\n",
      "[SparseLogi...] Epoch 17 (7.55s) \tTrain Loss=0.282\tDev Loss=0.550\tDev F1=64.62\n",
      "[SparseLogi...] Epoch 18 (7.95s) \tTrain Loss=0.280\tDev Loss=0.552\tDev F1=64.67\n",
      "[SparseLogi...] Epoch 19 (8.34s) \tTrain Loss=0.279\tDev Loss=0.557\tDev F1=64.77\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (9.04s) \tTrain Loss=0.277\tDev Loss=0.556\tDev F1=64.88\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 21 (9.87s) \tTrain Loss=0.276\tDev Loss=0.560\tDev F1=64.57\n",
      "[SparseLogi...] Epoch 22 (10.28s) \tTrain Loss=0.275\tDev Loss=0.565\tDev F1=64.69\n",
      "[SparseLogi...] Epoch 23 (10.68s) \tTrain Loss=0.274\tDev Loss=0.565\tDev F1=64.59\n",
      "[SparseLogi...] Epoch 24 (11.08s) \tTrain Loss=0.273\tDev Loss=0.565\tDev F1=64.88\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (11.68s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.64880952381\n",
      "============================================================\n",
      "[9] Testing dim = 32, l2_penalty = 1.00e-02, lr = 1.00e-04, l1_penalty = 0.00e+00, batch_size = 64\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=64\n",
      "[SparseLogi...] Epoch 0 (0.21s) \tTrain Loss=0.956\tDev Loss=0.944\tDev F1=53.66\n",
      "[SparseLogi...] Epoch 1 (0.52s) \tTrain Loss=0.918\tDev Loss=0.916\tDev F1=53.30\n",
      "[SparseLogi...] Epoch 2 (0.83s) \tTrain Loss=0.886\tDev Loss=0.892\tDev F1=55.69\n",
      "[SparseLogi...] Epoch 3 (1.13s) \tTrain Loss=0.859\tDev Loss=0.872\tDev F1=56.50\n",
      "[SparseLogi...] Epoch 4 (1.43s) \tTrain Loss=0.836\tDev Loss=0.856\tDev F1=56.60\n",
      "[SparseLogi...] Epoch 5 (1.75s) \tTrain Loss=0.816\tDev Loss=0.842\tDev F1=56.97\n",
      "[SparseLogi...] Epoch 6 (2.06s) \tTrain Loss=0.798\tDev Loss=0.830\tDev F1=57.24\n",
      "[SparseLogi...] Epoch 7 (2.37s) \tTrain Loss=0.783\tDev Loss=0.820\tDev F1=57.80\n",
      "[SparseLogi...] Epoch 8 (2.68s) \tTrain Loss=0.769\tDev Loss=0.812\tDev F1=57.93\n",
      "[SparseLogi...] Epoch 9 (3.14s) \tTrain Loss=0.757\tDev Loss=0.805\tDev F1=58.11\n",
      "[SparseLogi...] Epoch 10 (3.45s) \tTrain Loss=0.747\tDev Loss=0.799\tDev F1=58.11\n",
      "[SparseLogi...] Epoch 11 (3.77s) \tTrain Loss=0.737\tDev Loss=0.794\tDev F1=58.84\n",
      "[SparseLogi...] Epoch 12 (4.08s) \tTrain Loss=0.729\tDev Loss=0.789\tDev F1=58.92\n",
      "[SparseLogi...] Epoch 13 (4.39s) \tTrain Loss=0.721\tDev Loss=0.786\tDev F1=59.21\n",
      "[SparseLogi...] Epoch 14 (4.70s) \tTrain Loss=0.713\tDev Loss=0.782\tDev F1=59.23\n",
      "[SparseLogi...] Epoch 15 (5.00s) \tTrain Loss=0.707\tDev Loss=0.780\tDev F1=59.30\n",
      "[SparseLogi...] Epoch 16 (5.31s) \tTrain Loss=0.702\tDev Loss=0.778\tDev F1=59.54\n",
      "[SparseLogi...] Epoch 17 (5.61s) \tTrain Loss=0.696\tDev Loss=0.776\tDev F1=59.86\n",
      "[SparseLogi...] Epoch 18 (5.93s) \tTrain Loss=0.691\tDev Loss=0.774\tDev F1=59.97\n",
      "[SparseLogi...] Epoch 19 (6.24s) \tTrain Loss=0.686\tDev Loss=0.773\tDev F1=60.14\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (6.97s) \tTrain Loss=0.682\tDev Loss=0.772\tDev F1=60.21\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 21 (7.62s) \tTrain Loss=0.678\tDev Loss=0.772\tDev F1=61.35\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 22 (8.44s) \tTrain Loss=0.674\tDev Loss=0.771\tDev F1=61.35\n",
      "[SparseLogi...] Epoch 23 (8.74s) \tTrain Loss=0.671\tDev Loss=0.771\tDev F1=61.66\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 24 (9.58s) \tTrain Loss=0.670\tDev Loss=0.771\tDev F1=62.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (10.07s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.629722921914\n",
      "============================================================\n",
      "[10] Testing dim = 32, l2_penalty = 1.00e-03, lr = 1.00e-01, l1_penalty = 1.00e-04, batch_size = 64\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4766  #epochs=25  batch size=64\n",
      "[SparseLogi...] Epoch 0 (0.21s) \tTrain Loss=0.960\tDev Loss=2.012\tDev F1=62.72\n",
      "[SparseLogi...] Epoch 1 (0.50s) \tTrain Loss=2.006\tDev Loss=2.872\tDev F1=63.96\n",
      "[SparseLogi...] Epoch 2 (0.81s) \tTrain Loss=2.779\tDev Loss=3.598\tDev F1=63.94\n",
      "[SparseLogi...] Epoch 3 (1.13s) \tTrain Loss=3.497\tDev Loss=4.346\tDev F1=63.38\n",
      "[SparseLogi...] Epoch 4 (1.44s) \tTrain Loss=3.971\tDev Loss=4.774\tDev F1=63.21\n",
      "[SparseLogi...] Epoch 5 (1.73s) \tTrain Loss=4.505\tDev Loss=5.375\tDev F1=63.22\n",
      "[SparseLogi...] Epoch 6 (2.03s) \tTrain Loss=5.101\tDev Loss=5.902\tDev F1=63.52\n",
      "[SparseLogi...] Epoch 7 (2.33s) \tTrain Loss=5.429\tDev Loss=6.270\tDev F1=62.66\n",
      "[SparseLogi...] Epoch 8 (2.77s) \tTrain Loss=5.940\tDev Loss=6.855\tDev F1=65.07\n",
      "[SparseLogi...] Epoch 9 (3.18s) \tTrain Loss=6.520\tDev Loss=7.334\tDev F1=64.25\n",
      "[SparseLogi...] Epoch 10 (3.48s) \tTrain Loss=6.894\tDev Loss=7.736\tDev F1=63.23\n",
      "[SparseLogi...] Epoch 11 (3.78s) \tTrain Loss=7.317\tDev Loss=8.207\tDev F1=63.14\n",
      "[SparseLogi...] Epoch 12 (4.09s) \tTrain Loss=7.817\tDev Loss=8.699\tDev F1=63.32\n",
      "[SparseLogi...] Epoch 13 (4.39s) \tTrain Loss=8.147\tDev Loss=8.946\tDev F1=63.44\n",
      "[SparseLogi...] Epoch 14 (4.69s) \tTrain Loss=8.386\tDev Loss=9.274\tDev F1=63.61\n",
      "[SparseLogi...] Epoch 15 (4.99s) \tTrain Loss=8.642\tDev Loss=9.513\tDev F1=63.83\n",
      "[SparseLogi...] Epoch 16 (5.29s) \tTrain Loss=8.898\tDev Loss=9.801\tDev F1=63.46\n",
      "[SparseLogi...] Epoch 17 (5.60s) \tTrain Loss=9.180\tDev Loss=10.044\tDev F1=63.28\n",
      "[SparseLogi...] Epoch 18 (5.91s) \tTrain Loss=9.557\tDev Loss=10.434\tDev F1=62.89\n",
      "[SparseLogi...] Epoch 19 (6.22s) \tTrain Loss=9.900\tDev Loss=10.745\tDev F1=64.49\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogi...] Epoch 20 (6.97s) \tTrain Loss=10.189\tDev Loss=11.035\tDev F1=63.07\n",
      "[SparseLogi...] Epoch 21 (7.28s) \tTrain Loss=10.481\tDev Loss=11.371\tDev F1=64.04\n",
      "[SparseLogi...] Epoch 22 (7.59s) \tTrain Loss=10.767\tDev Loss=11.998\tDev F1=62.92\n",
      "[SparseLogi...] Epoch 23 (7.89s) \tTrain Loss=11.309\tDev Loss=12.277\tDev F1=62.52\n",
      "[SparseLogi...] Epoch 24 (8.19s) \tTrain Loss=11.595\tDev Loss=12.520\tDev F1=63.87\n",
      "[SparseLogisticRegression] Training done (8.29s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.644897959184\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_3>\n",
      "   dim  l2_penalty      lr  l1_penalty  batch_size     Prec.      Rec.  \\\n",
      "3   64     0.00000  0.0010     0.00100          32  0.621813  0.691339   \n",
      "5  128     0.00001  0.0001     0.00001          32  0.654952  0.645669   \n",
      "2   32     0.00001  0.0100     0.00100         128  0.613764  0.688189   \n",
      "7  128     0.00001  0.0010     0.00001          32  0.614951  0.686614   \n",
      "9   32     0.00100  0.1000     0.00010          64  0.567665  0.746457   \n",
      "0   32     0.01000  0.1000     0.00001          32  0.577500  0.727559   \n",
      "6   32     0.00000  0.1000     0.00001          32  0.571782  0.727559   \n",
      "8   32     0.01000  0.0001     0.00000          64  0.674460  0.590551   \n",
      "4   64     0.00001  0.0001     0.00000         128  0.673870  0.540157   \n",
      "1   64     0.00010  0.0001     0.00100         128  0.658586  0.513386   \n",
      "\n",
      "      F-1.0  \n",
      "3  0.654735  \n",
      "5  0.650278  \n",
      "2  0.648849  \n",
      "7  0.648810  \n",
      "9  0.644898  \n",
      "0  0.643902  \n",
      "6  0.640333  \n",
      "8  0.629723  \n",
      "4  0.599650  \n",
      "1  0.576991  \n",
      "[SparseLogisticRegression] Model saved as <discriminative_tacred>\n",
      "### Done in 109.7s.\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [F1 Score, Precision, Recall]\n",
      "Index: []\n",
      "CPU times: user 2min 16s, sys: 24.4 s, total: 2min 40s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from snorkel.models import Sentence\n",
    "# s = pipe.session.query(Sentence).first()\n",
    "# s.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from spacy.lemmatizer import Lemmatizer\n",
    "# from spacy.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "# lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "# lemmas = lemmatizer(u'ducks', u'NOUN')\n",
    "# print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from spacy.lemmatizer import Lemmatizer\n",
    "# lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
