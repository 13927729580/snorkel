{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro. to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: Training our End Extraction Model\n",
    "\n",
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last tutorial part to train our end extraction model.\n",
    "\n",
    "For this tutorial, we will be training a simple - but fairly effective - logistic regression model.  More generally, however, Snorkel plugs in with many ML libraries including [TensorFlow](https://www.tensorflow.org/), making it easy to use almost any state-of-the-art model as the end extractor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the database backend and initalize a Snorkel session\n",
    "from lib.init import *\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass, and load the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 Training a `SparseLogReg` Discriminative Model\n",
    "We use the training marginals to train a discriminative model that classifies each `Candidate` as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature Extraction\n",
    "Instead of using a deep learning approach to start, let's look at a standard sparse logistic regression model. First, we need to extract out features. This can take a while, but we only have to do it once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "from lib.features import hybrid_span_mention_ftrs\n",
    "\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev = featurizer.load_matrix(session, split=1)\n",
    "F_test = featurizer.load_matrix(session, split=2)\n",
    "\n",
    "# if F_train.size == 0:    \n",
    "#     %time F_train = featurizer.apply(split=0, parallelism=2, annotation_generator=hybrid_span_mention_ftrs)\n",
    "# if F_dev.size == 0:     \n",
    "#     %time F_dev  = featurizer.apply_existing(split=1, parallelism=2, annotation_generator=hybrid_span_mention_ftrs)\n",
    "# if F_test.size == 0:\n",
    "#     %time F_test = featurizer.apply_existing(split=2, parallelism=2, annotation_generator=hybrid_span_mention_ftrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "First, reload the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, F_train, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code performs model selection by tuning our learning algorithm's hyperparamters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 5. Search space size = 3375.\n",
      "Loading data...\n",
      "Launching jobs...\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=8757  #epochs=2000  batch size=64\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=17214  #epochs=2000  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (1.43s)\tAverage loss=1.043090\n",
      "[SparseLogisticRegression] Epoch 0 (3.34s)\tAverage loss=2.560153\n",
      "[SparseLogisticRegression] Epoch 250 (352.39s)\tAverage loss=4.345488\n",
      "[SparseLogisticRegression] Epoch 500 (710.75s)\tAverage loss=12.152730\n",
      "[SparseLogisticRegression] Epoch 250 (832.17s)\tAverage loss=2.388735\n",
      "[SparseLogisticRegression] Epoch 750 (1062.99s)\tAverage loss=24.711330\n",
      "[SparseLogisticRegression] Epoch 1000 (1422.07s)\tAverage loss=42.080006\n",
      "[SparseLogisticRegression] Epoch 500 (1661.71s)\tAverage loss=2.405527\n",
      "[SparseLogisticRegression] Epoch 1250 (1773.06s)\tAverage loss=64.184975\n",
      "[SparseLogisticRegression] Epoch 1500 (2131.31s)\tAverage loss=91.016563\n",
      "[SparseLogisticRegression] Epoch 1750 (2489.52s)\tAverage loss=122.680954\n",
      "[SparseLogisticRegression] Epoch 750 (2491.77s)\tAverage loss=2.420808\n",
      "[SparseLogisticRegression] Epoch 1999 (2850.73s)\tAverage loss=158.870148\n",
      "[SparseLogisticRegression] Training done (2850.74s)\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "Model 0 Done; score: 0.266666666667\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5254  #epochs=2000  batch size=64\n",
      "[SparseLogisticRegression] Epoch 0 (0.86s)\tAverage loss=290.427338\n",
      "[SparseLogisticRegression] Epoch 250 (228.91s)\tAverage loss=291.048462\n",
      "[SparseLogisticRegression] Epoch 500 (452.05s)\tAverage loss=291.248657\n",
      "[SparseLogisticRegression] Epoch 1000 (3348.09s)\tAverage loss=2.435041\n",
      "[SparseLogisticRegression] Epoch 750 (676.52s)\tAverage loss=291.412933\n",
      "[SparseLogisticRegression] Epoch 1000 (903.78s)\tAverage loss=291.575104\n",
      "[SparseLogisticRegression] Epoch 1250 (1156.93s)\tAverage loss=291.731659\n",
      "[SparseLogisticRegression] Epoch 1500 (1385.33s)\tAverage loss=292.001190\n",
      "[SparseLogisticRegression] Epoch 1250 (4245.45s)\tAverage loss=2.448962\n",
      "[SparseLogisticRegression] Epoch 1750 (1627.32s)\tAverage loss=292.195435\n",
      "[SparseLogisticRegression] Epoch 1999 (1891.41s)\tAverage loss=292.485016\n",
      "[SparseLogisticRegression] Training done (1891.42s)\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_2>\n",
      "Model 2 Done; score: 0.216110019646\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5254  #epochs=2000  batch size=64\n",
      "[SparseLogisticRegression] Epoch 0 (1.18s)\tAverage loss=5.481596\n",
      "[SparseLogisticRegression] Epoch 250 (229.95s)\tAverage loss=5.365909\n",
      "[SparseLogisticRegression] Epoch 1500 (5154.07s)\tAverage loss=2.463488\n",
      "[SparseLogisticRegression] Epoch 500 (441.61s)\tAverage loss=5.367777\n",
      "[SparseLogisticRegression] Epoch 750 (652.54s)\tAverage loss=5.374406\n",
      "[SparseLogisticRegression] Epoch 1000 (862.75s)\tAverage loss=5.381591\n",
      "[SparseLogisticRegression] Epoch 1250 (1079.74s)\tAverage loss=5.392536\n",
      "[SparseLogisticRegression] Epoch 1750 (5971.93s)\tAverage loss=2.477672\n",
      "[SparseLogisticRegression] Epoch 1500 (1316.40s)\tAverage loss=5.402205\n",
      "[SparseLogisticRegression] Epoch 1750 (1556.93s)\tAverage loss=5.413952\n",
      "[SparseLogisticRegression] Epoch 1999 (1789.09s)\tAverage loss=5.427506\n",
      "[SparseLogisticRegression] Training done (1789.09s)\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_3>\n",
      "Model 3 Done; score: 0.256302521008\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=8757  #epochs=2000  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (2.31s)\tAverage loss=1.198025\n",
      "[SparseLogisticRegression] Epoch 1999 (6861.60s)\tAverage loss=2.492258\n",
      "[SparseLogisticRegression] Training done (6861.60s)\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_1>\n",
      "Model 1 Done; score: 0.178660049628\n",
      "[SparseLogisticRegression] Epoch 250 (520.34s)\tAverage loss=28.651218\n",
      "[SparseLogisticRegression] Epoch 500 (1009.96s)\tAverage loss=104.562546\n",
      "[SparseLogisticRegression] Epoch 750 (1494.93s)\tAverage loss=228.676819\n",
      "[SparseLogisticRegression] Epoch 1000 (1970.75s)\tAverage loss=401.223145\n",
      "[SparseLogisticRegression] Epoch 1250 (2467.06s)\tAverage loss=621.952271\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param        = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param          = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param          = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "batch_size_param  = ListParameter('batch_size', [32, 64, 128])\n",
    "b_param           = ListParameter('b', [0.5, 0.6, 0.7])\n",
    "balance_param     = ListParameter('rebalance', [0.0, 0.3, 0.5])\n",
    "\n",
    "param_grid = [rate_param, l1_param, l2_param, batch_size_param, b_param, balance_param]\n",
    "\n",
    "np.random.seed(1701)\n",
    "searcher = RandomSearch(SparseLogisticRegression, param_grid, F_train,\n",
    "                        Y_train=train_marginals, n=5, n_threads=1)\n",
    "\n",
    "logreg, run_stats = searcher.fit(F_dev, L_gold_dev, n_epochs=2000, print_freq=250, n_threads=2)\n",
    "print run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Examining Features\n",
    "Extracting features allows us to inspect and interperet our learned weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lib.scoring import *\n",
    "print_top_k_features(session, logreg, F_train, top_k=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
