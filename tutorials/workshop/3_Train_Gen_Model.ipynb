{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro. to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Training the Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass, and load the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load development set gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Labeling Functions\n",
    "\n",
    "Add all your labeling functions (and their dependencies) here. These will be used to train our generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    return 1 if len(spouses.intersection(get_between_tokens(c))) > 0 else 0\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return 1\n",
    "    elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return -1 if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else 0\n",
    "\n",
    "def LF_and_married(c):\n",
    "    return 1 if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else 0\n",
    "    \n",
    "def LF_familial_relationship(c):\n",
    "    return -1 if len(family.intersection(get_between_tokens(c))) > 0 else 0\n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return -1\n",
    "    elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    return -1 if len(other.intersection(get_between_tokens(c))) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return 1 if (p1, p2) in known_spouses or (p2, p1) in known_spouses else 0\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return 1 if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n",
      "CPU times: user 29.3 s, sys: 363 ms, total: 29.7 s\n",
      "Wall time: 30.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4781, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Unifying supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Majority Vote\n",
    "The most simple way to unify the output of all your LFs is by computed the _unweighted majority vote_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply_existing(split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos/neg    7:221 3.1%/96.9%\n",
      "precision  50.00\n",
      "recall     85.71\n",
      "f1         63.16\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "majority_vote_score(L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generative Model\n",
    "In data programming, we use a more sophisitcated model to unify our labeling functions. We know that these labeling functions will not be perfect, and some may be quite low-quality, so we will _model_ their accuracies with a generative model, which Snorkel will help us easily apply.\n",
    "\n",
    "This will ultimately produce a single set of **noise-aware training labels**, which we will then use to train an end extraction model in the next notebook.  For more technical details of this overall approach, see our [NIPS 2016 paper](https://arxiv.org/abs/1605.07723)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=500, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Marginal Probabilities\n",
    "One immediate santity check  you can peform using the generative model is to visually examine the distribution of predicted training marginals. Ideally, there should get a bimodal distribution with large seperation between each peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12688d350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.857\n",
      "Neg. class accuracy: 0.973\n",
      "Precision            0.5\n",
      "Recall               0.857\n",
      "F1                   0.632\n",
      "----------------------------------------\n",
      "TP: 6 | FP: 6 | TN: 215 | FN: 1\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_marginals = gen_model.marginals(L_dev)\n",
    "_, _, _, _ = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Evaluating on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (`split` 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L_test = labeler.apply_existing(split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.833\n",
      "Neg. class accuracy: 0.96\n",
      "Precision            0.312\n",
      "Recall               0.833\n",
      "F1                   0.455\n",
      "----------------------------------------\n",
      "TP: 5 | FP: 11 | TN: 261 | FN: 1\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_marginals = gen_model.marginals(L_test)\n",
    "_, _, _, _ = gen_model.score(session, L_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that if this is the final test set that you will be reporting final numbers on, to avoid biasing results you should not inspect results.  However you can run the model on your _development set_ and, as we did in the previous part with the generative labeling function model, inspect examples to do error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our training labels\n",
    "\n",
    "Finally, we'll save the `training_marginals`, which are our **\"noise-aware training labels\"**, so that we can use them in the next tutorial to train our end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4781 marginals\n",
      "CPU times: user 2.68 s, sys: 16.4 ms, total: 2.7 s\n",
      "Wall time: 2.71 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structure Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to include the dependencies between our LFs when training the generative model. Snorkel makes it easy to do this! `DependencySelector` runs a fast structure learning algorithm over the matrix of LF outputs to identify a set of likely dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.3)\n",
    "print len(deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the generative model, using the `deps` argument to account for the learned dependencies. We'll also model LF propensity here, unlike the intro tutorial. In addition to learning the accuracies of the LFs, this also learns their likelihood of labeling an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, deps=deps, epochs=500, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3uEhc3QNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12699d610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.714\n",
      "Neg. class accuracy: 0.982\n",
      "Precision            0.556\n",
      "Recall               0.714\n",
      "F1                   0.625\n",
      "----------------------------------------\n",
      "TP: 5 | FP: 4 | TN: 217 | FN: 2\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_marginals = gen_model.marginals(L_dev)\n",
    "_, _, _, _ = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
