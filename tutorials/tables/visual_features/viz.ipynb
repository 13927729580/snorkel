{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Visual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/tables/')\n",
    "\n",
    "from pdb import set_trace as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Coordinate Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ines's code takes PDF and yields pdf_word_list, coordinate_map\n",
    "import os \n",
    "# import time\n",
    "import subprocess\n",
    "from parseHTMLoutput import extract_coordinates_HTML\n",
    "\n",
    "pdf_path = os.environ['SNORKELHOME'] + '/tutorials/tables/data/hardware/hardware100_pdf/'\n",
    "filename = 'bc546-d'\n",
    "pdf_file = pdf_path + filename + '.pdf' # Path to PDF file \n",
    "nb_pages = subprocess.check_output(\"pdfinfo {} | grep Pages  | sed 's/[^0-9]*//'\".format(pdf_file), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create words list and coordinates map\n",
    "pdf_word_list = []\n",
    "coordinate_map= {}\n",
    "for i in range(1,int(nb_pages)+1):\n",
    "    html_content = subprocess.check_output('pdftotext -f {} -l {} -bbox-layout {} -'.format(str(i), str(i), pdf_file), shell=True)\n",
    "    pdf_word_list_i, coordinate_map_i = extract_coordinates_HTML(html_content, str(i))\n",
    "    pdf_word_list += pdf_word_list_i\n",
    "    coordinate_map.update(coordinate_map_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sort pdf_word_list based on coordinates (left to right, top to bottom)\n",
    "sorted_pdf_word_list = sorted(pdf_word_list, key=lambda (word_id,_): (float(coordinate_map[word_id][0]), float(coordinate_map[word_id][1]), float(coordinate_map[word_id][2])), reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: PDF to HTML Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Adobe Acrobat (or other program of your choice) to convert PDF -> HTML with structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "CPU times: user 3.05 s, sys: 164 ms, total: 3.21 s\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "# Payal's parser takes HTML and yields corpus object, html_word_list\n",
    "import os\n",
    "os.remove('snorkel.db')\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser import HTMLParser\n",
    "from snorkel.parser import OmniParser\n",
    "\n",
    "html_path = os.environ['SNORKELHOME'] + '/tutorials/tables/data/hardware/hardware100_html/'\n",
    "filename = 'bc546-d'\n",
    "html_file = html_path + filename + '.html'\n",
    "doc_parser = HTMLParser(path=html_file)\n",
    "context_parser = OmniParser()\n",
    "cp = CorpusParser(doc_parser, context_parser, max_docs=100) \n",
    "\n",
    "%time corpus = cp.parse_corpus(name='Hardware', session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2662\n",
      "[((2, 0), u'BC546'),\n",
      " ((2, 1), u'-'),\n",
      " ((2, 2), u'NPN'),\n",
      " ((2, 3), u'Amplifier'),\n",
      " ((2, 4), u'Transistors'),\n",
      " ((3, 0), u'BC546B'),\n",
      " ((3, 1), u','),\n",
      " ((3, 2), u'BC547A'),\n",
      " ((3, 3), u','),\n",
      " ((3, 4), u'B')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "html_word_list = []\n",
    "for phrase in corpus.documents[0].phrases:\n",
    "    for i, word in enumerate(phrase.words):\n",
    "        html_word_list.append(((phrase.id, i), word))\n",
    "\n",
    "print len(html_word_list)\n",
    "pprint(html_word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visual Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 525 ms, sys: 6.4 ms, total: 532 ms\n",
      "Wall time: 550 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "visual_linking.py:11: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  offsets.append(wordsB.index(wordsA[i]) - i)\n",
      "visual_linking.py:46: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if a[1] == b[1]:\n"
     ]
    }
   ],
   "source": [
    "from visual_linking import link_lists\n",
    "\n",
    "%time links = link_lists(html_word_list, sorted_pdf_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Updating with coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Payal's code walks through phrases, updating each one's five visual attributes\n",
    "# (page, top, left, bottom, right)\n",
    "import pickle\n",
    "from snorkel.models import Phrase\n",
    "\n",
    "for phrase in corpus.documents[0].phrases:\n",
    "    (page, top, left, bottom, right) = zip(\n",
    "        *[coordinate_map[links[((phrase.id), i)]] for i in range(len(phrase.words))])\n",
    "    page = page[0]\n",
    "    session.query(Phrase).filter(Phrase.id == phrase.id).update({\"page\": page, \n",
    "                                                             \"top\":  top, \n",
    "                                                             \"left\": left, \n",
    "                                                             \"bottom\": bottom, \n",
    "                                                             \"right\": right})\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'BC546', u'-', u'NPN', u'Amplifier', u'Transistors']\n",
      "('277.176000', '268.296000', '136.652000', '109.020000', '109.020000')\n"
     ]
    }
   ],
   "source": [
    "print corpus.documents[0].phrases[0].words\n",
    "print corpus.documents[0].phrases[0].top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Candidates from Corpus (Hardware)\n",
      "[========================================] 100%\n",
      "CPU times: user 1.23 s, sys: 11.7 ms, total: 1.24 s\n",
      "Wall time: 1.25 s\n",
      "Candidate Set (Hardware Candidates) contains 540 Candidates\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from snorkel.models import Corpus, candidate_subclass\n",
    "from snorkel.matchers import RegexMatchSpan, Union\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "from snorkel.utils import get_ORM_instance\n",
    "from hardware_utils import OmniNgramsPart, OmniNgramsTemp, get_gold_dict\n",
    "\n",
    "# Candidate Type\n",
    "Part_Temp = candidate_subclass('Part_Temp', ['part','temp'])\n",
    "    \n",
    "# CandidateSpaces\n",
    "part_ngrams = OmniNgramsPart(parts_by_doc=None, n_max=3)\n",
    "temp_ngrams = OmniNgramsTemp(n_max=2)\n",
    "\n",
    "# Matchers\n",
    "eeca_matcher = RegexMatchSpan(rgx='([b]{1}[abcdefklnpqruyz]{1}[\\swxyz]?[0-9]{3,5}[\\s]?[A-Z\\/]{0,5}[0-9]?[A-Z]?([-][A-Z0-9]{1,7})?([-][A-Z0-9]{1,2})?)')\n",
    "jedec_matcher = RegexMatchSpan(rgx='([123]N\\d{3,4}[A-Z]{0,5}[0-9]?[A-Z]?)')\n",
    "jis_matcher = RegexMatchSpan(rgx='(2S[abcdefghjkmqrstvz]{1}[\\d]{2,4})')\n",
    "others_matcher = RegexMatchSpan(rgx='((NSVBC|SMBT|MJ|MJE|MPS|MRF|RCA|TIP|ZTX|ZT|TIS|TIPL|DTC|MMBT|PZT){1}[\\d]{2,4}[A-Z]{0,3}([-][A-Z0-9]{0,6})?([-][A-Z0-9]{0,1})?)')\n",
    "parts_matcher = Union(eeca_matcher, jedec_matcher, jis_matcher, others_matcher)\n",
    "\n",
    "temp_matcher = RegexMatchSpan(rgx=r'1[4-6]0', longest_match_only=False)\n",
    "\n",
    "# Throttler\n",
    "part_throttler = None\n",
    "\n",
    "# Extractor\n",
    "ce = CandidateExtractor(Part_Temp, \n",
    "                        [part_ngrams, temp_ngrams], \n",
    "                        [parts_matcher, temp_matcher], \n",
    "                        throttler=part_throttler)\n",
    "\n",
    "# Extract\n",
    "for corpus_name in ['Hardware']:\n",
    "    corpus = get_ORM_instance(Corpus, session, corpus_name)\n",
    "    print \"Extracting Candidates from %s\" % corpus\n",
    "    %time candidates = ce.extract(\\\n",
    "        corpus.documents, corpus_name + ' Candidates', session)\n",
    "    session.add(candidates)\n",
    "    print \"%s contains %d Candidates\" % (candidates, len(candidates))\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImplicitSpan(\"BC546B\", parent=3, words=[0,2], position=[0])\n",
      "0 13\n",
      "0\n",
      "2\n",
      "50.100000\n",
      "Phrase(Doc: bc546-d, Table: X, Row: X, Col: X, Position: 0, Text: BC546B, BC547A, B, C, BC548B, C)\n",
      "[0, 6, 8, 14, 16, 17, 19, 20, 22, 28, 30]\n",
      "('50.100000', '70.020600', '50.100000', '70.020600', '70.020600', '70.020600', '70.020600', '70.020600', '70.020600', '70.020600', '70.020600')\n",
      "('73.500000', '93.420600', '73.500000', '93.420600', '93.420600', '93.420600', '93.420600', '93.420600', '93.420600', '93.420600', '93.420600')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "c = candidates[5]\n",
    "print c.part\n",
    "print c.part.char_start, c.part.char_end\n",
    "print c.part.get_word_start()\n",
    "print c.part.get_word_end()\n",
    "print c.part.get_attrib_span('top')\n",
    "\n",
    "print c.part.parent\n",
    "print c.part.parent.char_offsets\n",
    "print c.part.parent.top\n",
    "print c.part.parent.bottom\n",
    "print c.part.parent.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assess Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Ines's code to plot locations of words on the original PDF\n",
    "import numpy as np\n",
    "import cv2\n",
    "from visual_linking import display_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_to_diplay = '2' # page number to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOUNDING BOXES on PDF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_box(pdf_file, [box for box in coordinate_map.values() if box[0]==page_to_diplay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Ordering on a black page - Not very easy to display because of superposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_height = 792\n",
    "page_width = 612\n",
    "img = np.zeros((page_height,page_width,3))\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "i = 0\n",
    "for word_id, _ in sorted_pdf_word_list:\n",
    "    if word_id[0] == page_to_diplay:\n",
    "        i += 1\n",
    "        __, top, left, bottom, right = coordinate_map[word_id]\n",
    "        cv2.rectangle(img,(int(float(left)),int(float(top))),(int(float(right)),int(float(bottom))),(0,255,0),1)\n",
    "        cv2.putText(img, str(i), (int((float(left)+float(right))/2), int(float(bottom))), font, 0.3, (0,0,255), 1)\n",
    "cv2.imshow('Ordering',img)\n",
    "cv2.waitKey() # press any key to exit the opencv output \n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
