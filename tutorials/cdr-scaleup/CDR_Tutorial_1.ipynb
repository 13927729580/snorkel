{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chemical-Disease Relation (CDR) Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* **chemical-induced-disease relationships** from Pubmed abstracts, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial will show off some of the more advanced features of Snorkel, so we'll assume you've followed the Intro tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task Description\n",
    "\n",
    "The CDR task is comprised of three sets of 500 documents each, called training, development, and test. A document consists of the title and abstract of an article from [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/), an archive of biomedical and life sciences journal literature. The documents have been hand-annotated with\n",
    "* Mentions of chemicals and diseases along with their [MESH](https://meshb.nlm.nih.gov/#/fieldSearch) IDs, canonical IDs for medical entities. For example, mentions of \"warfarin\" in two different documents will have the same ID.\n",
    "* Chemical-disease relations at the document-level. That is, if some piece of text in the document implies that a chemical with MESH ID `X` induces a disease with MESH ID `Y`, the document will be annotated with `Relation(X, Y)`.\n",
    "\n",
    "The goal is to extract the document-level relations on the test set (without accessing the entity or relation annotations). For this tutorial, we make the following assumptions and alterations to the task:\n",
    "* We discard all of the entity mention annotations and assume we have access to a state-of-the-art entity tagger (see Part I) to identify chemical and disease mentions, and link them to their canonical IDs.\n",
    "* We shuffle the training and development sets a bit, producing a new training set with 900 documents and a new development set with 100 documents. We discard the training set relation annotations, but keep the development set to evaluate our labeling functions and extraction model.\n",
    "* We evaluate the task at the mention-level, rather than the document-level. We will convert the document-level relation annotations to mention-level by simply saying that a mention pair `(X, Y)` in document `D` if `Relation(X, Y)` was hand-annotated at the document-level for `D`.\n",
    "\n",
    "In effect, the only inputs to this application are the plain text of the documents, a pre-trained entity tagger, and a small development set of annotated documents. This is representative of many information extraction tasks, and Snorkel is the perfect tool to bootstrap the extraction process with weak supervision. Let's get going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 0: Initial Prep\n",
    "\n",
    "In your shell, download the raw data by running:\n",
    "```bash\n",
    "cd tutorials/cdr\n",
    "./download_data.sh\n",
    "```\n",
    "\n",
    "Note that if you've previously run this tutorial (using SQLite), you can delete the old database by running (in the same directory as above):\n",
    "```bash\n",
    "rm snorkel.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part I: Corpus Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "#os.environ['SNORKELDB'] = \"postgres://localhost:4554/biocorpus\"\n",
    "os.environ['SNORKELDB'] = \"postgres:///biocorpus\"\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Configuring a `DocPreprocessor`\n",
    "\n",
    "We'll start by defining a `DocPreprocessor` object to read in Pubmed abstracts from [Pubtator]([Pubtator](http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/index.cgi). There some extra annotation information in the file, while we'll skip for now. We'll use the `XMLMultiDocPreprocessor` class, which allows us to use [XPath queries](https://en.wikipedia.org/wiki/XPath) to specify the relevant sections of the XML format.\n",
    "\n",
    "Note that we are newline-concatenating text from the title and abstract together for simplicity, but if we wanted to, we could easily extend the `DocPreprocessor` classes to preserve information about document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.models import SequenceTag, Document\n",
    "from snorkel.parser import XMLMultiDocPreprocessor\n",
    "\n",
    "doc_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=\"\",\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "# create standard CDR folds\n",
    "folds = {}\n",
    "fold_defs = {0:'CDR_TrainingSet.BioC.xml', 1:'CDR_DevelopmentSet.BioC.xml', 2:'CDR_TestSet.BioC.xml'}\n",
    "\n",
    "for split,file_name in fold_defs.items():\n",
    "    folds[split] = [doc.name for doc,text in doc_preprocessor.parse_file(\"data/\" + file_name, \"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating a `CorpusParser`\n",
    "\n",
    "Similar to the Intro tutorial, we'll now construct a `CorpusParser` using the preprocessor we just defined. However, this one has an extra ingredient: an entity tagger. [TaggerOne](https://www.ncbi.nlm.nih.gov/pubmed/27283952) is a popular entity tagger for PubMed, so we went ahead and preprocessed its tags on the CDR corpus for you. The function `TaggerOneTagger.tag` (in `utils.py`) tags sentences with mentions of chemicals and diseases. We'll use these tags to extract candidates in Part II. The tags are stored in `Sentence.entity_cids` and `Sentence.entity_types`, which are analog to `Sentence.words`.\n",
    "\n",
    "Recall that in the wild, we wouldn't have the manual labels included with the CDR data, and we'd have to use an automated tagger (like TaggerOne) to tag entity mentions. That's what we're doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import Spacy\n",
    "\n",
    "parser = Spacy()\n",
    "\n",
    "for split in folds:\n",
    "    # Check if documents already exist in database. \n",
    "    # If not, manually parse and add to database\n",
    "    documents = session.query(Document).filter(Document.name.in_(folds[split])).all()\n",
    "    missing = set(folds[split]).difference([doc.name for doc in documents])\n",
    "    if missing:\n",
    "        print \"Parsing {} documents...\".format(len(missing))\n",
    "        documents = [(doc,text) for doc,text in doc_preprocessor.parse_file(\"data/\" + fold_defs[split], \"\") \n",
    "                     if doc.name in missing]\n",
    "        sentences = []\n",
    "        for doc, text in documents:\n",
    "            for parts in parser.parse(doc, text):\n",
    "                s = Sentence(**parts)\n",
    "                session.add(s)\n",
    "    \n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TaggerOne Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27807\n"
     ]
    }
   ],
   "source": [
    "name2id = dict(session.query(Document.name, Document.id).all())\n",
    "\n",
    "# load taggerone tags\n",
    "seq_tags = []\n",
    "taggerone = [line.split(\"\\t\") for line in open(\"data/taggerone.tags.tsv\",\"rU\").read().splitlines()]\n",
    "for row in taggerone:\n",
    "    pmid, start, end, concept_type, concept_uid, source = row\n",
    "    seq_tags.append(SequenceTag(document_id=name2id[pmid], abs_char_start=start, abs_char_end=end,\n",
    "                                concept_type=concept_type, concept_uid=concept_uid, source=source))\n",
    "\n",
    "session.bulk_save_objects(seq_tags)\n",
    "session.commit()\n",
    "print len(seq_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Documents:', 2151L)\n",
      "('Sentences:', 20552L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part II: Candidate Extraction\n",
    "\n",
    "With the TaggerOne entity tags, candidate extraction is pretty easy! We split into some preset training, development, and test sets. Then we'll use PretaggedCandidateExtractor to extract candidates using the TaggerOne entity tags.\n",
    "\n",
    "## CDR Train/Dev/Test Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from custom_cand_generator import SequenceTagCandidateExtractor\n",
    "\n",
    "# bin sentences by CDR fold\n",
    "sentences = {}\n",
    "for split in folds:\n",
    "    documents = session.query(Document).filter(Document.name.in_(folds[split])).all() \n",
    "    sentences[split] = list(itertools.chain.from_iterable([doc.sentences for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "('Number of candidates:', 4465L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "('Number of candidates:', 4920L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "('Number of candidates:', 4727L)\n"
     ]
    }
   ],
   "source": [
    "candidate_extractor1 = SequenceTagCandidateExtractor(\n",
    "    ChemicalDisease, ['Disease', 'Chemical'], tag_sources=['TaggerOne']\n",
    ")\n",
    "\n",
    "for split, sents in sentences.items():\n",
    "    candidate_extractor1.apply(sents, clear=True, split=split)\n",
    "    print(\"Number of candidates:\", session.query(ChemicalDisease).filter(ChemicalDisease.split == split).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We should get 8268 candidates in the training set, 888 candidates in the development set, and 4620 candidates in the test set.\n",
    "\n",
    "## PubTator Random/Query Folds\n",
    "We generate 2 sets of 10K documents, generated via resevoir sampling. Random is sampled unformly from all docs; query is sampled from the set of documents matching any NER term found in CDR training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0/99986 random sampled documents\n",
      "Found 0/99997 query sampled documents\n"
     ]
    }
   ],
   "source": [
    "unlabeled = {\n",
    "    'random': open(\"data/random.pmids.txt\",\"rU\").read().splitlines(),\n",
    "    'query': open(\"data/query.pmids.txt\",\"rU\").read().splitlines()\n",
    "}\n",
    "\n",
    "random_docs = session.query(Document).filter(Document.name.in_(unlabeled['random'])).all()\n",
    "query_docs  = session.query(Document).filter(Document.name.in_(unlabeled['query'])).all()\n",
    "\n",
    "print \"Found {}/{} random sampled documents\".format(len(random_docs),len(unlabeled['random']))\n",
    "print \"Found {}/{} query sampled documents\".format(len(query_docs),len(unlabeled['query']))\n",
    "\n",
    "random_sents = list(itertools.chain.from_iterable([doc.sentences for doc in random_docs]))\n",
    "query_sents = list(itertools.chain.from_iterable([doc.sentences for doc in query_docs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate_extractor2 = SequenceTagCandidateExtractor(\n",
    "    ChemicalDisease, ['Disease', 'Chemical'], tag_sources=['PubTator']\n",
    ")\n",
    "\n",
    "for split,sents in [(3,random_sents), (4,query_sents)]:\n",
    "    candidate_extractor2.apply(sents, clear=True, split=split)\n",
    "    print(\"Number of candidates:\", session.query(ChemicalDisease).filter(ChemicalDisease.split == split).count())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
