{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('snorkel.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `Corpus`\n",
    "\n",
    "First, we will load and pre-process the corpus, storing it for convenience in a `Corpus` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a DocParser & ContextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser import HTMLParser\n",
    "from snorkel.parser import OmniParser\n",
    "\n",
    "doc_parser = HTMLParser(path='data/hardware/hardware_html/')\n",
    "context_parser = OmniParser()\n",
    "cp = CorpusParser(doc_parser, context_parser, max_docs=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradenhancock/snorkel/snorkel/parser.py:206: RuntimeWarning: Request from file PHGLS20126-1 too long. Max character count is 100K. Request was skipped.\n",
      "  warnings.warn(\"Request from file {} too long. Max character count is 100K. Request was skipped.\".format(document.name), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "CPU times: user 5min 3s, sys: 12.9 s, total: 5min 16s\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = cp.parse_corpus(name='Hardware Training', session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & loading the `Corpus`\n",
    "\n",
    "Finally, we'll put this all together using a `CorpusParser` object, which will execute the parsers and store the results as a `Corpus`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document SGSTS13702-1\n",
      "Document SIEMS01215-1\n",
      "Document TKCGS00622-1\n",
      "Document RECTS01158-1\n",
      "Document MOTOS04796-1\n"
     ]
    }
   ],
   "source": [
    "for d in corpus.documents[:5]: print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document SGSTS13702-1\n",
      "Phrase(Document SGSTS13702-1, 0, u'SMALL SIGNAL NPN TRANSISTORS')\n",
      "[u'SMALL', u'SIGNAL', u'NPN', u'TRANSISTORS']\n",
      "[u'NNP', u'NNP', u'NNP', u'NNS']\n"
     ]
    }
   ],
   "source": [
    "doc = corpus.documents[0]\n",
    "print doc\n",
    "phrase = doc.phrases[0]\n",
    "print phrase\n",
    "print phrase.words\n",
    "print phrase.poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the `Corpus`\n",
    "Finally, we persist the parsed corpus in Snorkel's database backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(corpus)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the `Corpus`\n",
    "If the corpus has already been parsed, load it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus (Hardware Training)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import Corpus\n",
    "\n",
    "corpus = session.query(Corpus).filter(Corpus.name == 'Hardware Training').one()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Part_Temp = candidate_subclass('Part_Temp', ['part','temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import OmniNgrams\n",
    "\n",
    "omni_ngrams = OmniNgrams(n_max=3, split_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5331 part numbers.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_extended_parts_dict\n",
    "\n",
    "filename='data/hardware/gold_all.csv'\n",
    "parts_dict = load_extended_parts_dict(filename)\n",
    "print \"Loaded %d part numbers.\" % len(parts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch, RegexMatchSpan\n",
    "\n",
    "parts_matcher = DictionaryMatch(d=parts_dict, ignore_case=True)\n",
    "# temp_matcher = RegexMatchSpan(rgx=ur'[\\-\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015\\u2212]\\s?[5-7][05]')\n",
    "temp_matcher = RegexMatchSpan(rgx=ur'-\\s?[5-7][05]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "ce = CandidateExtractor(Part_Temp, [omni_ngrams, omni_ngrams], [parts_matcher, temp_matcher])\n",
    "# ce = CandidateExtractor(Temp, [ngrams], [temp_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 47.7 s, sys: 913 ms, total: 48.6 s\n",
      "Wall time: 48.7 s\n",
      "Number of candidates: 646\n"
     ]
    }
   ],
   "source": [
    "%time c = ce.extract(corpus.documents, 'Hardware Training Candidates', session)\n",
    "print \"Number of candidates:\", len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# session.rollback()\n",
    "# session.delete(c)\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part_Temp(Span(\"BC547C\", parent=82184, chars=[9,14], words=[2,2]), Span(\"-65\", parent=124033, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC143ZE\", parent=125634, chars=[72,79], words=[8,8]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC124XE\", parent=125634, chars=[81,88], words=[9,9]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC144EE\", parent=125634, chars=[18,25], words=[2,2]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC114TE\", parent=125634, chars=[36,43], words=[4,4]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC123EE\", parent=125634, chars=[54,61], words=[6,6]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC123JE\", parent=83605, chars=[12,19], words=[2,2]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC124EE\", parent=125634, chars=[9,16], words=[1,1]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC114YE\", parent=125634, chars=[27,34], words=[3,3]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n",
      "Part_Temp(Span(\"DTC143TE\", parent=125634, chars=[45,52], words=[5,5]), Span(\"-55\", parent=125458, chars=[0,2], words=[0,0]))\n"
     ]
    }
   ],
   "source": [
    "for cand in c[:10]:\n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the extracted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Candidate Set (Hardware Training Candidates)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "c = session.query(CandidateSet).filter(CandidateSet.name == 'Hardware Training Candidates').one()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make labels gold, candidates gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'Hardware Training Candidates').one()\n",
    "# dev = session.query(CandidateSet).filter(CandidateSet.name == 'Hardware Development Candidates').one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.create(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unicode(F_train.get_candidate(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_train.get_key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part_Temp(Span(\"BC547C\", parent=82184, chars=[9,14], words=[2,2]), Span(\"-65\", parent=124033, chars=[0,2], words=[0,0]))\n",
      "[u'bc547b', u'/']\n",
      "[u'to', u'150']\n",
      "[u'bc547b', u'/']\n",
      "[]\n",
      "[]\n",
      "[u'to', u'150']\n",
      "Span(\"-65\", parent=124033, chars=[0,2], words=[0,0])\n",
      "[u'-65', u'to', u'150']\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import get_right_tokens, get_left_tokens, contains_token\n",
    "from snorkel.lf_helpers import get_cell_ngrams\n",
    "cand = c[0]\n",
    "print cand\n",
    "print get_left_tokens(cand)\n",
    "print get_right_tokens(cand)\n",
    "print get_left_tokens(cand[0])\n",
    "print get_right_tokens(cand[0])\n",
    "print get_left_tokens(cand[1])\n",
    "print get_right_tokens(cand[1])\n",
    "print cand[1]\n",
    "print get_cell_ngrams(cand[1], n_max=1, case_sensitive=True)\n",
    "print get_neighborhood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens, contains_token\n",
    "\n",
    "LFs = []\n",
    "\n",
    "def LF_to_range(c):\n",
    "    return 1 if 'to' in get_right_tokens(c) else 0\n",
    "LFs.append(LF_to_range)\n",
    "\n",
    "def LF_tilde_range(c):\n",
    "    return 1 if '~' in get_right_tokens(c) else 0\n",
    "LFs.append(LF_tilde_range)\n",
    "\n",
    "def LF_contains_minus(c):\n",
    "    return 1 if contains_token(c, '-') or contains_token(c,'-50') else -1\n",
    "LFs.append(LF_contains_minus)\n",
    "\n",
    "# def LF_storage(m):\n",
    "#     return 1 if 'storage' in m.aligned_ngrams('words') else -1\n",
    "# LFs.append(LF_storage)\n",
    "\n",
    "# def LF_tstg(m):\n",
    "#     return 1 if 'tstg' in m.aligned_ngrams('words') else -1\n",
    "# LFs.append(LF_tstg)\n",
    "\n",
    "# def LF_tj(m):\n",
    "#     return 1 if 'tj' in m.aligned_ngrams('words') else -1\n",
    "# LFs.append(LF_tj)\n",
    "\n",
    "# def LF_temperature(m):\n",
    "#     return 1 if 'temperature' in m.aligned_ngrams('words') else -1\n",
    "# LFs.append(LF_temperature)\n",
    "\n",
    "# def LF_celsius(m):\n",
    "#     return 1 if 'c' in m.aligned_ngrams('words') else -1\n",
    "# LFs.append(LF_celsius)\n",
    "\n",
    "# def LF_max(m):\n",
    "#     return 1 if 'max' in m.aligned_ngrams('words') else 0\n",
    "# LFs.append(LF_max)\n",
    "\n",
    "# def LF_min(m):\n",
    "#     return 1 if 'min' in m.aligned_ngrams('words') else 0\n",
    "# LFs.append(LF_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train = label_manager.create(session, train, 'LF Labels', f=LFs)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# session.rollback()\n",
    "# session.delete(L_train)\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(L_train, n_iter=3000, rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_model.save(session, 'Generative Params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import LogReg\n",
    "\n",
    "disc_model = LogReg()\n",
    "disc_model.train(F_train, train_marginals, n_iter=5000, rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_model.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time disc_model.save(session, \"Discriminative Params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.update(session, dev, 'Train Features', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_dev = label_manager.load(session, dev, \"CDR Development Labels -- Gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_dev_set = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates -- Gold').one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.score(F_dev, L_dev, gold_dev_set)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
