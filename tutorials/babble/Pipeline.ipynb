{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'domain': 'drink',\n",
    "    'debug': True,\n",
    "    'postgres': False,\n",
    "    'parallelism': 1,\n",
    "    'splits': [0,1,2],\n",
    "    'disc_model_class': 'logreg',\n",
    "    'supervision': 'traditional',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$SNORKELDB = sqlite:///babble_drink_debug.db\n"
     ]
    }
   ],
   "source": [
    "# Get DB connection string and add to globals\n",
    "# NOTE: $SNORKELDB must be set before any snorkel imports\n",
    "import os\n",
    "\n",
    "default_db_name = 'babble_' + config['domain'] + ('_debug' if config.get('debug', False) else '')\n",
    "DB_NAME = config.get('db_name', default_db_name)\n",
    "if 'postgres' in config and config['postgres']:\n",
    "    DB_TYPE = 'postgres'\n",
    "else:\n",
    "    DB_TYPE = 'sqlite'\n",
    "    DB_NAME += '.db'\n",
    "DB_ADDR = \"localhost:{0}\".format(config['db_port']) if 'db_port' in config else \"\"\n",
    "os.environ['SNORKELDB'] = '{0}://{1}/{2}'.format(DB_TYPE, DB_ADDR, DB_NAME)\n",
    "print(\"$SNORKELDB = {0}\".format(os.environ['SNORKELDB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting splits=[0, 1] to splits=[0, 1, 2]\n",
      "Overwriting disc_model_class=inception_v3 to disc_model_class=logreg\n",
      "Overwriting epochs=[25, 50, 75] to epochs=[50]\n",
      "Overwriting step_size=[0.01, 0.001, 0.0001, 1e-05] to step_size=[1e-05]\n",
      "Overwriting reg_param=[0.0, 0.01, 0.1, 0.25, 0.5] to reg_param=[0.01]\n",
      "Overwriting decay=[0.9, 0.95, 0.99] to decay=[0.9]\n",
      "Overwriting LF_acc_prior_weight_default=1.0 to LF_acc_prior_weight_default=0.5\n",
      "Overwriting decay=0.95 to decay=0.9\n",
      "Overwriting epochs=50 to epochs=100\n",
      "Overwriting reg_param=0.1 to reg_param=0.01\n",
      "Overwriting domain=None to domain=drink\n",
      "Overwriting tune_b=True to tune_b=False\n",
      "Overwriting gen_model_search_space=10 to gen_model_search_space=1\n",
      "Overwriting disc_model_class=lstm to disc_model_class=logreg\n",
      "Overwriting supervision=generative to supervision=traditional\n",
      "Overwriting debug=False to debug=True\n",
      "Overwriting lr=[0.01, 0.001, 0.0001] to lr=[1, 50]\n",
      "NOTE: --debug=True: modifying parameters...\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Resolve config conflicts (nb_config > local_config > global_config)\n",
    "from snorkel.contrib.babble.pipelines import merge_configs, get_local_pipeline\n",
    "config = merge_configs(config)\n",
    "\n",
    "if config['debug']:\n",
    "    print(\"NOTE: --debug=True: modifying parameters...\")\n",
    "    config['max_docs'] = 100\n",
    "    config['gen_model_search_space'] = 2\n",
    "    config['disc_model_search_space'] = 2\n",
    "    config['gen_params_default']['epochs'] = 25\n",
    "    config['disc_params_default']['n_epochs'] = 5\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "candidate_class = candidate_subclass(config['candidate_name'], config['candidate_entities'])\n",
    "\n",
    "pipeline = get_local_pipeline(config['domain'])\n",
    "pipe = pipeline(session, candidate_class, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Running UDF...\n",
      "CPU times: user 20.7 s, sys: 860 ms, total: 21.5 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction was performed during parse stage.\n",
      "Candidates [Split 0]: 15711\n",
      "Candidates [Split 1]: 3377\n",
      "Candidates [Split 2]: 0\n",
      "CPU times: user 24 ms, sys: 24 ms, total: 48 ms\n",
      "Wall time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time pipe.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train CSV!\n",
      "Num HITs unique: 1525\n",
      "Num HITs total: 3050\n",
      "Unanimous: 4542\n",
      "Majority: 2211\n",
      "Bad: 450\n",
      "Reading val CSV!\n",
      "Num HITs unique: 184\n",
      "Num HITs total: 368\n",
      "Unanimous: 474\n",
      "Majority: 318\n",
      "Bad: 100\n",
      "AnnotatorLabels created: 5648\n",
      "AnnotatorLabels created: 634\n",
      "CPU times: user 1min, sys: 672 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.load_gold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time pipe.featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train CSV!\n",
      "Num HITs unique: 44\n",
      "Num HITs total: 132\n",
      "Unanimous: 112\n",
      "Majority: 54\n",
      "Bad: 37\n",
      "Building list of target candidate ids...\n",
      "Collected 139 unique target candidate ids from 370 explanations.\n",
      "Gathering desired candidates...\n",
      "Could not find 139 target candidates with the following stable_ids (first 5):\n",
      "0:266::bbox:14~~0:266::bbox:7\n",
      "0:379::bbox:1~~0:379::bbox:5\n",
      "0:724::bbox:21~~0:724::bbox:11\n",
      "0:518::bbox:4~~0:518::bbox:7\n",
      "0:731::bbox:2~~0:731::bbox:1\n",
      "Found 0/139 desired candidates\n",
      "Linking explanations to candidates...\n",
      "Linked 0/370 explanations\n",
      "Linking candidates...\n",
      "Building list of target candidate ids...\n",
      "Collected 139 unique target candidate ids from 370 explanations.\n",
      "Gathering desired candidates...\n",
      "Could not find 139 target candidates with the following stable_ids (first 5):\n",
      "0:266::bbox:14~~0:266::bbox:7\n",
      "0:379::bbox:1~~0:379::bbox:5\n",
      "0:724::bbox:21~~0:724::bbox:11\n",
      "0:518::bbox:4~~0:518::bbox:7\n",
      "0:731::bbox:2~~0:731::bbox:1\n",
      "Found 0/139 desired candidates\n",
      "Linking explanations to candidates...\n",
      "Linked 0/370 explanations\n",
      "Calling babbler...\n",
      "WARNING: Number of candidates (3377) does not equal the number of pos (168) + neg (466) = 634 labels.\n",
      "Flushing all parses from previous explanation set.\n",
      "Created grammar with 453 rules\n",
      "121 explanation(s) out of 370 were parseable.\n",
      "171 parse(s) generated from 370 explanation(s).\n",
      "49 parse(s) remain (122 parse(s) removed by DuplicateSemanticsFilter).\n",
      "Note: 49 LFs did not have candidates and therefore could not be filtered.\n",
      "49 parse(s) remain (0 parse(s) removed by ConsistencyFilter).\n",
      "### Applying labeling functions to split 1\n",
      "[========================================] 100%\n",
      "\n",
      "### Done in 4.9s.\n",
      "\n",
      "31 parse(s) remain (18 parse(s) removed by UniformSignatureFilter: (12 None, 6 All)).\n",
      "20 parse(s) remain (11 parse(s) removed by DuplicateSignatureFilter).\n",
      "Added 20 parse(s) from 18 explanations to set. (Total # parses = 20)\n",
      "CPU times: user 13.3 s, sys: 1.38 s, total: 14.7 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 'traditional' supervision mode...skipping 'label' stage.\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 317 µs\n"
     ]
    }
   ],
   "source": [
    "%time pipe.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 'traditional' supervision mode...skipping 'supervise' stage.\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 303 µs\n"
     ]
    }
   ],
   "source": [
    "%time pipe.supervise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 'traditional' supervision mode...grabbing candidate and gold label subsets.\n",
      "Train size: 858\n",
      "Dev size: 78\n",
      "Test size: 118\n",
      "Assuming MSCOCO data is already downloaded and converted (download_data = False).\n",
      "Starting training over space of 2 configurations\n",
      "\n",
      "Configuration 0.\n",
      "Running the following configuration:\n",
      "dim: 128\n",
      "dropout: 0.25\n",
      "lr: 50\n",
      "max_steps: 1000\n",
      "rebalance: 0.5\n",
      "weight_decay: 0.0001\n",
      "Calling TFSlim train...\n",
      "Calling TFSlim eval on validation...\n",
      "> /afs/cs.stanford.edu/u/paroma/snorkel_new/babble_snorkel/snorkel/contrib/babble/pipelines/image_pipeline.py(236)classify()\n",
      "-> accuracy, precision, recall = scrape_output(output_file)\n",
      "(Pdb) scrape_output(output_file)\n",
      "*** UnboundLocalError: local variable 'accuracy' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "config['display_marginals'] = False\n",
    "config['download_data'] = False\n",
    "config['disc_model_class'] = 'inception_v3'\n",
    "    \n",
    "%time pipe.classify(slim_ws_path='/dfs/scratch0/paroma/slim_new/slim_ws/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
