{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'domain': 'protein',\n",
    "    'postgres': False,\n",
    "    'debug': False,\n",
    "    'babbler_candidate_split': 1,\n",
    "    'babbler_label_split': 1,\n",
    "    'supervision': 'generative',\n",
    "    'disc_model_class': 'logreg',    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$SNORKELDB = sqlite:///babble_protein.db\n"
     ]
    }
   ],
   "source": [
    "# Get DB connection string and add to globals\n",
    "# NOTE: $SNORKELDB must be set before any snorkel imports\n",
    "import os\n",
    "\n",
    "default_db_name = 'babble_' + config['domain'] + ('_debug' if config.get('debug', False) else '')\n",
    "DB_NAME = config.get('db_name', default_db_name)\n",
    "if 'postgres' in config and config['postgres']:\n",
    "    DB_TYPE = 'postgres'\n",
    "else:\n",
    "    DB_TYPE = 'sqlite'\n",
    "    DB_NAME += '.db'\n",
    "DB_ADDR = \"localhost:{0}\".format(config['db_port']) if 'db_port' in config else \"\"\n",
    "os.environ['SNORKELDB'] = '{0}://{1}/{2}'.format(DB_TYPE, DB_ADDR, DB_NAME)\n",
    "print(\"$SNORKELDB = {0}\".format(os.environ['SNORKELDB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting domain=None to domain=protein\n",
      "Overwriting disc_model_class=lstm to disc_model_class=logreg\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Resolve config conflicts (nb_config > local_config > global_config)\n",
    "from snorkel.contrib.babble.pipelines import merge_configs, get_local_pipeline\n",
    "config = merge_configs(config)\n",
    "\n",
    "if config['debug']:\n",
    "    print(\"NOTE: --debug=True: modifying parameters...\")\n",
    "    config['max_docs'] = 100\n",
    "    config['gen_model_search_space'] = 1\n",
    "    config['disc_model_search_space'] = 1\n",
    "    config['gen_params_default']['epochs'] = 25\n",
    "    config['disc_params_default']['n_epochs'] = 5\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "candidate_class = candidate_subclass(config['candidate_name'], config['candidate_entities'])\n",
    "\n",
    "pipeline = get_local_pipeline(config['domain'])\n",
    "pipe = pipeline(session, candidate_class, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Documents: 4735\n",
      "Sentences: 43386\n",
      "CPU times: user 1min 8s, sys: 2.09 s, total: 1min 10s\n",
      "Wall time: 11min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not kill CoreNLP server [23544] [Errno 3] No such process\n"
     ]
    }
   ],
   "source": [
    "%time pipe.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Candidates [Split 0]: 7011\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Candidates [Split 1]: 1168\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Candidates [Split 2]: 1203\n",
      "CPU times: user 45 s, sys: 979 ms, total: 46 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 0\n",
      "AnnotatorLabels created: 1168\n",
      "AnnotatorLabels created: 1203\n",
      "CPU times: user 52.3 s, sys: 765 ms, total: 53.1 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.load_gold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Featurized split 0: (7011,117650) sparse (nnz = 365530)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Featurized split 1: (1168,117650) sparse (nnz = 39454)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Featurized split 2: (1203,117650) sparse (nnz = 40178)\n",
      "CPU times: user 8min 10s, sys: 5.23 s, total: 8min 15s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed or header row 23...\n",
      "Linking candidates...\n",
      "Building list of target candidate ids...\n",
      "Collected 29 unique target candidate ids from 56 explanations.\n",
      "Gathering desired candidates...\n",
      "Found 29/29 desired candidates\n",
      "Linking explanations to candidates...\n",
      "Linked 56/56 explanations\n",
      "Calling babbler...\n",
      "Flushing all parses from previous explanation set.\n",
      "Created grammar with 593 rules\n",
      "54 explanation(s) out of 56 were parseable.\n",
      "142 parse(s) generated from 56 explanation(s).\n",
      "46 parse(s) remain (96 parse(s) removed by DuplicateSemanticsFilter).\n",
      "23 parse(s) remain (23 parse(s) removed by ConsistencyFilter).\n",
      "### Applying labeling functions to split 1\n",
      "[========================================] 100%\n",
      "\n",
      "### Done in 2.5s.\n",
      "\n",
      "23 parse(s) remain (0 parse(s) removed by UniformSignatureFilter: (0 None, 0 All)).\n",
      "21 parse(s) remain (2 parse(s) removed by DuplicateSignatureFilter).\n",
      "Added 21 parse(s) from 21 explanations to set. (Total # parses = 21)\n",
      "CPU times: user 9.58 s, sys: 586 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY\n",
      "123 TOTAL:\n",
      "2 Unparseable Explanation\n",
      "96 Duplicate Semantics\n",
      "23 Inconsistency with Example\n",
      "0 Uniform Signature\n",
      "2 Duplicate Signature\n",
      "\n",
      "[#1]: Unparseable Explanation\n",
      "\n",
      "Explanation: because word 'cause an accumulation of' is between them\n",
      "\n",
      "Reason: This explanation couldn't be parsed.\n",
      "\n",
      "\n",
      "[#2]: Unparseable Explanation\n",
      "\n",
      "Explanation: because word 'cause an accumulation of' is before the Protein\n",
      "\n",
      "Reason: This explanation couldn't be parsed.\n",
      "\n",
      "\n",
      "[#3]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'substrate of'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is between them \"\n",
      "\n",
      "\n",
      "[#4]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'substrate of'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is before the kinase\"\n",
      "\n",
      "\n",
      "[#5]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'substrate of'.in(text(greater than 0 word(s) to the left of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is before the kinase\"\n",
      "\n",
      "\n",
      "[#6]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interaction of'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interaction of' is before the kinase followed by the Protein\"\n",
      "\n",
      "\n",
      "[#7]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interaction of'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interaction of' is before the kinase followed by the Protein\"\n",
      "\n",
      "\n",
      "[#8]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interaction of'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interaction of' is before the kinase followed by the Protein\"\n",
      "\n",
      "\n",
      "[#9]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'phosphorylate'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'phosphorylate' is between them \"\n",
      "\n",
      "\n",
      "[#10]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'phosphorylate'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'phosphorylate' is after the kinase\"\n",
      "\n",
      "\n",
      "[#11]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'phosphorylate'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'phosphorylate' is after the kinase\"\n",
      "\n",
      "\n",
      "[#12]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'not alter'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'not able to alter' is between them \"\n",
      "\n",
      "\n",
      "[#13]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'co - localization'.(all([in(text(greater than 0 word(s) to the right of X)),in(text(greater than 0 word(s) to the right of Y))])) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'co-localization' is after them \"\n",
      "\n",
      "\n",
      "[#14]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'co - localization'.(all([in(text(greater than 0 word(s) to the right of X)),in(text(greater than 0 word(s) to the right of Y))])) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'co-localization' is after them \"\n",
      "\n",
      "\n",
      "[#15]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'co - localization'.(all([in(text(greater than 0 word(s) to the right of X)),in(text(greater than 0 word(s) to the right of Y))])) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'co-localization' is after them \"\n",
      "\n",
      "\n",
      "[#16]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'substrate of'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is between them \"\n",
      "\n",
      "\n",
      "[#17]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'substrate of'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is between them \"\n",
      "\n",
      "\n",
      "[#18]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'substrate of'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is before the kinase\"\n",
      "\n",
      "\n",
      "[#19]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'substrate of'.in(text(greater than 0 word(s) to the left of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is before the kinase\"\n",
      "\n",
      "\n",
      "[#20]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'substrate of'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is before the kinase\"\n",
      "\n",
      "\n",
      "[#21]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'substrate of'.in(text(greater than 0 word(s) to the left of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'substrate of' is before the kinase\"\n",
      "\n",
      "\n",
      "[#22]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'as'.(= 'kinase_arg 2') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'as' is after entity \"kinase_LRRK2\" and implies new context\"\n",
      "\n",
      "\n",
      "[#23]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'as'.(= 'kinase_arg 2') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'as' is after entity \"kinase_LRRK2\" and implies new context\"\n",
      "\n",
      "\n",
      "[#24]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'as'.(= 'kinase_arg 2') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'as' is after entity \"kinase_LRRK2\" and implies new context\"\n",
      "\n",
      "\n",
      "[#25]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interaction between'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interaction between' is before the kinase followed by the protein\"\n",
      "\n",
      "\n",
      "[#26]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interaction between'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interaction between' is before the kinase followed by the protein\"\n",
      "\n",
      "\n",
      "[#27]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interaction between'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interaction between' is before the kinase followed by the protein\"\n",
      "\n",
      "\n",
      "[#28]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is between them\"\n",
      "\n",
      "\n",
      "[#29]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is between them\"\n",
      "\n",
      "\n",
      "[#30]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is between them\"\n",
      "\n",
      "\n",
      "[#31]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#32]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#33]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#34]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#35]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#36]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#37]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#38]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#39]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'interacts with' is after the Kinase and followed by combination of words 'three beta-tubulin isoforms' before the Protein\"\n",
      "\n",
      "\n",
      "[#40]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'one of which'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of punctuation ',' andword 'one of which' is before the Protein and implies new context\"\n",
      "\n",
      "\n",
      "[#41]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'one of which'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of punctuation ',' andword 'one of which' is before the Protein and implies new context\"\n",
      "\n",
      "\n",
      "[#42]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if all([s.(= text(X)) for s in [',','one of which']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of punctuation ',' and word 'one of which' is before the Protein and implies new context\"\n",
      "\n",
      "\n",
      "[#43]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'localization to'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is after the kinase\"\n",
      "\n",
      "\n",
      "[#44]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'localization to'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is after the kinase\"\n",
      "\n",
      "\n",
      "[#45]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'localization to'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is before the Protein\"\n",
      "\n",
      "\n",
      "[#46]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'localization to'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is before the Protein\"\n",
      "\n",
      "\n",
      "[#47]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'localized to'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of words 'localized to' after the kinase and 'rather than' before the Protein\"\n",
      "\n",
      "\n",
      "[#48]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'localization to'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is before the Protein\"\n",
      "\n",
      "\n",
      "[#49]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'localization to'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is before the Protein\"\n",
      "\n",
      "\n",
      "[#50]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'localization to'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is before the Protein\"\n",
      "\n",
      "\n",
      "[#51]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'localization to'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'localization to' is before the Protein\"\n",
      "\n",
      "\n",
      "[#52]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if any([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because a new condition of the kinase is described by word 'knock-out mice' after the kinase\"\n",
      "\n",
      "\n",
      "[#53]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if sum([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]).(>= 1) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because a new condition of the kinase is described by word 'knock-out mice' after the kinase\"\n",
      "\n",
      "\n",
      "[#54]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if any([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because a new condition of the kinase is described by word 'knock-out mice' after the kinase\"\n",
      "\n",
      "\n",
      "[#55]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if sum([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]).(>= 1) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because a new condition of the kinase is described by word 'knock-out mice' after the kinase\"\n",
      "\n",
      "\n",
      "[#56]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if any([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because a new condition of the kinase is described by word 'knock-out mice' after the kinase\"\n",
      "\n",
      "\n",
      "[#57]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if sum([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]).(>= 1) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because a new condition of the kinase is described by word 'knock-out mice' after the kinase\"\n",
      "\n",
      "\n",
      "[#58]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if ('increased'.in(text(between([X,Y]))) and 'acetylation'.in(text(greater than 0 word(s) to the right of X))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of words 'increased' is between them and 'acetylation' after the protein\"\n",
      "\n",
      "\n",
      "[#59]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'increased'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'increased' is before the Protein\"\n",
      "\n",
      "\n",
      "[#60]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'increased'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'increased' is before the Protein\"\n",
      "\n",
      "\n",
      "[#61]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if '-'.(= 'interaction') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because punctuation character '-' is between them followed by word 'interaction'\"\n",
      "\n",
      "\n",
      "[#62]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if '-'.(= 'interaction') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because punctuation character '-' is between them followed by word 'interaction'\"\n",
      "\n",
      "\n",
      "[#63]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if '-'.(= 'interaction') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because punctuation character '-' is between them followed by word 'interaction'\"\n",
      "\n",
      "\n",
      "[#64]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if all([s.in(text(between([X,Y]))) for s in [',','and indicates']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of punctuation ',' and word 'and indicates' is between entities and implies new context\"\n",
      "\n",
      "\n",
      "[#65]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if all([s.in(text(between([X,Y]))) for s in [',','and indicates']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of punctuation ',' and word 'and indicates' is between entities and implies new context\"\n",
      "\n",
      "\n",
      "[#66]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if all([s.in(text(between([X,Y]))) for s in [',','and indicates']]) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because combination of punctuation ',' and word 'and indicates' is between entities and implies new context\"\n",
      "\n",
      "\n",
      "[#67]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'and indicates'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'and indicates' is between entities and implies new context\"\n",
      "\n",
      "\n",
      "[#68]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'and indicates'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'and indicates' is between entities and implies new context\"\n",
      "\n",
      "\n",
      "[#69]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'and indicates'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'and indicates' is between entities and implies new context\"\n",
      "\n",
      "\n",
      "[#70]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'caused by'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'caused by' is between them\"\n",
      "\n",
      "\n",
      "[#71]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'caused by'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'caused by' is after the protein\"\n",
      "\n",
      "\n",
      "[#72]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'caused by'.in(text(greater than 0 word(s) to the right of X)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'caused by' is after the protein\"\n",
      "\n",
      "\n",
      "[#73]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'regulating'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is between them\"\n",
      "\n",
      "\n",
      "[#74]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#75]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#76]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'regulating'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is between them\"\n",
      "\n",
      "\n",
      "[#77]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'regulating'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is between them\"\n",
      "\n",
      "\n",
      "[#78]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#79]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#80]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#81]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#82]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'regulating'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is between them\"\n",
      "\n",
      "\n",
      "[#83]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'regulating'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is between them\"\n",
      "\n",
      "\n",
      "[#84]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#85]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#86]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#87]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'regulating'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'regulating' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#88]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'impairs'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'impairs' is between them\"\n",
      "\n",
      "\n",
      "[#89]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'impairs'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'impairs' is after the kinase\"\n",
      "\n",
      "\n",
      "[#90]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'impairs'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'impairs' is after the kinase\"\n",
      "\n",
      "\n",
      "[#91]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'affect'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'affect' is between them\"\n",
      "\n",
      "\n",
      "[#92]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'affect'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'affect' is after the kinase\"\n",
      "\n",
      "\n",
      "[#93]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'affect'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'affect' is after the kinase\"\n",
      "\n",
      "\n",
      "[#94]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'decreasing'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'decreasing' is between them \"\n",
      "\n",
      "\n",
      "[#95]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'decreasing'.(= 'entitiy : Protein_arg 1') else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'decreasing' is before \"entitiy: Protein_DRD2\"\"\n",
      "\n",
      "\n",
      "[#96]: Duplicate Semantics\n",
      "\n",
      "Parse: return 1 if 'influences'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'influences' is between them\"\n",
      "\n",
      "\n",
      "[#97]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'influences'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'influences' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#98]: Duplicate Semantics\n",
      "\n",
      "Parse: return -1 if 'influences'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse is identical to one produced by the following explanation:\n",
      "\t\"because word 'influences' is after the Kinase\"\n",
      "\n",
      "\n",
      "[#99]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'substrate of'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (4E-BP1, LRRK2)\n",
      "\n",
      "\n",
      "[#100]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'phosphorylate'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (4E-BP1, LRRK2)\n",
      "\n",
      "\n",
      "[#101]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'not alter'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (4E-BP1, LRRK2)\n",
      "\n",
      "\n",
      "[#102]: Inconsistency with Example\n",
      "\n",
      "Parse: return 1 if 'co - localization'.(all([in(text(greater than 0 word(s) to the right of X)),in(text(greater than 0 word(s) to the right of Y))])) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (4E-BP1, LRRK2)\n",
      "\n",
      "\n",
      "[#103]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'as'.(= 'kinase_arg 2') else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (Tau, LRRK2)\n",
      "\n",
      "\n",
      "[#104]: Inconsistency with Example\n",
      "\n",
      "Parse: return 1 if 'interacts with'.(= 'three isoforms'[0]) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (TUBB, LRRK2)\n",
      "\n",
      "\n",
      "[#105]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'one of which'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (TUBB4, LRRK2)\n",
      "\n",
      "\n",
      "[#106]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if all([s.(= text(X)) for s in [',','one of which']]) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (TUBB4, LRRK2)\n",
      "\n",
      "\n",
      "[#107]: Inconsistency with Example\n",
      "\n",
      "Parse: return 1 if 'localization to'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubules, LRRK2)\n",
      "\n",
      "\n",
      "[#108]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'localization to'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubules, LRRK2)\n",
      "\n",
      "\n",
      "[#109]: Inconsistency with Example\n",
      "\n",
      "Parse: return 1 if 'localized to'.in(text(greater than 0 word(s) to the right of Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubule, LRRK2)\n",
      "\n",
      "\n",
      "[#110]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if any([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubule, LRRK2)\n",
      "\n",
      "\n",
      "[#111]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if sum([s.in(text(greater than 0 word(s) to the right of Y)) for s in [text(Y),'knock - out mice']]).(>= 1) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubule, LRRK2)\n",
      "\n",
      "\n",
      "[#112]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'increased'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubule, LRRK2)\n",
      "\n",
      "\n",
      "[#113]: Inconsistency with Example\n",
      "\n",
      "Parse: return 1 if '-'.(= 'interaction') else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (tubulin, LRRK2)\n",
      "\n",
      "\n",
      "[#114]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if all([s.in(text(between([X,Y]))) for s in [',','and indicates']]) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubule, LRRK2)\n",
      "\n",
      "\n",
      "[#115]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'and indicates'.in(text(between([X,Y]))) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (tubulin, LRRK2)\n",
      "\n",
      "\n",
      "[#116]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'caused by'.(= text(X)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (microtubule, LRRK2)\n",
      "\n",
      "\n",
      "[#117]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'regulating'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (receptor, LRRK2)\n",
      "\n",
      "\n",
      "[#118]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'impairs'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (DRD1, LRRK2)\n",
      "\n",
      "\n",
      "[#119]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'affect'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (receptor, LRRK2)\n",
      "\n",
      "\n",
      "[#120]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'decreasing'.(= 'entitiy : Protein_arg 1') else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (DRD2, LRRK2)\n",
      "\n",
      "\n",
      "[#121]: Inconsistency with Example\n",
      "\n",
      "Parse: return -1 if 'influences'.(= text(Y)) else 0\n",
      "\n",
      "Reason: This parse did not agree with the candidate (receptor, LRRK2)\n",
      "\n",
      "\n",
      "[#122]: Duplicate Signature\n",
      "\n",
      "Parse: return -1 if 'substrate of'.in(text(greater than 0 word(s) to the left of Y)) else 0\n",
      "\n",
      "Reason: This parse labeled identically to the following existing parse:\n",
      "\tExplanation(\"Explanation0: True, because word 'substrate of' is between them \")\n",
      "\n",
      "\n",
      "[#123]: Duplicate Signature\n",
      "\n",
      "Parse: return -1 if 'localization to'.in(text(greater than 0 word(s) to the left of X)) else 0\n",
      "\n",
      "Reason: This parse labeled identically to the following existing parse:\n",
      "\tExplanation(\"Explanation26: True, because word 'localization to' is after the kinase\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.babbler.filtered_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Labeled split 0: (7011,21) sparse (nnz = 866)\n",
      "\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Labeled split 1: (1168,21) sparse (nnz = 290)\n",
      "\n",
      "                  j  Coverage  Overlaps  Conflicts  TP  FP  FN  TN  \\\n",
      "Explanation0_0    0  0.006849  0.001712   0.000856   4   4   0   0   \n",
      "Explanation2_0    1  0.006849  0.000856   0.000856   4   4   0   0   \n",
      "Explanation4_0    2  0.047945  0.035959   0.035103  19  37   0   0   \n",
      "Explanation5_1    3  0.044521  0.035959   0.035959   0   0  17  35   \n",
      "Explanation15_0   4  0.017123  0.000000   0.000000  12   8   0   0   \n",
      "Explanation17_0   5  0.020548  0.001712   0.001712  14  10   0   0   \n",
      "Explanation24_1   6  0.000856  0.000856   0.000856   0   0   0   1   \n",
      "Explanation26_1   7  0.001712  0.000000   0.000000   0   2   0   0   \n",
      "Explanation32_0   8  0.000856  0.000856   0.000856   0   1   0   0   \n",
      "Explanation33_1   9  0.041096  0.005993   0.005137   0   0   7  41   \n",
      "Explanation40_0  10  0.002568  0.002568   0.002568   0   3   0   0   \n",
      "Explanation41_1  11  0.004281  0.002568   0.002568   0   0   1   4   \n",
      "Explanation42_0  12  0.005993  0.005993   0.005993   5   2   0   0   \n",
      "Explanation43_1  13  0.009418  0.006849   0.005993   0   0   5   6   \n",
      "Explanation48_0  14  0.000856  0.000856   0.000856   0   1   0   0   \n",
      "Explanation49_1  15  0.002568  0.000856   0.000856   0   0   1   2   \n",
      "Explanation50_0  16  0.009418  0.006849   0.006849   1  10   0   0   \n",
      "Explanation51_1  17  0.017979  0.006849   0.006849   0   0   1  20   \n",
      "Explanation52_0  18  0.000856  0.000856   0.000856   0   1   0   0   \n",
      "Explanation54_0  19  0.002568  0.002568   0.002568   2   1   0   0   \n",
      "Explanation55_1  20  0.003425  0.002568   0.002568   0   0   2   2   \n",
      "\n",
      "                 Empirical Acc.  \n",
      "Explanation0_0         0.500000  \n",
      "Explanation2_0         0.500000  \n",
      "Explanation4_0         0.339286  \n",
      "Explanation5_1         0.673077  \n",
      "Explanation15_0        0.600000  \n",
      "Explanation17_0        0.583333  \n",
      "Explanation24_1        1.000000  \n",
      "Explanation26_1        0.000000  \n",
      "Explanation32_0        0.000000  \n",
      "Explanation33_1        0.854167  \n",
      "Explanation40_0        0.000000  \n",
      "Explanation41_1        0.800000  \n",
      "Explanation42_0        0.714286  \n",
      "Explanation43_1        0.545455  \n",
      "Explanation48_0        0.000000  \n",
      "Explanation49_1        0.666667  \n",
      "Explanation50_0        0.090909  \n",
      "Explanation51_1        0.952381  \n",
      "Explanation52_0        0.000000  \n",
      "Explanation54_0        0.666667  \n",
      "Explanation55_1        0.500000  \n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Labeled split 2: (1203,21) sparse (nnz = 326)\n",
      "\n",
      "CPU times: user 51.4 s, sys: 502 ms, total: 51.9 s\n",
      "Wall time: 51.9 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L_train: <7011x21 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 866 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_train: <7011x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 0.0%\n",
      "\n",
      "Using L_dev: <1168x21 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 290 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_dev: <1168x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 1168 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 19.2%\n",
      "\n",
      "Using L_test: <1203x21 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 326 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_test: <1203x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 1203 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 19.0%\n",
      "\n",
      "============================================================\n",
      "[1] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-02, reg_param = 1.00e-01, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.261146496815\n",
      "[GenerativeModel] Model saved as <GenerativeModel_0>.\n",
      "============================================================\n",
      "[2] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-05, reg_param = 5.00e-01, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.245033112583\n",
      "============================================================\n",
      "[3] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-03, reg_param = 0.00e+00, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.27027027027\n",
      "[GenerativeModel] Model saved as <GenerativeModel_2>.\n",
      "============================================================\n",
      "[4] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-02, reg_param = 0.00e+00, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.257425742574\n",
      "============================================================\n",
      "[5] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-05, reg_param = 5.00e-01, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.245033112583\n",
      "============================================================\n",
      "[6] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-05, reg_param = 1.00e-01, decay = 9.50e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.245033112583\n",
      "============================================================\n",
      "[7] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-05, reg_param = 1.00e-02, decay = 9.90e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.337142857143\n",
      "[GenerativeModel] Model saved as <GenerativeModel_6>.\n",
      "============================================================\n",
      "[8] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-05, reg_param = 0.00e+00, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.24584717608\n",
      "============================================================\n",
      "[9] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-03, reg_param = 1.00e-02, decay = 9.90e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.331460674157\n",
      "============================================================\n",
      "[10] Testing LF_acc_prior_weight_default = 1.00e+00, step_size = 1.00e-05, reg_param = 1.00e-02, decay = 9.00e-01\n",
      "============================================================\n",
      "Inferred cardinality: 2\n",
      "[GenerativeModel] F-1 Score: 0.24584717608\n",
      "[GenerativeModel] Model <GenerativeModel_6> loaded.\n",
      "   LF_acc_prior_weight_default  step_size  reg_param  decay     Prec.  \\\n",
      "6                          1.0    0.00001       0.01   0.99  0.468254   \n",
      "8                          1.0    0.00100       0.01   0.99  0.446970   \n",
      "2                          1.0    0.00100       0.00   0.95  0.412844   \n",
      "0                          1.0    0.01000       0.10   0.95  0.455556   \n",
      "3                          1.0    0.01000       0.00   0.90  0.288889   \n",
      "7                          1.0    0.00001       0.00   0.90  0.480519   \n",
      "9                          1.0    0.00001       0.01   0.90  0.480519   \n",
      "1                          1.0    0.00001       0.50   0.90  0.474359   \n",
      "4                          1.0    0.00001       0.50   0.90  0.474359   \n",
      "5                          1.0    0.00001       0.10   0.95  0.474359   \n",
      "\n",
      "       Rec.       F-1  \n",
      "6  0.263393  0.337143  \n",
      "8  0.263393  0.331461  \n",
      "2  0.200893  0.270270  \n",
      "0  0.183036  0.261146  \n",
      "3  0.232143  0.257426  \n",
      "7  0.165179  0.245847  \n",
      "9  0.165179  0.245847  \n",
      "1  0.165179  0.245033  \n",
      "4  0.165179  0.245033  \n",
      "5  0.165179  0.245033  \n",
      "[GenerativeModel] Model saved as <generative_protein>.\n",
      "\n",
      "Gen. model (DP) score on dev set (b=0.5):\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.263\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.468\n",
      "Recall               0.263\n",
      "F1                   0.337\n",
      "----------------------------------------\n",
      "TP: 59 | FP: 67 | TN: 877 | FN: 165\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktTFwJAkdTEwJEld/hZqBoeJ67KtgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16ab279d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7011 marginals\n",
      "CPU times: user 4min, sys: 1.56 s, total: 4min 2s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.supervise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7011,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktTFwJAkdTEwJEld/hZqBoeJ67KtgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126ea0050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [7.1] Begin training discriminative model\n",
      "============================================================\n",
      "[1] Testing dim = 64, dropout = 2.50e-01, rebalance = 0.00e+00, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=684  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.752066\tDev F1=25.65\n",
      "[SparseLogisticRegression] Epoch 1 (0.13s)\tAverage loss=0.734316\tDev F1=25.29\n",
      "[SparseLogisticRegression] Epoch 2 (0.20s)\tAverage loss=0.722492\tDev F1=25.45\n",
      "[SparseLogisticRegression] Epoch 3 (0.28s)\tAverage loss=0.713663\tDev F1=25.61\n",
      "[SparseLogisticRegression] Epoch 4 (0.36s)\tAverage loss=0.707419\tDev F1=25.69\n",
      "[SparseLogisticRegression] Epoch 5 (0.43s)\tAverage loss=0.703589\tDev F1=25.75\n",
      "[SparseLogisticRegression] Epoch 6 (0.51s)\tAverage loss=0.700732\tDev F1=25.51\n",
      "[SparseLogisticRegression] Epoch 7 (0.59s)\tAverage loss=0.698286\tDev F1=24.84\n",
      "[SparseLogisticRegression] Epoch 8 (0.66s)\tAverage loss=0.696790\tDev F1=24.81\n",
      "[SparseLogisticRegression] Epoch 9 (0.74s)\tAverage loss=0.695366\tDev F1=25.06\n",
      "[SparseLogisticRegression] Epoch 10 (0.82s)\tAverage loss=0.694629\tDev F1=24.74\n",
      "[SparseLogisticRegression] Epoch 11 (0.89s)\tAverage loss=0.693825\tDev F1=24.40\n",
      "[SparseLogisticRegression] Epoch 12 (0.97s)\tAverage loss=0.693179\tDev F1=24.55\n",
      "[SparseLogisticRegression] Epoch 13 (1.04s)\tAverage loss=0.692816\tDev F1=24.74\n",
      "[SparseLogisticRegression] Epoch 14 (1.12s)\tAverage loss=0.692374\tDev F1=24.49\n",
      "[SparseLogisticRegression] Epoch 15 (1.20s)\tAverage loss=0.691951\tDev F1=24.78\n",
      "[SparseLogisticRegression] Epoch 16 (1.27s)\tAverage loss=0.691514\tDev F1=24.74\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.55s)\tAverage loss=0.691443\tDev F1=24.81\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (1.84s)\tAverage loss=0.691182\tDev F1=24.39\n",
      "[SparseLogisticRegression] Epoch 19 (1.92s)\tAverage loss=0.690913\tDev F1=24.61\n",
      "[SparseLogisticRegression] Training done (1.95s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.248081841432\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "============================================================\n",
      "[2] Testing dim = 128, dropout = 5.00e-01, rebalance = 2.50e-01, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=553  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.742039\tDev F1=27.29\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.740238\tDev F1=27.49\n",
      "[SparseLogisticRegression] Epoch 2 (0.17s)\tAverage loss=0.739567\tDev F1=27.52\n",
      "[SparseLogisticRegression] Epoch 3 (0.24s)\tAverage loss=0.737452\tDev F1=27.55\n",
      "[SparseLogisticRegression] Epoch 4 (0.31s)\tAverage loss=0.739122\tDev F1=27.62\n",
      "[SparseLogisticRegression] Epoch 5 (0.37s)\tAverage loss=0.737187\tDev F1=27.42\n",
      "[SparseLogisticRegression] Epoch 6 (0.44s)\tAverage loss=0.734161\tDev F1=27.45\n",
      "[SparseLogisticRegression] Epoch 7 (0.51s)\tAverage loss=0.731347\tDev F1=27.22\n",
      "[SparseLogisticRegression] Epoch 8 (0.58s)\tAverage loss=0.734201\tDev F1=27.25\n",
      "[SparseLogisticRegression] Epoch 9 (0.65s)\tAverage loss=0.733420\tDev F1=27.31\n",
      "[SparseLogisticRegression] Epoch 10 (0.72s)\tAverage loss=0.730211\tDev F1=27.35\n",
      "[SparseLogisticRegression] Epoch 11 (0.79s)\tAverage loss=0.730404\tDev F1=27.41\n",
      "[SparseLogisticRegression] Epoch 12 (0.86s)\tAverage loss=0.728373\tDev F1=27.24\n",
      "[SparseLogisticRegression] Epoch 13 (0.93s)\tAverage loss=0.728172\tDev F1=27.24\n",
      "[SparseLogisticRegression] Epoch 14 (1.00s)\tAverage loss=0.727404\tDev F1=27.47\n",
      "[SparseLogisticRegression] Epoch 15 (1.07s)\tAverage loss=0.724523\tDev F1=27.47\n",
      "[SparseLogisticRegression] Epoch 16 (1.13s)\tAverage loss=0.724337\tDev F1=27.67\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.41s)\tAverage loss=0.726144\tDev F1=27.87\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (1.88s)\tAverage loss=0.721913\tDev F1=27.94\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 19 (2.20s)\tAverage loss=0.722954\tDev F1=27.97\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.49s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.27972027972\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_1>\n",
      "============================================================\n",
      "[3] Testing dim = 128, dropout = 1.00e-01, rebalance = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=538  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.746535\tDev F1=27.39\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.770549\tDev F1=30.81\n",
      "[SparseLogisticRegression] Epoch 2 (0.18s)\tAverage loss=0.725461\tDev F1=30.94\n",
      "[SparseLogisticRegression] Epoch 3 (0.25s)\tAverage loss=0.729190\tDev F1=28.65\n",
      "[SparseLogisticRegression] Epoch 4 (0.32s)\tAverage loss=0.715969\tDev F1=28.75\n",
      "[SparseLogisticRegression] Epoch 5 (0.39s)\tAverage loss=0.714602\tDev F1=28.82\n",
      "[SparseLogisticRegression] Epoch 6 (0.45s)\tAverage loss=0.710200\tDev F1=28.96\n",
      "[SparseLogisticRegression] Epoch 7 (0.53s)\tAverage loss=0.703797\tDev F1=30.59\n",
      "[SparseLogisticRegression] Epoch 8 (0.59s)\tAverage loss=0.705537\tDev F1=30.30\n",
      "[SparseLogisticRegression] Epoch 9 (0.66s)\tAverage loss=0.705665\tDev F1=29.75\n",
      "[SparseLogisticRegression] Epoch 10 (0.73s)\tAverage loss=0.701255\tDev F1=29.40\n",
      "[SparseLogisticRegression] Epoch 11 (0.80s)\tAverage loss=0.696831\tDev F1=29.67\n",
      "[SparseLogisticRegression] Epoch 12 (0.87s)\tAverage loss=0.701461\tDev F1=29.23\n",
      "[SparseLogisticRegression] Epoch 13 (0.94s)\tAverage loss=0.701750\tDev F1=28.10\n",
      "[SparseLogisticRegression] Epoch 14 (1.01s)\tAverage loss=0.700511\tDev F1=28.91\n",
      "[SparseLogisticRegression] Epoch 15 (1.08s)\tAverage loss=0.698000\tDev F1=29.27\n",
      "[SparseLogisticRegression] Epoch 16 (1.15s)\tAverage loss=0.706017\tDev F1=28.06\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.60s)\tAverage loss=0.696410\tDev F1=29.32\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (1.89s)\tAverage loss=0.698330\tDev F1=28.84\n",
      "[SparseLogisticRegression] Epoch 19 (1.95s)\tAverage loss=0.701730\tDev F1=29.29\n",
      "[SparseLogisticRegression] Training done (1.99s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.293193717277\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_2>\n",
      "============================================================\n",
      "[4] Testing dim = 64, dropout = 5.00e-01, rebalance = 5.00e-01, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=538  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.766582\tDev F1=26.91\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.763305\tDev F1=26.90\n",
      "[SparseLogisticRegression] Epoch 2 (0.18s)\tAverage loss=0.761705\tDev F1=26.90\n",
      "[SparseLogisticRegression] Epoch 3 (0.25s)\tAverage loss=0.763184\tDev F1=26.90\n",
      "[SparseLogisticRegression] Epoch 4 (0.32s)\tAverage loss=0.758380\tDev F1=26.79\n",
      "[SparseLogisticRegression] Epoch 5 (0.39s)\tAverage loss=0.758312\tDev F1=26.96\n",
      "[SparseLogisticRegression] Epoch 6 (0.64s)\tAverage loss=0.759082\tDev F1=26.82\n",
      "[SparseLogisticRegression] Epoch 7 (0.72s)\tAverage loss=0.751106\tDev F1=26.86\n",
      "[SparseLogisticRegression] Epoch 8 (0.79s)\tAverage loss=0.752482\tDev F1=26.86\n",
      "[SparseLogisticRegression] Epoch 9 (0.85s)\tAverage loss=0.749555\tDev F1=26.89\n",
      "[SparseLogisticRegression] Epoch 10 (0.93s)\tAverage loss=0.748977\tDev F1=26.78\n",
      "[SparseLogisticRegression] Epoch 11 (1.00s)\tAverage loss=0.748768\tDev F1=26.75\n",
      "[SparseLogisticRegression] Epoch 12 (1.07s)\tAverage loss=0.749734\tDev F1=26.81\n",
      "[SparseLogisticRegression] Epoch 13 (1.14s)\tAverage loss=0.742934\tDev F1=26.87\n",
      "[SparseLogisticRegression] Epoch 14 (1.21s)\tAverage loss=0.740094\tDev F1=26.63\n",
      "[SparseLogisticRegression] Epoch 15 (1.28s)\tAverage loss=0.739290\tDev F1=26.84\n",
      "[SparseLogisticRegression] Epoch 16 (1.35s)\tAverage loss=0.740914\tDev F1=27.11\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.63s)\tAverage loss=0.741787\tDev F1=27.24\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (2.11s)\tAverage loss=0.738108\tDev F1=27.37\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 19 (2.42s)\tAverage loss=0.739309\tDev F1=26.99\n",
      "[SparseLogisticRegression] Training done (2.45s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.273709483794\n",
      "============================================================\n",
      "[5] Testing dim = 128, dropout = 1.00e-01, rebalance = 5.00e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=538  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.746501\tDev F1=24.71\n",
      "[SparseLogisticRegression] Epoch 1 (0.10s)\tAverage loss=0.765343\tDev F1=25.07\n",
      "[SparseLogisticRegression] Epoch 2 (0.17s)\tAverage loss=0.727210\tDev F1=25.78\n",
      "[SparseLogisticRegression] Epoch 3 (0.24s)\tAverage loss=0.715491\tDev F1=27.52\n",
      "[SparseLogisticRegression] Epoch 4 (0.31s)\tAverage loss=0.711239\tDev F1=28.01\n",
      "[SparseLogisticRegression] Epoch 5 (0.37s)\tAverage loss=0.706586\tDev F1=27.09\n",
      "[SparseLogisticRegression] Epoch 6 (0.60s)\tAverage loss=0.702427\tDev F1=26.03\n",
      "[SparseLogisticRegression] Epoch 7 (0.67s)\tAverage loss=0.703433\tDev F1=26.10\n",
      "[SparseLogisticRegression] Epoch 8 (0.74s)\tAverage loss=0.701200\tDev F1=26.23\n",
      "[SparseLogisticRegression] Epoch 9 (0.80s)\tAverage loss=0.701531\tDev F1=27.02\n",
      "[SparseLogisticRegression] Epoch 10 (0.87s)\tAverage loss=0.696228\tDev F1=27.13\n",
      "[SparseLogisticRegression] Epoch 11 (0.94s)\tAverage loss=0.697764\tDev F1=27.07\n",
      "[SparseLogisticRegression] Epoch 12 (1.01s)\tAverage loss=0.697283\tDev F1=26.65\n",
      "[SparseLogisticRegression] Epoch 13 (1.08s)\tAverage loss=0.700747\tDev F1=27.55\n",
      "[SparseLogisticRegression] Epoch 14 (1.14s)\tAverage loss=0.701658\tDev F1=26.90\n",
      "[SparseLogisticRegression] Epoch 15 (1.21s)\tAverage loss=0.699531\tDev F1=26.19\n",
      "[SparseLogisticRegression] Epoch 16 (1.28s)\tAverage loss=0.706277\tDev F1=26.46\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.54s)\tAverage loss=0.703476\tDev F1=27.53\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (1.98s)\tAverage loss=0.698773\tDev F1=28.93\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 19 (2.27s)\tAverage loss=0.709433\tDev F1=27.79\n",
      "[SparseLogisticRegression] Training done (2.31s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.289325842697\n",
      "============================================================\n",
      "[6] Testing dim = 128, dropout = 1.00e-01, rebalance = 5.00e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=538  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.752535\tDev F1=29.40\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.738483\tDev F1=29.49\n",
      "[SparseLogisticRegression] Epoch 2 (0.17s)\tAverage loss=0.722889\tDev F1=29.29\n",
      "[SparseLogisticRegression] Epoch 3 (0.24s)\tAverage loss=0.714759\tDev F1=29.66\n",
      "[SparseLogisticRegression] Epoch 4 (0.31s)\tAverage loss=0.709292\tDev F1=29.69\n",
      "[SparseLogisticRegression] Epoch 5 (0.37s)\tAverage loss=0.706507\tDev F1=29.87\n",
      "[SparseLogisticRegression] Epoch 6 (0.44s)\tAverage loss=0.702668\tDev F1=29.84\n",
      "[SparseLogisticRegression] Epoch 7 (0.51s)\tAverage loss=0.700304\tDev F1=29.60\n",
      "[SparseLogisticRegression] Epoch 8 (0.58s)\tAverage loss=0.698781\tDev F1=29.68\n",
      "[SparseLogisticRegression] Epoch 9 (0.80s)\tAverage loss=0.696856\tDev F1=30.17\n",
      "[SparseLogisticRegression] Epoch 10 (0.87s)\tAverage loss=0.695256\tDev F1=30.39\n",
      "[SparseLogisticRegression] Epoch 11 (0.94s)\tAverage loss=0.695485\tDev F1=30.50\n",
      "[SparseLogisticRegression] Epoch 12 (1.00s)\tAverage loss=0.694697\tDev F1=30.83\n",
      "[SparseLogisticRegression] Epoch 13 (1.07s)\tAverage loss=0.693639\tDev F1=30.63\n",
      "[SparseLogisticRegression] Epoch 14 (1.14s)\tAverage loss=0.693150\tDev F1=30.53\n",
      "[SparseLogisticRegression] Epoch 15 (1.21s)\tAverage loss=0.692414\tDev F1=30.61\n",
      "[SparseLogisticRegression] Epoch 16 (1.27s)\tAverage loss=0.692279\tDev F1=30.79\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.53s)\tAverage loss=0.691803\tDev F1=31.14\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (1.96s)\tAverage loss=0.691412\tDev F1=31.39\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 19 (2.25s)\tAverage loss=0.691218\tDev F1=31.31\n",
      "[SparseLogisticRegression] Training done (2.28s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.313924050633\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_5>\n",
      "============================================================\n",
      "[7] Testing dim = 128, dropout = 1.00e-01, rebalance = 2.50e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=553  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.05s)\tAverage loss=0.763974\tDev F1=30.62\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.742471\tDev F1=30.79\n",
      "[SparseLogisticRegression] Epoch 2 (0.18s)\tAverage loss=0.730363\tDev F1=29.50\n",
      "[SparseLogisticRegression] Epoch 3 (0.25s)\tAverage loss=0.721738\tDev F1=29.31\n",
      "[SparseLogisticRegression] Epoch 4 (0.48s)\tAverage loss=0.715315\tDev F1=29.01\n",
      "[SparseLogisticRegression] Epoch 5 (0.55s)\tAverage loss=0.710319\tDev F1=29.38\n",
      "[SparseLogisticRegression] Epoch 6 (0.62s)\tAverage loss=0.704607\tDev F1=27.99\n",
      "[SparseLogisticRegression] Epoch 7 (0.68s)\tAverage loss=0.702018\tDev F1=27.91\n",
      "[SparseLogisticRegression] Epoch 8 (0.75s)\tAverage loss=0.699967\tDev F1=28.20\n",
      "[SparseLogisticRegression] Epoch 9 (0.82s)\tAverage loss=0.697988\tDev F1=28.24\n",
      "[SparseLogisticRegression] Epoch 10 (0.89s)\tAverage loss=0.696967\tDev F1=28.03\n",
      "[SparseLogisticRegression] Epoch 11 (0.96s)\tAverage loss=0.695618\tDev F1=28.36\n",
      "[SparseLogisticRegression] Epoch 12 (1.03s)\tAverage loss=0.694616\tDev F1=28.61\n",
      "[SparseLogisticRegression] Epoch 13 (1.10s)\tAverage loss=0.694568\tDev F1=28.01\n",
      "[SparseLogisticRegression] Epoch 14 (1.16s)\tAverage loss=0.693443\tDev F1=28.18\n",
      "[SparseLogisticRegression] Epoch 15 (1.23s)\tAverage loss=0.693026\tDev F1=28.14\n",
      "[SparseLogisticRegression] Epoch 16 (1.30s)\tAverage loss=0.692713\tDev F1=28.22\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.57s)\tAverage loss=0.692000\tDev F1=28.31\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (2.03s)\tAverage loss=0.691757\tDev F1=28.35\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 19 (2.33s)\tAverage loss=0.691614\tDev F1=28.31\n",
      "[SparseLogisticRegression] Training done (2.37s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.283536585366\n",
      "============================================================\n",
      "[8] Testing dim = 128, dropout = 5.00e-01, rebalance = 5.00e-01, lr = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=538  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.757071\tDev F1=28.57\n",
      "[SparseLogisticRegression] Epoch 1 (0.10s)\tAverage loss=0.762312\tDev F1=28.57\n",
      "[SparseLogisticRegression] Epoch 2 (0.17s)\tAverage loss=0.753328\tDev F1=28.63\n",
      "[SparseLogisticRegression] Epoch 3 (0.23s)\tAverage loss=0.756722\tDev F1=28.60\n",
      "[SparseLogisticRegression] Epoch 4 (0.30s)\tAverage loss=0.754573\tDev F1=28.54\n",
      "[SparseLogisticRegression] Epoch 5 (0.37s)\tAverage loss=0.756104\tDev F1=28.37\n",
      "[SparseLogisticRegression] Epoch 6 (0.59s)\tAverage loss=0.747217\tDev F1=28.37\n",
      "[SparseLogisticRegression] Epoch 7 (0.66s)\tAverage loss=0.746535\tDev F1=28.37\n",
      "[SparseLogisticRegression] Epoch 8 (0.73s)\tAverage loss=0.746498\tDev F1=28.40\n",
      "[SparseLogisticRegression] Epoch 9 (0.79s)\tAverage loss=0.740449\tDev F1=28.40\n",
      "[SparseLogisticRegression] Epoch 10 (0.86s)\tAverage loss=0.743639\tDev F1=28.43\n",
      "[SparseLogisticRegression] Epoch 11 (0.93s)\tAverage loss=0.742675\tDev F1=28.51\n",
      "[SparseLogisticRegression] Epoch 12 (1.00s)\tAverage loss=0.741007\tDev F1=28.51\n",
      "[SparseLogisticRegression] Epoch 13 (1.07s)\tAverage loss=0.739972\tDev F1=28.48\n",
      "[SparseLogisticRegression] Epoch 14 (1.14s)\tAverage loss=0.738453\tDev F1=28.31\n",
      "[SparseLogisticRegression] Epoch 15 (1.20s)\tAverage loss=0.738569\tDev F1=28.34\n",
      "[SparseLogisticRegression] Epoch 16 (1.27s)\tAverage loss=0.739392\tDev F1=28.37\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.52s)\tAverage loss=0.743333\tDev F1=28.34\n",
      "[SparseLogisticRegression] Epoch 18 (1.59s)\tAverage loss=0.744646\tDev F1=28.37\n",
      "[SparseLogisticRegression] Epoch 19 (1.66s)\tAverage loss=0.735494\tDev F1=28.40\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (2.05s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.283963227783\n",
      "============================================================\n",
      "[9] Testing dim = 128, dropout = 2.50e-01, rebalance = 2.50e-01, lr = 1.00e-02\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=553  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.753362\tDev F1=20.90\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.772994\tDev F1=22.73\n",
      "[SparseLogisticRegression] Epoch 2 (0.17s)\tAverage loss=0.727926\tDev F1=26.78\n",
      "[SparseLogisticRegression] Epoch 3 (0.24s)\tAverage loss=0.715338\tDev F1=27.44\n",
      "[SparseLogisticRegression] Epoch 4 (0.31s)\tAverage loss=0.713203\tDev F1=25.88\n",
      "[SparseLogisticRegression] Epoch 5 (0.37s)\tAverage loss=0.706282\tDev F1=26.58\n",
      "[SparseLogisticRegression] Epoch 6 (0.44s)\tAverage loss=0.701568\tDev F1=25.45\n",
      "[SparseLogisticRegression] Epoch 7 (0.50s)\tAverage loss=0.699740\tDev F1=25.82\n",
      "[SparseLogisticRegression] Epoch 8 (0.57s)\tAverage loss=0.701551\tDev F1=24.74\n",
      "[SparseLogisticRegression] Epoch 9 (0.64s)\tAverage loss=0.697886\tDev F1=25.91\n",
      "[SparseLogisticRegression] Epoch 10 (0.70s)\tAverage loss=0.700618\tDev F1=25.97\n",
      "[SparseLogisticRegression] Epoch 11 (0.77s)\tAverage loss=0.698176\tDev F1=26.22\n",
      "[SparseLogisticRegression] Epoch 12 (0.84s)\tAverage loss=0.696436\tDev F1=26.22\n",
      "[SparseLogisticRegression] Epoch 13 (0.91s)\tAverage loss=0.695655\tDev F1=25.78\n",
      "[SparseLogisticRegression] Epoch 14 (0.97s)\tAverage loss=0.696553\tDev F1=26.35\n",
      "[SparseLogisticRegression] Epoch 15 (1.04s)\tAverage loss=0.696307\tDev F1=27.61\n",
      "[SparseLogisticRegression] Epoch 16 (1.11s)\tAverage loss=0.697037\tDev F1=27.31\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.53s)\tAverage loss=0.694980\tDev F1=25.82\n",
      "[SparseLogisticRegression] Epoch 18 (1.60s)\tAverage loss=0.693190\tDev F1=27.36\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 19 (1.88s)\tAverage loss=0.695900\tDev F1=26.81\n",
      "[SparseLogisticRegression] Training done (1.91s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.273572377158\n",
      "============================================================\n",
      "[10] Testing dim = 128, dropout = 5.00e-01, rebalance = 2.50e-01, lr = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=553  #epochs=20  batch size=128\n",
      "[SparseLogisticRegression] Epoch 0 (0.04s)\tAverage loss=0.748479\tDev F1=27.47\n",
      "[SparseLogisticRegression] Epoch 1 (0.11s)\tAverage loss=0.735882\tDev F1=27.76\n",
      "[SparseLogisticRegression] Epoch 2 (0.17s)\tAverage loss=0.721852\tDev F1=27.63\n",
      "[SparseLogisticRegression] Epoch 3 (0.24s)\tAverage loss=0.713425\tDev F1=28.34\n",
      "[SparseLogisticRegression] Epoch 4 (0.31s)\tAverage loss=0.708584\tDev F1=28.33\n",
      "[SparseLogisticRegression] Epoch 5 (0.38s)\tAverage loss=0.705780\tDev F1=28.45\n",
      "[SparseLogisticRegression] Epoch 6 (0.45s)\tAverage loss=0.702844\tDev F1=28.17\n",
      "[SparseLogisticRegression] Epoch 7 (0.52s)\tAverage loss=0.700395\tDev F1=27.68\n",
      "[SparseLogisticRegression] Epoch 8 (0.58s)\tAverage loss=0.697882\tDev F1=28.01\n",
      "[SparseLogisticRegression] Epoch 9 (0.65s)\tAverage loss=0.696711\tDev F1=28.21\n",
      "[SparseLogisticRegression] Epoch 10 (0.72s)\tAverage loss=0.695565\tDev F1=28.65\n",
      "[SparseLogisticRegression] Epoch 11 (0.79s)\tAverage loss=0.695024\tDev F1=28.33\n",
      "[SparseLogisticRegression] Epoch 12 (0.86s)\tAverage loss=0.693822\tDev F1=28.53\n",
      "[SparseLogisticRegression] Epoch 13 (1.07s)\tAverage loss=0.693651\tDev F1=28.25\n",
      "[SparseLogisticRegression] Epoch 14 (1.14s)\tAverage loss=0.693560\tDev F1=28.21\n",
      "[SparseLogisticRegression] Epoch 15 (1.21s)\tAverage loss=0.692336\tDev F1=28.13\n",
      "[SparseLogisticRegression] Epoch 16 (1.27s)\tAverage loss=0.692114\tDev F1=28.41\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 17 (1.53s)\tAverage loss=0.691830\tDev F1=28.61\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 18 (1.80s)\tAverage loss=0.691687\tDev F1=28.17\n",
      "[SparseLogisticRegression] Epoch 19 (1.87s)\tAverage loss=0.691412\tDev F1=28.41\n",
      "[SparseLogisticRegression] Training done (1.90s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.286115007013\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_5>\n",
      "   dim  dropout  rebalance      lr     Prec.      Rec.     F-1.0\n",
      "5  128     0.10       0.50  0.0010  0.219081  0.553571  0.313924\n",
      "2  128     0.10       0.50  0.0100  0.207407  0.500000  0.293194\n",
      "4  128     0.10       0.50  0.0100  0.211066  0.459821  0.289326\n",
      "9  128     0.50       0.25  0.0010  0.208589  0.455357  0.286115\n",
      "7  128     0.50       0.50  0.0001  0.184106  0.620536  0.283963\n",
      "6  128     0.10       0.25  0.0010  0.215278  0.415179  0.283537\n",
      "1  128     0.50       0.25  0.0001  0.189274  0.535714  0.279720\n",
      "3   64     0.50       0.50  0.0001  0.187192  0.508929  0.273709\n",
      "8  128     0.25       0.25  0.0100  0.194707  0.459821  0.273572\n",
      "0   64     0.25       0.00  0.0010  0.173835  0.433036  0.248082\n",
      "[SparseLogisticRegression] Model saved as <discriminative_protein>\n",
      "### Done in 26.5s.\n",
      "\n",
      "### [7.2] Evaluate generative model (opt_b=0.25)\n",
      "### Done in 0.2s.\n",
      "\n",
      "### [7.3] Evaluate discriminative model (opt_b=0.25)\n",
      "### Done in 0.0s.\n",
      "\n",
      "      F1 Score  Precision    Recall\n",
      "Disc  0.322902   0.192863  0.991266\n",
      "Gen   0.319832   0.190357  1.000000\n",
      "CPU times: user 39 s, sys: 2.31 s, total: 41.3 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
