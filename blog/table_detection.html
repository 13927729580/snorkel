<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>TabLoc: Machine Learning for Table Localization in PDF documents</title>
    <style type="text/css">
        .effectfront {
        border: none;
        margin: 0 auto;
        }

        .effectfront:hover {
        -webkit-transform: scale(1.2);
        -moz-transform: scale(1.2);
        -o-transform: scale(1.2);
        transform: scale(1.2);
        transition: all 0.3s;
        -webkit-transition: all 0.3s;
        }

        body {
        font-family: Helvetica, arial, sans-serif;
        font-size: 14px;
        line-height: 1.6;
        padding-top: 10px;
        padding-bottom: 10px;
        background-color: white;
        padding: 30px;
        }

        body>*:first-child {
        margin-top: 0 !important;
        }

        body>*:last-child {
        margin-bottom: 0 !important;
        }

        a {
        color: #4183C4;
        }

        a.absent {
        color: #cc0000;
        }

        a.anchor {
        display: block;
        padding-left: 30px;
        margin-left: -30px;
        cursor: pointer;
        position: absolute;
        top: 0;
        left: 0;
        bottom: 0;
        }

        h1,
        h2,
        h3,
        h4,
        h5,
        h6 {
        margin: 20px 0 10px;
        padding: 0;
        font-weight: bold;
        -webkit-font-smoothing: antialiased;
        cursor: text;
        position: relative;
        }

        h1:hover a.anchor,
        h2:hover a.anchor,
        h3:hover a.anchor,
        h4:hover a.anchor,
        h5:hover a.anchor,
        h6:hover a.anchor {
        background:
        url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==)
        no-repeat 10px center;
        text-decoration: none;
        }

        h1 tt,
        h1 code {
        font-size: inherit;
        }

        h2 tt,
        h2 code {
        font-size: inherit;
        }

        h3 tt,
        h3 code {
        font-size: inherit;
        }

        h4 tt,
        h4 code {
        font-size: inherit;
        }

        h5 tt,
        h5 code {
        font-size: inherit;
        }

        h6 tt,
        h6 code {
        font-size: inherit;
        }

        h1 {
        font-size: 28px;
        color: black;
        }

        h2 {
        font-size: 24px;
        border-bottom: 1px solid #cccccc;
        color: black;
        }

        h3 {
        font-size: 18px;
        }

        h4 {
        font-size: 16px;
        }

        h5 {
        font-size: 14px;
        }

        h6 {
        color: #777777;
        font-size: 14px;
        }

        p,
        blockquote,
        ul,
        ol,
        dl,
        li,
        table,
        pre {
        margin: 15px 0;
        }

        hr {
        background: transparent
        url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC)
        repeat-x 0 0;
        border: 0 none;
        color: #cccccc;
        height: 4px;
        padding: 0;
        }

        body>h2:first-child {
        margin-top: 0;
        padding-top: 0;
        }

        body>h1:first-child {
        margin-top: 0;
        padding-top: 0;
        }

        body>h1:first-child+h2 {
        margin-top: 0;
        padding-top: 0;
        }

        body>h3:first-child,
        body>h4:first-child,
        body>h5:first-child,
        body>h6:first-child {
        margin-top: 0;
        padding-top: 0;
        }

        a:first-child h1,
        a:first-child h2,
        a:first-child h3,
        a:first-child h4,
        a:first-child h5,
        a:first-child h6 {
        margin-top: 0;
        padding-top: 0;
        }

        h1 p,
        h2 p,
        h3 p,
        h4 p,
        h5 p,
        h6 p {
        margin-top: 0;
        }

        li p.first {
        display: inline-block;
        }

        li {
        margin: 0;
        }

        ul,
        ol {
        padding-left: 30px;
        }

        ul :first-child,
        ol :first-child {
        margin-top: 0;
        }

        dl {
        padding: 0;
        }

        dl dt {
        font-size: 14px;
        font-weight: bold;
        font-style: italic;
        padding: 0;
        margin: 15px 0 5px;
        }

        dl dt:first-child {
        padding: 0;
        }

        dl dt> :first-child {
        margin-top: 0;
        }

        dl dt> :last-child {
        margin-bottom: 0;
        }

        dl dd {
        margin: 0 0 15px;
        padding: 0 15px;
        }

        dl dd> :first-child {
        margin-top: 0;
        }

        dl dd> :last-child {
        margin-bottom: 0;
        }

        blockquote {
        border-left: 4px solid #dddddd;
        padding: 0 15px;
        color: #777777;
        }

        blockquote> :first-child {
        margin-top: 0;
        }

        blockquote> :last-child {
        margin-bottom: 0;
        }

        table {
        padding: 0;
        border-collapse: collapse;
        }

        table tr {
        border-top: 1px solid #cccccc;
        background-color: white;
        margin: 0;
        padding: 0;
        }

        table tr:nth-child(2n) {
        background-color: #f8f8f8;
        }

        table tr th {
        font-weight: bold;
        border: 1px solid #cccccc;
        margin: 0;
        padding: 6px 13px;
        }

        table tr td {
        border: 1px solid #cccccc;
        margin: 0;
        padding: 6px 13px;
        }

        table tr th :first-child,
        table tr td :first-child {
        margin-top: 0;
        }

        table tr th :last-child,
        table tr td :last-child {
        margin-bottom: 0;
        }

        img {
        max-width: 100%;
        }

        span.frame {
        display: block;
        overflow: hidden;
        }

        span.frame>span {
        border: 1px solid #dddddd;
        display: block;
        float: left;
        overflow: hidden;
        margin: 13px 0 0;
        padding: 7px;
        width: auto;
        }

        span.frame span img {
        display: block;
        float: left;
        }

        span.frame span span {
        clear: both;
        color: #333333;
        display: block;
        padding: 5px 0 0;
        }

        span.align-center {
        display: block;
        overflow: hidden;
        clear: both;
        }

        span.align-center>span {
        display: block;
        overflow: hidden;
        margin: 13px auto 0;
        text-align: center;
        }

        span.align-center span img {
        margin: 0 auto;
        text-align: center;
        }

        span.align-right {
        display: block;
        overflow: hidden;
        clear: both;
        }

        span.align-right>span {
        display: block;
        overflow: hidden;
        margin: 13px 0 0;
        text-align: right;
        }

        span.align-right span img {
        margin: 0;
        text-align: right;
        }

        span.float-left {
        display: block;
        margin-right: 13px;
        overflow: hidden;
        float: left;
        }

        span.float-left span {
        margin: 13px 0 0;
        }

        span.float-right {
        display: block;
        margin-left: 13px;
        overflow: hidden;
        float: right;
        }

        span.float-right>span {
        display: block;
        overflow: hidden;
        margin: 13px auto 0;
        text-align: right;
        }

        code,
        tt {
        margin: 0 2px;
        padding: 0 5px;
        white-space: nowrap;
        border: 1px solid #eaeaea;
        background-color: #f8f8f8;
        border-radius: 3px;
        }

        pre code {
        margin: 0;
        padding: 0;
        white-space: pre;
        border: none;
        background: transparent;
        }

        .highlight pre {
        background-color: #f8f8f8;
        border: 1px solid #cccccc;
        font-size: 13px;
        line-height: 19px;
        overflow: auto;
        padding: 6px 10px;
        border-radius: 3px;
        }

        pre {
        background-color: #f8f8f8;
        border: 1px solid #cccccc;
        font-size: 13px;
        line-height: 19px;
        overflow: auto;
        padding: 6px 10px;
        border-radius: 3px;
        }

        pre code,
        pre tt {
        background-color: transparent;
        border: none;
        }

        sup {
        font-size: 0.83em;
        vertical-align: super;
        line-height: 0;
        }

        * {
        -webkit-print-color-adjust: exact;
        }

        @media screen and (min-width: 914px) {
        body {
        width: 854px;
        margin: 0 auto;
        }
        }

        @media print {
        table,
        pre {
        page-break-inside: avoid;
        }
        pre {
        word-wrap: break-word;
        }
        }

        .top-banner {
        position: absolute;
        top: 0;
        left: 0;
        z-index: 0;
        width: 100%;
        }

        #top-banner-img {
        opacity: 0.5;
        width: 100%;
        max-height: 350px;
        }

        #main-title {
        position: relative;
        margin-top: 100px;
        margin-bottom: 100px;
        padding: 10px;
        font-size: 40px;
        background: #333333;
        color: #f8f8f8;
        z-index: 10;
        }

        blockquote {
        font-size: large;
        font-weight: 300;
        }

        p.img {
        text-align: center;
        }
    </style>
</head>

<body>
<span class="top-banner">
    <img id="top-banner-img" src="tables_img/tablesVar.png">
</span>

<p id="main-title">TabLoc: Machine Learning for Table Localization in PDF documents</p>

<p align="justify">Post by <a href="https://stanford.edu/~chami/" target="_blank">Ines Chami</a>, <a href="https://stanford.edu/~pabajaj/" target="_blank">Payal Bajaj</a>, <a href="http://stephenbach.net/">Stephen Bach</a> and <a
        href="https://cs.stanford.edu/people/chrismre/" target="_blank">Chris Ré</a></p>

<!--<p><em>And referencing work by many other <a href="http://cs.stanford.edu/people/chrismre/#students" target="_blank">members of Hazy Research</a></em></p>-->

<blockquote>
<p align="justify">
TLDR; Table Detection is a crucial pre-processing step in information extraction from documents. We introduce TabLoc, a machine learning based table localization algorithm to identify table boundaries in digitized PDF documents. TabLoc is based on logistic regression and it outperforms the state of the art approaches in table detection and generalizes well to multiple kinds of documents.
</p>
</blockquote>

<h1>Motivation: Table Localization</h1>

<p align="justify">
    <p align="justify"><img src="tables_img/investigate.jpg" style="float:left;width:100px;height:100px;">Information extraction from data is becoming increasingly popular to convert data to a queryable form which can be leveraged for various purposes. For instance, extracting "Spouse" relations from natural language can be used to populate a knowledge base which can then further support downstream applications such as information retrieval and question answering. Snorkel is currently being used for information extraction from unstructured data, namely text.Text mining can however be a limiting approach since important knowledge and relational information can also be
    found in semi-structured data such as tables or figures.
    </p>
    <p align="justify"><a href="https://hazyresearch.github.io/snorkel/blog/fonduer.html">Fonduer</a> has been recently developed for information extraction from <b>richly formatted</b> data, where
    "information is conveyed via combinations of textual, structural, tabular, and
    visual expressions".
    A crucial step in relation extraction from tables is the localization of tables in PDF documents.
    Most of the state-of-the-art approaches for table detection rely on manually engineered heuristics which focus particular type of dataset, and cannot generalize well to other variations of tables.
    TabLoc is a robust machine learning approach for identifying bounding boxes of tables in digitized PDF documents which can then directly be fed into Fonduer for relation extraction from such tables. We walk through the different steps of the TabLoc in this blog post.
</p>
<p align="justify">The source code is available <a href="https://github.com/HazyResearch/TreeStructure" target="_blank">here</a>
    and we also provide a demo <a href="https://github.com/HazyResearch/TreeStructure/blob/master/table-extraction/tutorials/table-extraction-demo.ipynb" target="_blank">tutorial</a>.</p>

<!--<p class="img"><img src="fonduer_img/rfd.png" /></p>-->
<!--<blockquote>-->
<!--<p>-->
<!--</p>-->
<!--</blockquote>-->

<h1>Challenges: Tables variation across documents</h1>

<p>
    Tables can vary greatly from one document to another.
    For instance, tables can be horizontal or vertical, tables may or may not be bounded by vertical and horizontal lines,
    tables can span multiple pages, tables may or may not have captions, tables can contain text or numbers, etc. We present below some examples of tables found in PDF documents.</p>

<p class="img"><img class="effectfront" src="tables_img/tablesVar.png" width="900px"/></p>
<p align="justify"> 
    Previous methods are based on heuristic rules to detect tables[4]. For instance, some methods rely on the caption
    "table" to reduce the search area to the one around the word "table" [3].
    Other methods rely on the presence of lines to detect tables.
    However, these heuristic based approaches would only work for the type of tables they were built for and thus do not generalize well to different kinds of documents.
    However for information extraction, it is crucial to have a system in place that is able to localize tables in varied documents.
</p>

<blockquote>
    <p align="justify">
        TabLoc is a robust machine learning algorithm that can detect tables in digitized PDF documents and that generalizes
        easily to different kinds of tables. It doesn't depend on heuristics such as the presence of caption with the word "table", the presence of line boundaries around or in the table or any restriction on amount and type of content in the table.
    </p>
</blockquote>
<h1>Our Approach</h1>
<p>
    TabLoc is unique TabLoc can be broken down into four units:
</p>
<p class="img"><img class="effectfront" src="tables_img/framework.png" width="900px"/></p>

<h1>Candidates generation:</h1>

<p align="justify"><img src="tables_img/candLogo.png" style="float:right;width:200px;height:100px;">Tabloc begins by generating candidate regions that are likely to contain tables.
We use <a href="https://pypi.python.org/pypi/pdfminer/">PDFminer</a>, a parsing library that can that extracts lines and
words coordinates in PDF documents, to analyze alignments of text and position of lines and then return tables candidates
as tuples of 5 numbers (page number, top, left, bottom, right).</p>

<blockquote>
    <p align="justify">The candidate generation is essentially based on two structural properties often found in tables: structural alignment of text and presence of lines in tables.</p>
</blockquote>

<ul>
    <li>
        <p align="justify">
            <b>Alignments:</b> Text within tables is often aligned, that is the elements within a table are often
            centered around the same vertical (or horizontal) axis within a column (or row).
            TabLoc qualifies objects with this property as <b>vertically or horizontally aligned</b> and uses structural alignments of text in PDF documents to generate tables candidates. Then it extracts the bounding boxes of each word in the document and performs hierarchical clustering by first combining
            text aligned in adjacent rows to obtain columns of the table, followed by combining these columns to create table
            candidates. This iterative clustering strategy allows TabLoc to generate multiple candidates, with various sizes as
            shown below.</p>
    </li>
</ul>
<p class="img"><img class="effectfront" src="tables_img/alignments.png" width="900px"/></p>
<ul>
    <li>
        <p align="justify">
            <b>Lines:</b> TabLoc uses the presence of vertical or horizontal lines in tables to generate additional candidates.
            It first merges small segments that are next to each other and add additional lines to create bounding boxes.
            For instance, if two lines are vertically aligned, the vertical lines at the edges are added to create a
            bounding box.
            It then iterates over all lines coordinates in order to find pairs of horizontal and vertical lines that share
            the same (top, left) coordinates and return the corresponding bounding box as a table candidate.
            In the figure below, we show an example of how TabLoc generates candidates based on lines coordinates.
        <p class="img"><img class="effectfront" src="tables_img/lines.png" width="700px"/></p>
        </p>
    </li>
</ul>

TabLoc merges the candidates generated by lines and alignments.
Note that these candidates contain a lot of false positive but our machine learning approach will remove these! We display below some examples for candidate generation.

<p class="img"><img class="effectfront" src="tables_img/candidates.png" width="700px"/></p>

<h1 id="toc_3">Features Extraction</h1>

For each candidate region, TabLoc extracts a set of features (24 in total). This is a crucial step compared to heuristic based approaches since we can use any type of features, ranging from generic features based on alignments of text to very specific features such as presence of numbers or symbols. The algorithm then learns which features are significant.

These features include:
<ul>
    <li>
        <p>number of lines within a candidate region</p>
    </li>
    <li>
        <p>proportion of digits in text within a region</p>
    </li>
    <li>
        <p>average white spacing between words</p>
    </li>
    <li>
        <p>number of columns, rows that are aligned</p>
    </li>
    <li>
        <p>text area coverage (area covered by text / region area)</p>
    </li>
    <li>
        <p>... and many others!</p>
    </li>
</ul>

This featurization a key step allowing robustness to any type of document. For instance, if a table does not contain lines, a heuristic based approach would simply ignore that table but our algorithm can still rely on the 23 other features to detect that table. The features we extract relfect the characteristics of table structure such as vertical and horizontal text alignment, whitespaces between the columns, etc. and do not rely on non-table specific features such as presence of the word "table" in the caption or when vertical distance of the block to the current table region is less than 2.5 times the average line height of the document[4], etc.

<h1 id="toc_3">Labels Generation</h1>

<p align="justify">
    <img src="tables_img/ar.jpg" style="float:left;width:150px;height:100px;">It is always hard to reduce a retrieval problem to a simple classification task. In our setting, our goal is to retrieve tables regions but we need ground truth labels to classify the candidates that we generated. In order to generate these ground truth labels, TabLoc compares each candidate with all ground truth tables. Note that this requires us to have ground truth table annotations for documents in the training set. It computes the intersection over union of bounding box area of a candidate with all ground truth tables and when the maximum IoU is greater than some preset threshold, the candidate is labeled as a true table region.
</p>
<h1 id="toc_3">Candidates Classification</h1>

<p align="justify">
    A logistic regression classifier is then trained using the ground truth labels to predict whether a candidate region is a table or not.
    During test time, TabLoc allows user to simply reuse the pretrained model to extract tables for any test PDF!
    We display below the final results for the previous candidate generation examples. We can see that TabLoc removed the false
    positive in the first page and kept one correct table candidate in the second page.
</p>

<p class="img"><img class="effectfront" src="tables_img/final.png" width="700px"/></p>

<h1 id="toc_4">Results</h1>

<p align="justify">
We first present the results of TabLoc on the ICDAR competition dataset. Nurimnen's algorithm performed best in this competition and as we can see, it outperforms our candidates but the differencce is not too large. 
</p>

<table style="margin: 0px auto;">
<tr>
<th>Algorithm</th>
<th>Precision</th>
<th>Recall</th>
<th>F1 score</th>
</tr>
<tr>
<td>Nurimen's Algorithm[1]</td>
<td>91.3</td>
<td>77.1</td>
<td>83.6</td>
</tr>
<tr>
<td>OCR (Tesseract)[2]</td>
<td>58.8</td>
<td>53.1</td>
<td>55.8</td>
</tr>
<tr>
<td>Lines Candidates</td>
<td>87.7</td>
<td>53.5</td>
<td>66.5</td>
</tr>
<tr>
<td>Alignment Candidates</td>
<td>84.23</td>
<td>75.15</td>
<td>79.43</td>
</tr>
</table>

<p align="justify">
We present the results of table localization by TabLoc on 166 Paleontology research papers. The state-of-the-art approach for table detection is Nurimnen's algorithm[1] which performed best in the ICDAR table detection competition in 2013. Tabloc outperforms this approach significantly as shown in the table below. This is because the former is built to work well on the documents in the ICDAR table detection competition and thus doesn't generalize well to other documents. Further, TabLoc also provides better results than Tesseract based OCR based approach[2]. This is because TabLoc is a learning algorithm as opposed to others which just rely on heuristics. Thus it is able to generalize better to table variations and varied documents.
</p>

<table style="margin: 0px auto;">
    <tr>
    <th>Algorithm</th>
    <th>Precision</th>
    <th>Recall</th>
    <th>F1 score</th>
    </tr>
    <tr>
    <td>Nurimnen's Algorithm[1]</td>
    <td>50.3</td>
    <td>22.7</td>
    <td>31.3</td>
    </tr>
    <tr>
    <td>OCR (Tesseract)[2]</td>
    <td>75.0</td>
    <td>67.2</td>
    <td>70.9</td>
    </tr>
    <tr>
    <td>Lines Candidates</td>
    <td>88.8</td>
    <td>37.2</td>
    <td>52.4</td>
    </tr>
    <tr>
    <td>Alignment Candidates</td>
    <td>92.7</td>
    <td>44.6</td>
    <td>60.3</td>
    </tr>
    <tr>
    <td>Lines+Alignment Candidates</td>
    <td>82.4</td>
    <td>75.1</td>
    <td>73.9</td>
    </tr>
    <tr>
    <td>TabLoc</td>
    <td>81.4</td>
    <td>81.0</td>
    <td>81.2</td>
    </tr>
</table>

<h1 id="toc_6">Next Steps</h1>

<ul>
    <li>
        <p align="justify">
            <b> Label generation using weak supervision:</b> Collecting training data for table detection is hard. We build an annotation that allows us to render pdf pages and manually crop table regions. We use these regions to generate labels for our candidates.
            However, this can be very time consuming for more than hundreds of documents. 
            We are currently working towards a weak supervision approach, namely use our current small model to generate more training examples and use this noisy data to train larger models.
        </p>
    </li>
    <li>
        <p align="justify">
            <b> Visual Features:</b> Tables are not only structurally but also visually differentiable from other parts of a document.
            We are working towards a computer vision approach that would allow us to also extract and add visual features into the loop.</p>
    </li>
    <li>
        <p align="justify">
            <b> Tree Structure:</b> Our ultimate goal is the construction of a hierarchical tree of context objects such as text blocks, figures, tables, etc.
            This will allow us to build an end to end information extraction system that parses digitized PDFs to understand
            the underlying tree structure and then performs information extraction on it.
        </p>
    </li>

</ul>

<h2>References</h2>
<p>
    <ol>
    <li>Nurminen, A. (2013). Algorithmic extraction of data in tables in pdf documents. Master’s thesis, Tampere University of Technology.</li>
    <li>https://github.com/UW-Deepdive-Infrastructure/table-extract</li>
    <li>Clark, Christopher, and Santosh Divvala. "Pdffigures 2.0: Mining figures from research papers." Digital Libraries (JCDL), 2016 IEEE/ACM Joint Conference on. IEEE, 2016.</li>
    <li>Klampfl, Stefan, Kris Jack, and Roman Kern. "A comparison of two unsupervised table recognition methods from digital scientific articles." D-Lib Magazine 20.11 (2014): 7.</li>
    </ol>
</p>
<p>
    TODO: Replace background image<br>
</p>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-93927842-1', 'auto');
      ga('send', 'pageview');


</script>

</body>

</html>
