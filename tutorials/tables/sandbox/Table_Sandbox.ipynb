{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.remove('snorkel.db')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser import HTMLParser\n",
    "from snorkel.parser import OmniParser\n",
    "\n",
    "file_name = os.environ['SNORKELHOME'] + '/tutorials/tables/sandbox/diseases.xhtml'\n",
    "doc_parser = HTMLParser(path=file_name)\n",
    "context_parser = OmniParser(tabular=True, lingual=True)\n",
    "cp = CorpusParser(doc_parser, context_parser, max_docs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup, NavigableString, Comment\n",
    "# with open(file_name, 'rb') as f:\n",
    "#     html_doc = f.read()\n",
    "# soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for child in soup.body.children:\n",
    "#     print isinstance(child, Comment)\n",
    "# print isinstance(soup.table.tr, Comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "CPU times: user 157 ms, sys: 17.2 ms, total: 174 ms\n",
      "Wall time: 534 ms\n",
      "36 Phrases in Corpus (Sandbox)\n"
     ]
    }
   ],
   "source": [
    "%time corpus = cp.parse_corpus(name='Sandbox', session=session)\n",
    "print \"%d Phrases in %s\" % (\n",
    "    len([phrase for doc in corpus.documents for phrase in doc.phrases]), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/html/body/h1[1]\n",
      "<html lang\n",
      "<Element h1 at 0x1146ff260>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import _get_node\n",
    "from lxml.html import fromstring\n",
    "from lxml import etree\n",
    "\n",
    "p = corpus.documents[0].phrases[0]\n",
    "print p.xpath\n",
    "print p.document.text[:10]\n",
    "tree = etree.ElementTree(fromstring(p.document.text))\n",
    "tree.xpath('/html/body/table[1]')\n",
    "# print type(p.document.etree.xpath('/html'))\n",
    "# .xpath(p.xpath[1:])\n",
    "print _get_node(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'h1', u'/html/body/h1[1]')\n",
      "(u'span', u'/html/body/p[2]/span')\n",
      "(u'h1', u'/html/body/h1[2]')\n",
      "(u'h2', u'/html/body/h2')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[2]/th[1]')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[2]/th[2]')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[2]/th[3]')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[3]/th')\n",
      "(u'p', u'/html/body/table[1]/tbody/tr[3]/td[1]/p')\n",
      "(u'span', u'/html/body/table[1]/tbody/tr[3]/td[1]/p/span')\n",
      "(u'td', u'/html/body/table[1]/tbody/tr[3]/td[2]')\n",
      "(u'span', u'/html/body/table[1]/tbody/tr[3]/td[2]/span')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[4]/th')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[4]/th')\n",
      "(u'td', u'/html/body/table[1]/tbody/tr[4]/td[1]')\n",
      "(u'th', u'/html/body/table[1]/tbody/tr[5]/th')\n",
      "(u'td', u'/html/body/table[1]/tbody/tr[5]/td[1]')\n",
      "(u'td', u'/html/body/table[1]/tbody/tr[5]/td[2]')\n",
      "(u'td', u'/html/body/table[1]/tbody/tr[5]/td[2]')\n",
      "(u'caption', u'/html/body/table[1]/caption')\n",
      "(u'p', u'/html/body/p[3]')\n",
      "(u'th', u'/html/body/table[2]/tbody/tr[1]/th[1]')\n",
      "(u'th', u'/html/body/table[2]/tbody/tr[1]/th[2]')\n",
      "(u'th', u'/html/body/table[2]/tbody/tr[1]/th[3]')\n",
      "(u'th', u'/html/body/table[2]/tbody/tr[2]/th')\n",
      "(u'td', u'/html/body/table[2]/tbody/tr[2]/td[1]')\n",
      "(u'td', u'/html/body/table[2]/tbody/tr[2]/td[2]')\n",
      "(u'th', u'/html/body/table[2]/tbody/tr[3]/th')\n",
      "(u'i', u'/html/body/table[2]/tbody/tr[3]/th/i')\n",
      "(u'td', u'/html/body/table[2]/tbody/tr[3]/td[1]')\n",
      "(u'td', u'/html/body/table[2]/tbody/tr[3]/td[2]')\n",
      "(u'th', u'/html/body/table[2]/tbody/tr[4]/th')\n",
      "(u'td', u'/html/body/table[2]/tbody/tr[4]/td[1]')\n",
      "(u'td', u'/html/body/table[2]/tbody/tr[4]/td[2]')\n",
      "(u'caption', u'/html/body/table[2]/caption')\n",
      "(u'p', u'/html/body/p[4]')\n"
     ]
    }
   ],
   "source": [
    "for p in corpus.documents[0].phrases:\n",
    "#     print (p.table, p.cell)\n",
    "        print (p.html_tag, p.xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xpath = '/html/body/table[1]/tbody/tr[5]/td[2]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# for i in range(34):\n",
    "#     print corpus.documents[0].phrases[i].is_tabular()\n",
    "p = corpus.documents[0].phrases[5]\n",
    "print p.is_lingual()\n",
    "print p.is_tabular()\n",
    "print p.is_visual()\n",
    "print p.is_html()\n",
    "# # print [phrase for phrase in corpus.documents[0].phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for cell in corpus.documents[0].cells: print cell.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of viruses, coughs, and colds\n",
      "Brain Cancer\n",
      "See Table Below.\n",
      "Common Ailments\n",
      "Disease\n",
      "Location\n",
      "Year\n",
      "Polio is BC546 degrees C cold.\n",
      "-\n",
      "Dublin to Milwaukee\n",
      "?\n",
      "2001\n",
      "I don't like Chicken Pox or pizza.\n",
      "Shingles is also bad.\n",
      "whooping cough\n",
      "Scurvy\n",
      "Annapolis\n",
      "Junction and Storage Temperature -55 to 150 o ?\n",
      "C\n",
      "Table 1: Infectious diseases and where to find them.\n",
      "In between the tables there is a nasty case of heart attack\n",
      "Problem\n",
      "Cause\n",
      "Cost\n",
      "Arthritis\n",
      "Pokemon Go\n",
      "Free\n",
      "Yellow\n",
      "Fever\n",
      "Unicorns\n",
      "$17.75\n",
      "Hypochondria\n",
      "Fear\n",
      "$100\n",
      "Table 2: Three ways to get Pneumonia and how much they cost.\n",
      "And here is a final sentence with warts.\n"
     ]
    }
   ],
   "source": [
    "for phrase in [phrase for doc in corpus.documents for phrase in doc.phrases]:\n",
    "    print phrase.text\n",
    "#     phrase_total = len(phrase.cell.phrases) if phrase.cell else 0\n",
    "#      print \"%d: %d/%d\" % (phrase.phrase_id, phrase.position, phrase_total)\n",
    "#     document = phrase.document.name\n",
    "#     table = phrase.table.position if phrase.table else 'X'\n",
    "#     cell = phrase.cell.position if phrase.cell else 'X'\n",
    "#     print (document, table, cell, phrase.position, phrase.words[0], phrase.html_anc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (a,b,c) = (2,(3 if True else 5) ,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "# Year = candidate_subclass('Year', ['year'])\n",
    "# Temp = candidate_subclass('Temp', ['temp'])\n",
    "# Disease = candidate_subclass('Disease', ['disease'])\n",
    "# Part = candidate_subclass('Part', ['part'])\n",
    "Disease_Part = candidate_subclass('Disease_Part', ['disease','part'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import OmniNgrams\n",
    "from hardware_utils import OmniNgramsPart, OmniNgramsTemp\n",
    "\n",
    "omni_ngrams = OmniNgrams(n_max=3, split_tokens=None)\n",
    "omni_part = OmniNgramsPart(n_max=1)\n",
    "# omni_temp = OmniNgramsTemp(n_max=3)\n",
    "# omni_temp = OmniNgrams(n_max=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23 diseases.\n"
     ]
    }
   ],
   "source": [
    "diseases = ['viruses', 'coughs', 'colds', 'brain', 'cancer', 'shingles', 'warts',\n",
    "              'brain cancer', 'disease', 'location', 'polio', 'chicken pox', \n",
    "              'bubonic plague', 'plague', 'scurvy', 'infectious diseases', \n",
    "              'arthritis', 'yellow fever', 'fever', 'hypochondria', 'pneumonia',\n",
    "              'whooping cough', 'heart attack']\n",
    "print \"Loaded %d diseases.\" % len(diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch, RegexMatchEach, RegexMatchSpan, DateMatcher\n",
    "\n",
    "disease_matcher = DictionaryMatch(d=diseases, ignore_case=True)\n",
    "# year_matcher = DateMatcher()\n",
    "part_matcher = RegexMatchEach(rgx='BC.*')\n",
    "# temp_matcher = RegexMatchSpan(rgx=r'-[5-7][05]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "# ce = CandidateExtractor(Part, [omni_part], [part_matcher])\n",
    "# ce = CandidateExtractor(Disease, [omni_ngrams], [disease_matcher])\n",
    "# ce = CandidateExtractor(Temp, [omni_temp], [temp_matcher])\n",
    "ce = CandidateExtractor(Disease_Part, \n",
    "                        [omni_ngrams, omni_part], \n",
    "                        [disease_matcher, part_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "CPU times: user 81.4 ms, sys: 4.46 ms, total: 85.9 ms\n",
      "Wall time: 162 ms\n",
      "Candidate Set (Sandbox Candidates) contains 20 Candidates\n"
     ]
    }
   ],
   "source": [
    "%time candidates = ce.extract(corpus.documents, 'Sandbox Candidates', session)\n",
    "print \"%s contains %d Candidates\" % (candidates, len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease_Part(Span(\"Shingles\", parent=45, chars=[0,7], words=[0,0]), Span(\"BC546\", parent=39, chars=[9,13], words=[2,2]))\n"
     ]
    }
   ],
   "source": [
    "c = candidates[0]\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span(\"Shingles\", parent=45, chars=[0,7], words=[0,0])\n",
      "th\n",
      "tr\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "\n",
    "s = c[0]\n",
    "print s\n",
    "print get_tag(s)\n",
    "print get_parent_tag(s)\n",
    "print get_prev_sibling_tags(s)\n",
    "print get_prev_sibling_tags(s)\n",
    "print get_next_sibling_tags(s)\n",
    "print get_ancesot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "\n",
    "# print [cell.text[:8] for cell in _get_aligned_cells(root_phrase.cell, axis, infer=infer)]\n",
    "# print [phrase.text for cell in _get_aligned_cells(root_phrase.cell, axis, infer=infer) for phrase in cell.phrases]\n",
    "print c.disease.parent\n",
    "print c.part.parent\n",
    "print list(get_left_ngrams(c.part, n_max=1, window=10, lower=False))\n",
    "print list(get_right_ngrams(c.part, n_max=1, window=5))\n",
    "print list(get_phrase_ngrams(c.part, n_max=1))\n",
    "print list(get_cell_ngrams(c.part, n_max=1))\n",
    "print list(get_row_ngrams(c.part, n_max=1, direct=True, infer=True))\n",
    "print list(get_head_ngrams(c.part, 'row'))\n",
    "print list(get_head_ngrams(c.part, 'col'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "train = CandidateSet(name='train', candidates=candidates[:25])\n",
    "dev = CandidateSet(name='test', candidates=candidates[25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()\n",
    "%time F_train = feature_manager.create(session, train, 'Sandbox Features')\n",
    "F_train\n",
    "\n",
    "# from snorkel.features import get_span_feats\n",
    "# c = candidates[10]\n",
    "# %prun for feat in get_span_feats(c): print feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import get_keys_by_candidate\n",
    "\n",
    "c = candidates[0]\n",
    "print c\n",
    "print\n",
    "for f in get_keys_by_candidate(c, F_train)[:10]: print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c = candidates[0]\n",
    "# print c\n",
    "# print c.year\n",
    "# print c.year.parent\n",
    "# print c.year.char_start\n",
    "# print c.year.char_end\n",
    "# print c.year.parent.words\n",
    "# print c.year.parent.char_offsets\n",
    "# print c.year.parent.lemmas\n",
    "# print c.year.parent.pos_tags\n",
    "# print c.year.parent.ner_tags\n",
    "# print c.year.parent.dep_parents\n",
    "# print c.year.parent.dep_labels\n",
    "# print c.year.get_word_start()\n",
    "# print c.year.get_word_end()\n",
    "# # print get_row_ngrams(c.year)\n",
    "# # print get_row_ngrams(c.year, infer=True)\n",
    "# %time for i in range(1000): _get_aligned_cells((c.year).parent.cell, 'col', infer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from snorkel.models import Span, ImplicitSpan, TemporarySpan\n",
    "\n",
    "# print isinstance(c.year, TemporarySpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.lf_helpers import get_between_ngrams, get_left_ngrams, get_right_ngrams\n",
    "# from snorkel.lf_helpers import contains_token, contains_regex\n",
    "# from snorkel.lf_helpers import get_phrase_ngrams, get_cell_ngrams, get_neighbor_cell_ngrams\n",
    "# from snorkel.lf_helpers import get_row_ngrams, get_col_ngrams, get_aligned_ngrams\n",
    "# from snorkel.lf_helpers import same_document, same_table, same_cell, same_phrase\n",
    "# from snorkel.lf_helpers import _get_aligned_cells, _get_nonempty_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "c = candidates[2]\n",
    "print list(get_head_ngrams(c.disease, axis='col'))\n",
    "# print get_left_ngrams(c.disease)\n",
    "# print get_right_ngrams(c.disease)\n",
    "# print contains_token(c, 'plague')\n",
    "# print contains_regex(c, r'pla')\n",
    "# print same_document(c)\n",
    "# print same_table(c)\n",
    "# print same_cell(c)\n",
    "# print same_phrase(c)\n",
    "# print get_phrase_ngrams(c.disease, n_min=1, n_max=3, case_sensitive=True)\n",
    "# print get_cell_ngrams(c.disease, attrib='pos_tags')\n",
    "# print get_neighbor_cell_ngrams(c.disease, dist=2, directions=True)\n",
    "# print get_row_ngrams(c.disease)\n",
    "# print get_col_ngrams(c.disease)\n",
    "# print get_aligned_ngrams(c.disease)\n",
    "# print get_aligned_ngrams(c.disease, infer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from hardware_utils import expand_implicit_text \n",
    "\n",
    "# for part in expand_implicit_text(''.join(['BC856/857/858', '/', '859/860'])): print part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def foo(n):\n",
    "#     phrase = 'repeat me'\n",
    "#     pmul = phrase * n\n",
    "#     pjoi = ''.join([phrase for x in xrange(n)])\n",
    "#     pinc = ''\n",
    "#     for x in xrange(n):\n",
    "#         pinc += phrase\n",
    "#     del pmul, pjoi, pinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %lprun -f foo foo(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from hardware_utils import get_gold_dict\n",
    "\n",
    "# filename = os.environ['SNORKELHOME'] + '/tutorials/tables/data/hardware/hardware_gold.csv'\n",
    "# gold_dict = get_gold_dict(filename, 'stg_temp_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(gold_dict.values())\n",
    "# print gold_dict.values().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from hardware_utils import count_hardware_labels\n",
    "\n",
    "# filename = os.environ['SNORKELHOME'] + '/tutorials/tables/data/hardware/hardware_gold.csv'\n",
    "# %time (certain, maybe) = count_hardware_labels(loader, candidates, filename, attrib='stg_temp_min', attrib_class='temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time \n",
    "# for i in range(10000): \n",
    "#     for j in range(100):\n",
    "#         1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.utils import ProgressBar;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# n = 100000\n",
    "# pb = ProgressBar(n)\n",
    "# for i in xrange(n):\n",
    "#     pb.bar(i)\n",
    "# pb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N = 235\n",
    "# N = 10\n",
    "# ticks = set([int(i * N/100.0) for i in range(1,101)])\n",
    "# print ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc, text in doc_parser.parse():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_xpath(tags, counts):\n",
    "    xpath = ''\n",
    "    for i, tag in enumerate(tags):\n",
    "        xpath += '/' + tag\n",
    "        if counts[i] != 1:\n",
    "            xpath += '[%d]' % (counts[i] - 1)\n",
    "    return xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = ['html','body','table','tr','th']\n",
    "counts = [1, 1, 2, 2, 1]\n",
    "print create_xpath(tags, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.html.body.table.next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml.html import fromstring\n",
    "import lxml\n",
    "\n",
    "html = lxml.html.fromstring(text)\n",
    "# htmlparser = etree.HTMLParser()\n",
    "# xml.etree.ElementTree.fromstring(text)\n",
    "# tree = lxml.etree.fromstring(text, htmlparser)\n",
    "# tree.xpath(xpathselector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etree.tostring(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tree.xpath('/html/body/table[1]')\n",
    "print tree.xpath('/html/body/table[1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for noe in "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
