{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Snorkel Workshop: Extracting Spouse Relations from the News\n",
    "\n",
    "## Part 4: Hyperparameter Tuning via Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the database backend and initalize a Snorkel session\n",
    "from lib.init import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass, and load the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 Training a `SparseLogReg` Discriminative Model\n",
    "We use the training marginals to train a discriminative model that classifies each `Candidate` as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature Extraction\n",
    "Instead of using a deep learning approach to start, let's look at a standard sparse logistic regression model. First, we need to extract out features. This can take a while, but we only have to do it once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22254, 547350)\n",
      "(2811, 547350)\n",
      "(2701, 547350)\n"
     ]
    }
   ],
   "source": [
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev = featurizer.load_matrix(session, split=1)\n",
    "F_test = featurizer.load_matrix(session, split=2)\n",
    "\n",
    "# #if F_train.size == 0:    \n",
    "# %time F_train = featurizer.apply(split=0, parallelism=4)\n",
    "# #if F_dev.size == 0:     \n",
    "# %time F_dev  = featurizer.apply_existing(split=1, parallelism=4)\n",
    "# #if F_test.size == 0:\n",
    "# %time F_test = featurizer.apply_existing(split=2, parallelism=4)\n",
    "\n",
    "print F_train.shape\n",
    "print F_dev.shape\n",
    "print F_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "First, reload the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code performs model selection by tuning our learning algorithm's hyperparamters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 4. Search space size = 125.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, \n",
    "                        F_train, train_marginals, \n",
    "                        [rate_param, l1_param, l2_param], \n",
    "                        n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2811, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-02, l1_penalty = 1.00e-02, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5252  #epochs=400  batch size=256\n",
      "[SparseLogisticRegression] Epoch 0 (0.51s)\tAverage loss=438.817993\n",
      "[SparseLogisticRegression] Epoch 25 (13.13s)\tAverage loss=442.243896\n",
      "[SparseLogisticRegression] Epoch 50 (26.21s)\tAverage loss=442.291931\n",
      "[SparseLogisticRegression] Epoch 75 (38.79s)\tAverage loss=442.853973\n",
      "[SparseLogisticRegression] Epoch 100 (51.16s)\tAverage loss=443.277283\n",
      "[SparseLogisticRegression] Epoch 125 (63.12s)\tAverage loss=443.277100\n",
      "[SparseLogisticRegression] Epoch 150 (75.71s)\tAverage loss=443.391602\n",
      "[SparseLogisticRegression] Epoch 175 (88.29s)\tAverage loss=443.762268\n",
      "[SparseLogisticRegression] Epoch 200 (101.26s)\tAverage loss=443.870178\n",
      "[SparseLogisticRegression] Epoch 225 (114.11s)\tAverage loss=444.545288\n",
      "[SparseLogisticRegression] Epoch 250 (127.38s)\tAverage loss=444.714417\n",
      "[SparseLogisticRegression] Epoch 275 (140.65s)\tAverage loss=445.127594\n",
      "[SparseLogisticRegression] Epoch 300 (153.57s)\tAverage loss=445.486267\n",
      "[SparseLogisticRegression] Epoch 325 (166.47s)\tAverage loss=446.225922\n",
      "[SparseLogisticRegression] Epoch 350 (179.80s)\tAverage loss=446.130157\n",
      "[SparseLogisticRegression] Epoch 375 (192.75s)\tAverage loss=447.184418\n",
      "[SparseLogisticRegression] Epoch 399 (205.59s)\tAverage loss=447.345764\n",
      "[SparseLogisticRegression] Training done (205.59s)\n",
      "[SparseLogisticRegression] F1 Score: 0.362369337979\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "============================================================\n",
      "[2] Testing lr = 1.00e-04, l1_penalty = 1.00e-06, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5252  #epochs=400  batch size=256\n",
      "[SparseLogisticRegression] Epoch 0 (0.50s)\tAverage loss=0.836718\n",
      "[SparseLogisticRegression] Epoch 25 (13.69s)\tAverage loss=0.697059\n",
      "[SparseLogisticRegression] Epoch 50 (27.79s)\tAverage loss=0.663277\n",
      "[SparseLogisticRegression] Epoch 75 (41.46s)\tAverage loss=0.650850\n",
      "[SparseLogisticRegression] Epoch 100 (54.98s)\tAverage loss=0.645094\n",
      "[SparseLogisticRegression] Epoch 125 (68.34s)\tAverage loss=0.642580\n",
      "[SparseLogisticRegression] Epoch 150 (82.37s)\tAverage loss=0.640419\n",
      "[SparseLogisticRegression] Epoch 175 (96.27s)\tAverage loss=0.639200\n",
      "[SparseLogisticRegression] Epoch 200 (109.62s)\tAverage loss=0.638446\n",
      "[SparseLogisticRegression] Epoch 225 (123.18s)\tAverage loss=0.638167\n",
      "[SparseLogisticRegression] Epoch 250 (137.75s)\tAverage loss=0.637846\n",
      "[SparseLogisticRegression] Epoch 275 (152.59s)\tAverage loss=0.637770\n",
      "[SparseLogisticRegression] Epoch 300 (166.17s)\tAverage loss=0.637395\n",
      "[SparseLogisticRegression] Epoch 325 (180.14s)\tAverage loss=0.637683\n",
      "[SparseLogisticRegression] Epoch 350 (194.32s)\tAverage loss=0.637455\n",
      "[SparseLogisticRegression] Epoch 375 (208.12s)\tAverage loss=0.637009\n",
      "[SparseLogisticRegression] Epoch 399 (221.33s)\tAverage loss=0.637018\n",
      "[SparseLogisticRegression] Training done (221.33s)\n",
      "[SparseLogisticRegression] F1 Score: 0.271929824561\n",
      "============================================================\n",
      "[3] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5252  #epochs=400  batch size=256\n",
      "[SparseLogisticRegression] Epoch 0 (0.49s)\tAverage loss=1.078406\n",
      "[SparseLogisticRegression] Epoch 25 (13.09s)\tAverage loss=0.911308\n",
      "[SparseLogisticRegression] Epoch 50 (25.78s)\tAverage loss=0.911211\n",
      "[SparseLogisticRegression] Epoch 75 (38.85s)\tAverage loss=0.911492\n",
      "[SparseLogisticRegression] Epoch 100 (52.28s)\tAverage loss=0.911290\n",
      "[SparseLogisticRegression] Epoch 125 (64.83s)\tAverage loss=0.912185\n",
      "[SparseLogisticRegression] Epoch 150 (77.32s)\tAverage loss=0.911667\n",
      "[SparseLogisticRegression] Epoch 175 (89.74s)\tAverage loss=0.911662\n",
      "[SparseLogisticRegression] Epoch 200 (102.61s)\tAverage loss=0.912235\n",
      "[SparseLogisticRegression] Epoch 225 (116.75s)\tAverage loss=0.912451\n",
      "[SparseLogisticRegression] Epoch 250 (132.29s)\tAverage loss=0.912278\n",
      "[SparseLogisticRegression] Epoch 275 (147.94s)\tAverage loss=0.912360\n",
      "[SparseLogisticRegression] Epoch 300 (162.34s)\tAverage loss=0.912671\n",
      "[SparseLogisticRegression] Epoch 325 (177.84s)\tAverage loss=0.912809\n",
      "[SparseLogisticRegression] Epoch 350 (193.68s)\tAverage loss=0.912695\n",
      "[SparseLogisticRegression] Epoch 375 (208.78s)\tAverage loss=0.912792\n",
      "[SparseLogisticRegression] Epoch 399 (223.12s)\tAverage loss=0.911650\n",
      "[SparseLogisticRegression] Training done (223.12s)\n",
      "[SparseLogisticRegression] F1 Score: 0.287037037037\n",
      "============================================================\n",
      "[4] Testing lr = 1.00e-03, l1_penalty = 1.00e-02, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5252  #epochs=400  batch size=256\n",
      "[SparseLogisticRegression] Epoch 0 (0.56s)\tAverage loss=439.856781\n",
      "[SparseLogisticRegression] Epoch 25 (15.03s)\tAverage loss=440.271912\n",
      "[SparseLogisticRegression] Epoch 50 (29.50s)\tAverage loss=440.680939\n",
      "[SparseLogisticRegression] Epoch 75 (43.96s)\tAverage loss=440.805664\n",
      "[SparseLogisticRegression] Epoch 100 (59.39s)\tAverage loss=440.862640\n",
      "[SparseLogisticRegression] Epoch 125 (74.04s)\tAverage loss=440.875916\n",
      "[SparseLogisticRegression] Epoch 150 (88.11s)\tAverage loss=440.874176\n",
      "[SparseLogisticRegression] Epoch 175 (102.25s)\tAverage loss=440.881989\n",
      "[SparseLogisticRegression] Epoch 200 (116.69s)\tAverage loss=440.889130\n",
      "[SparseLogisticRegression] Epoch 225 (131.11s)\tAverage loss=440.882355\n",
      "[SparseLogisticRegression] Epoch 250 (145.32s)\tAverage loss=440.897827\n",
      "[SparseLogisticRegression] Epoch 275 (160.10s)\tAverage loss=440.895966\n",
      "[SparseLogisticRegression] Epoch 300 (175.20s)\tAverage loss=440.914215\n",
      "[SparseLogisticRegression] Epoch 325 (192.14s)\tAverage loss=440.905212\n",
      "[SparseLogisticRegression] Epoch 350 (209.68s)\tAverage loss=440.915466\n",
      "[SparseLogisticRegression] Epoch 375 (225.30s)\tAverage loss=440.925598\n",
      "[SparseLogisticRegression] Epoch 399 (239.60s)\tAverage loss=440.913910\n",
      "[SparseLogisticRegression] Training done (239.60s)\n",
      "[SparseLogisticRegression] F1 Score: 0.262376237624\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/SparseLogisticRegression_0/SparseLogisticRegression_0-0\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_0>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>l1_penalty</th>\n",
       "      <th>l2_penalty</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.362369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.256198</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.287037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.233083</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.271930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.247664</td>\n",
       "      <td>0.278947</td>\n",
       "      <td>0.262376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr  l1_penalty  l2_penalty     Prec.      Rec.        F1\n",
       "0  0.0100    0.010000    0.000100  0.536082  0.273684  0.362369\n",
       "2  0.0010    0.000001    0.000100  0.256198  0.326316  0.287037\n",
       "1  0.0001    0.000001    0.000001  0.233083  0.326316  0.271930\n",
       "3  0.0010    0.010000    0.001000  0.247664  0.278947  0.262376"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=400, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Examining Features\n",
    "Extracting features allows us to inspect and interperet our learned weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reversed object at 0x135a0e8d0>\n",
      "Feature: TDL_INV_LEMMA:RIGHT-OF-MENTION['s actress]                                                Weight: 0.728911\n",
      "Feature: TDL_INV_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[ROOT|write prep|on]                   Weight: 0.708714\n",
      "Feature: TDL_INV_LEMMA:BETWEEN-MENTION-and-MENTION[bruk]                                           Weight: 0.659242\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[actress daughter]                                               Weight: 0.615915\n",
      "Feature: TDL_LEMMA:PARENTS-OF-BETWEEN-MENTION-and-MENTION[that lynxx leader]                       Weight: 0.573827\n",
      "Feature: TDL_INV_LEMMA:RIGHT-OF-MENTION[von before]                                                Weight: 0.567666\n",
      "Feature: TDL_INV_LEMMA:RIGHT-OF-MENTION[f.]                                                        Weight: 0.539246\n",
      "Feature: TDL_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[ROOT|disagree nsubj|parent prep|with]     Weight: 0.514130\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[wife]                                                           Weight: 0.509419\n",
      "Feature: TDL_INV_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[ROOT|bruk]                            Weight: 0.507583\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[child mental health]                                                Weight: 0.490069\n",
      "Feature: TDL_LEMMA:PARENTS-OF-BETWEEN-MENTION-and-MENTION[purchase]                                Weight: 0.480377\n",
      "Feature: TDL_LEMMA:RIGHT-OF-MENTION[and automobile]                                                Weight: 0.480360\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN['s father ,]                                                    Weight: 0.478335\n",
      "Feature: TDL_LEMMA:PARENTS-OF-BETWEEN-MENTION-and-MENTION[lynxx leader]                            Weight: 0.472351\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[kyle enjoy]                                                         Weight: 0.471136\n",
      "Feature: TDL_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[conj|husband appos|richard conj|mew]      Weight: 0.466361\n",
      "Feature: TDL_INV_LEMMA:PARENTS-OF-BETWEEN-MENTION-and-MENTION[as company]                          Weight: 0.463228\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[georgetown]                                                         Weight: 0.461206\n",
      "Feature: TDL_INV_LEMMA:RIGHT-OF-MENTION[journal entertainer]                                       Weight: 0.458545\n",
      "Feature: TDL_INV_LEMMA:PARENTS-OF-BETWEEN-MENTION-and-MENTION[None underpin]                       Weight: 0.458375\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[-PRON- husband]                                                 Weight: 0.453904\n",
      "Feature: TDL_INV_LEMMA:BETWEEN-MENTION-and-MENTION[wife]                                           Weight: 0.453309\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[a bonnet with]                                                      Weight: 0.452367\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[be photograph in]                                               Weight: 0.450792\n",
      "Feature: TDL_LEMMA:RIGHT-OF-MENTION[queen]                                                         Weight: 0.450740\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[pull -]                                                             Weight: 0.444896\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[both burst into]                                                Weight: 0.440322\n",
      "Feature: TDL_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[appos|gwandu conj|kuru conj|tell]         Weight: 0.438116\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[pregnant wife]                                                      Weight: 0.436116\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[wife]                                                               Weight: 0.427990\n",
      "Feature: TDL_DEP_LABEL|LEMMA:BETWEEN-MENTION-and-MENTION[conj|sudekeis]                            Weight: 0.427793\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[as -PRON- show]                                                 Weight: 0.427255\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[role  mainly]                                                   Weight: 0.427139\n",
      "Feature: TDL_INV_LEMMA:BETWEEN-MENTION-and-MENTION[husband]                                        Weight: 0.426681\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[blonde hair sweep]                                              Weight: 0.425656\n",
      "Feature: TDL_LEMMA:SEQ-BETWEEN[lax airport+]                                                       Weight: 0.425311\n",
      "Feature: TDL_INV_LEMMA:LEFT-OF-MENTION[attend]                                                     Weight: 0.424937\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[on wife]                                                        Weight: 0.424616\n",
      "Feature: TDL_INV_LEMMA:SEQ-BETWEEN[- divorce apostolic]                                            Weight: 0.421861\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = list(reversed(np.argsort(np.abs(w))[-50:]))\n",
    "\n",
    "pos_ftrs = [(F_train.get_key(session, i).name, w[i]) for i in largest_idxs if w[i] > 0.0]\n",
    "neg_ftrs = [(F_train.get_key(session, i).name, w[i]) for i in largest_idxs if w[i] <= 0.0]\n",
    "\n",
    "pos_ftrs = sorted(pos_ftrs, key=lambda x:x[-1], reverse=1)\n",
    "neg_ftrs = sorted(neg_ftrs, key=lambda x:x[-1], reverse=1)\n",
    "\n",
    "for ftr in pos_ftrs:\n",
    "    print('Feature: {0: <90}Weight: {1:.6f}'.format(*ftr))\n",
    "print \"-\" * 90\n",
    "for ftr in neg_ftrs:\n",
    "    print('Feature: {0: <90}Weight: {1:.6f}'.format(*ftr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
