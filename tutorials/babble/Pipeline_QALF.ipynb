{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'domain': 'spouse',\n",
    "    'supervision': 'majority',\n",
    "#     'learn_deps': False,\n",
    "#     'gen_model_search_space': 1,\n",
    "#     'gen_init_params': {\n",
    "#         'lf_propensity'         : True,\n",
    "#         'class_prior'           : True,\n",
    "#         'lf_class_propensity'   : True,\n",
    "#         'seed'                  : None,\n",
    "#     },\n",
    "#     'gen_params_default': {\n",
    "#         'step_size'     : 0.0001,\n",
    "#         'decay'         : 0.90,\n",
    "#         'reg_param'     : 0.50,\n",
    "#         'epochs'        : 25,\n",
    "#     },    \n",
    "    'disc_model_class': 'logreg',    \n",
    "    'disc_model_search_space': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$SNORKELDB = sqlite:///babble_spouse.db\n"
     ]
    }
   ],
   "source": [
    "# Get DB connection string and add to globals\n",
    "# NOTE: $SNORKELDB must be set before any snorkel imports\n",
    "import os\n",
    "\n",
    "default_db_name = 'babble_' + config['domain'] + ('_debug' if config.get('debug', False) else '')\n",
    "DB_NAME = config.get('db_name', default_db_name)\n",
    "if 'postgres' in config and config['postgres']:\n",
    "    DB_TYPE = 'postgres'\n",
    "else:\n",
    "    DB_TYPE = 'sqlite'\n",
    "    DB_NAME += '.db'\n",
    "DB_ADDR = \"localhost:{0}\".format(config['db_port']) if 'db_port' in config else \"\"\n",
    "os.environ['SNORKELDB'] = '{0}://{1}/{2}'.format(DB_TYPE, DB_ADDR, DB_NAME)\n",
    "print(\"$SNORKELDB = {0}\".format(os.environ['SNORKELDB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting domain=None to domain=spouse\n",
      "Overwriting print_freq=1 to print_freq=5\n",
      "Overwriting LF_acc_prior_weight_default=1.0 to LF_acc_prior_weight_default=0.5\n",
      "Overwriting decay=0.95 to decay=0.99\n",
      "Overwriting init_class_prior=0 to init_class_prior=-1.15\n",
      "Overwriting reg_param=0.1 to reg_param=0.5\n",
      "Overwriting disc_model_class=lstm to disc_model_class=logreg\n",
      "Overwriting supervision=generative to supervision=majority\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Resolve config conflicts (nb_config > local_config > global_config)\n",
    "from snorkel.contrib.babble.pipelines import merge_configs, get_local_pipeline\n",
    "config = merge_configs(config)\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "candidate_class = candidate_subclass(config['candidate_name'], config['candidate_entities'])\n",
    "\n",
    "pipeline = get_local_pipeline(config['domain'])\n",
    "pipe = pipeline(session, candidate_class, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.load_gold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time pipe.featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time pipe.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time pipe.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from tutorials.babble.spouse.spouse_qalf import QalfConverter\n",
    "\n",
    "qc = QalfConverter(session, pipe.candidate_class)\n",
    "matrix_path = (os.environ['SNORKELHOME'] + \n",
    "    '/tutorials/babble/spouse/data/qalf_matrix_hp.tsv')\n",
    "stats_path = (os.environ['SNORKELHOME'] + \n",
    "    '/tutorials/babble/spouse/data/qalf_stats_hp.tsv')\n",
    "L_train, L_dev, L_test = qc.convert(matrix_path, stats_path)\n",
    "\n",
    "pipe.L_train = L_train\n",
    "pipe.L_dev = L_dev\n",
    "pipe.L_test = L_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "# L_gold_train = load_gold_labels(session, annotator_name='gold', split=0)\n",
    "L_gold_dev   = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test  = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1_married_to?</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2_married_to?</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_got_married_to_ENT1_at_their_wedding?</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013948</td>\n",
       "      <td>0.012160</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_got_married_to_ENT2_at_their_wedding?</th>\n",
       "      <td>3</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_husband?</th>\n",
       "      <td>4</td>\n",
       "      <td>0.047210</td>\n",
       "      <td>0.044707</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>58</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.439394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_husband?</th>\n",
       "      <td>5</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_wife?</th>\n",
       "      <td>6</td>\n",
       "      <td>0.053648</td>\n",
       "      <td>0.047568</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>57</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_wife?</th>\n",
       "      <td>7</td>\n",
       "      <td>0.015379</td>\n",
       "      <td>0.013948</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.162791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_else_is_the_parent_of_a_child_with_ENT1?</th>\n",
       "      <td>8</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_else_is_the_parent_of_a_child_with_ENT2?</th>\n",
       "      <td>9</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_brother?</th>\n",
       "      <td>10</td>\n",
       "      <td>0.029328</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.015379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_brother?</th>\n",
       "      <td>11</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>0.015379</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_father?</th>\n",
       "      <td>12</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.017167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_father?</th>\n",
       "      <td>13</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_son?</th>\n",
       "      <td>14</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.024320</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_son?</th>\n",
       "      <td>15</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_boss?</th>\n",
       "      <td>16</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.015379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_boss?</th>\n",
       "      <td>17</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT1's_employee?</th>\n",
       "      <td>18</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>0.016810</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>0.893939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_is_ENT2's_employee?</th>\n",
       "      <td>19</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.013948</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_does_ENT1_work_with?</th>\n",
       "      <td>20</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.017883</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>0.893939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_does_ENT2_work_with?</th>\n",
       "      <td>21</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_works_at_the_same_company_as_ENT1?</th>\n",
       "      <td>22</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Who_works_at_the_same_company_as_ENT2?</th>\n",
       "      <td>23</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               j  Coverage  Overlaps  \\\n",
       "Who_is_ENT1_married_to?                        0  0.033262  0.030401   \n",
       "Who_is_ENT2_married_to?                        1  0.009299  0.008584   \n",
       "Who_got_married_to_ENT1_at_their_wedding?      2  0.013948  0.012160   \n",
       "Who_got_married_to_ENT2_at_their_wedding?      3  0.008941  0.006438   \n",
       "Who_is_ENT1's_husband?                         4  0.047210  0.044707   \n",
       "Who_is_ENT2's_husband?                         5  0.012518  0.012518   \n",
       "Who_is_ENT1's_wife?                            6  0.053648  0.047568   \n",
       "Who_is_ENT2's_wife?                            7  0.015379  0.013948   \n",
       "Who_else_is_the_parent_of_a_child_with_ENT1?   8  0.008941  0.008584   \n",
       "Who_else_is_the_parent_of_a_child_with_ENT2?   9  0.001788  0.001788   \n",
       "Who_is_ENT1's_brother?                        10  0.029328  0.028255   \n",
       "Who_is_ENT2's_brother?                        11  0.015737  0.015379   \n",
       "Who_is_ENT1's_father?                         12  0.028612  0.028255   \n",
       "Who_is_ENT2's_father?                         13  0.013591  0.013591   \n",
       "Who_is_ENT1's_son?                            14  0.025036  0.024320   \n",
       "Who_is_ENT2's_son?                            15  0.011445  0.011445   \n",
       "Who_is_ENT1's_boss?                           16  0.021459  0.020386   \n",
       "Who_is_ENT2's_boss?                           17  0.015021  0.014306   \n",
       "Who_is_ENT1's_employee?                       18  0.023605  0.022532   \n",
       "Who_is_ENT2's_employee?                       19  0.014664  0.013948   \n",
       "Who_does_ENT1_work_with?                      20  0.023605  0.017883   \n",
       "Who_does_ENT2_work_with?                      21  0.007511  0.007511   \n",
       "Who_works_at_the_same_company_as_ENT1?        22  0.007153  0.005007   \n",
       "Who_works_at_the_same_company_as_ENT2?        23  0.005365  0.004292   \n",
       "\n",
       "                                              Conflicts  TP  FP  FN  TN  \\\n",
       "Who_is_ENT1_married_to?                        0.017525  35  58   0   0   \n",
       "Who_is_ENT2_married_to?                        0.007153   4  22   0   0   \n",
       "Who_got_married_to_ENT1_at_their_wedding?      0.006795  17  22   0   0   \n",
       "Who_got_married_to_ENT2_at_their_wedding?      0.003934   9  16   0   0   \n",
       "Who_is_ENT1's_husband?                         0.023247  58  74   0   0   \n",
       "Who_is_ENT2's_husband?                         0.008941   7  28   0   0   \n",
       "Who_is_ENT1's_wife?                            0.025036  57  93   0   0   \n",
       "Who_is_ENT2's_wife?                            0.009299   7  36   0   0   \n",
       "Who_else_is_the_parent_of_a_child_with_ENT1?   0.005722   8  17   0   0   \n",
       "Who_else_is_the_parent_of_a_child_with_ENT2?   0.001431   2   3   0   0   \n",
       "Who_is_ENT1's_brother?                         0.015379   0   0   3  79   \n",
       "Who_is_ENT2's_brother?                         0.007868   0   0   1  43   \n",
       "Who_is_ENT1's_father?                          0.017167   0   0   5  75   \n",
       "Who_is_ENT2's_father?                          0.007868   0   0   1  37   \n",
       "Who_is_ENT1's_son?                             0.015021   0   0   4  66   \n",
       "Who_is_ENT2's_son?                             0.006438   0   0   1  31   \n",
       "Who_is_ENT1's_boss?                            0.015379   0   0   5  55   \n",
       "Who_is_ENT2's_boss?                            0.010014   0   0   1  41   \n",
       "Who_is_ENT1's_employee?                        0.016810   0   0   7  59   \n",
       "Who_is_ENT2's_employee?                        0.009657   0   0   1  40   \n",
       "Who_does_ENT1_work_with?                       0.014664   0   0   7  59   \n",
       "Who_does_ENT2_work_with?                       0.004649   0   0   1  20   \n",
       "Who_works_at_the_same_company_as_ENT1?         0.002861   0   0   0  20   \n",
       "Who_works_at_the_same_company_as_ENT2?         0.002504   0   0   0  15   \n",
       "\n",
       "                                              Empirical Acc.  \n",
       "Who_is_ENT1_married_to?                             0.376344  \n",
       "Who_is_ENT2_married_to?                             0.153846  \n",
       "Who_got_married_to_ENT1_at_their_wedding?           0.435897  \n",
       "Who_got_married_to_ENT2_at_their_wedding?           0.360000  \n",
       "Who_is_ENT1's_husband?                              0.439394  \n",
       "Who_is_ENT2's_husband?                              0.200000  \n",
       "Who_is_ENT1's_wife?                                 0.380000  \n",
       "Who_is_ENT2's_wife?                                 0.162791  \n",
       "Who_else_is_the_parent_of_a_child_with_ENT1?        0.320000  \n",
       "Who_else_is_the_parent_of_a_child_with_ENT2?        0.400000  \n",
       "Who_is_ENT1's_brother?                              0.963415  \n",
       "Who_is_ENT2's_brother?                              0.977273  \n",
       "Who_is_ENT1's_father?                               0.937500  \n",
       "Who_is_ENT2's_father?                               0.973684  \n",
       "Who_is_ENT1's_son?                                  0.942857  \n",
       "Who_is_ENT2's_son?                                  0.968750  \n",
       "Who_is_ENT1's_boss?                                 0.916667  \n",
       "Who_is_ENT2's_boss?                                 0.976190  \n",
       "Who_is_ENT1's_employee?                             0.893939  \n",
       "Who_is_ENT2's_employee?                             0.975610  \n",
       "Who_does_ENT1_work_with?                            0.893939  \n",
       "Who_does_ENT2_work_with?                            0.952381  \n",
       "Who_works_at_the_same_company_as_ENT1?              1.000000  \n",
       "Who_works_at_the_same_company_as_ENT2?              1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L_train.lf_stats(session, labels=L_gold_train)\n",
    "L_dev.lf_stats(session, labels=L_gold_dev)\n",
    "# L_test.lf_stats(session, labels=L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L_train: <22195x24 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 9463 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_train: <22195x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 22195 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 7.0%\n",
      "\n",
      "Using L_dev: <2796x24 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 1250 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_dev: <2796x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 2796 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 7.0%\n",
      "\n",
      "Using L_test: <2697x24 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 1198 stored elements in Compressed Sparse Row format>\n",
      "Using L_gold_test: <2697x1 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 2697 stored elements in Compressed Sparse Row format>\n",
      "Positive Fraction: 8.3%\n",
      "\n",
      "Saved 22195 marginals\n",
      "CPU times: user 2.03 s, sys: 78.9 ms, total: 2.11 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.supervise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22195,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngPYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2e9a21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [7.1] Begin training discriminative model\n",
      "============================================================\n",
      "[1] Testing dim = 64, dropout = 2.50e-01, batch_size = 64, n_epochs = 50, lr = 1.00e-03, rebalance = 2.50e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5388  #epochs=50  batch size=64\n",
      "[SparseLogisticRegression] Epoch 0 (0.58s)\tAverage loss=0.628199\tDev F1=14.48\n",
      "[SparseLogisticRegression] Epoch 5 (3.44s)\tAverage loss=0.213911\tDev F1=39.41\n",
      "[SparseLogisticRegression] Epoch 10 (6.40s)\tAverage loss=0.128316\tDev F1=43.09\n",
      "[SparseLogisticRegression] Epoch 15 (9.32s)\tAverage loss=0.089512\tDev F1=45.45\n",
      "[SparseLogisticRegression] Epoch 20 (12.21s)\tAverage loss=0.066939\tDev F1=46.27\n",
      "[SparseLogisticRegression] Epoch 25 (15.25s)\tAverage loss=0.051900\tDev F1=45.84\n",
      "[SparseLogisticRegression] Epoch 30 (18.21s)\tAverage loss=0.042390\tDev F1=46.00\n",
      "[SparseLogisticRegression] Epoch 35 (21.29s)\tAverage loss=0.033474\tDev F1=45.66\n",
      "[SparseLogisticRegression] Epoch 40 (24.24s)\tAverage loss=0.027547\tDev F1=45.43\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 45 (27.60s)\tAverage loss=0.022684\tDev F1=44.88\n",
      "[SparseLogisticRegression] Epoch 49 (29.96s)\tAverage loss=0.019801\tDev F1=44.77\n",
      "[SparseLogisticRegression] Training done (30.11s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.454320987654\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_0>\n",
      "============================================================\n",
      "[2] Testing dim = 128, dropout = 5.00e-01, batch_size = 16, n_epochs = 100, lr = 1.00e-03, rebalance = 2.50e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5388  #epochs=100  batch size=16\n",
      "[SparseLogisticRegression] Epoch 0 (1.04s)\tAverage loss=0.559972\tDev F1=28.30\n",
      "[SparseLogisticRegression] Epoch 5 (6.89s)\tAverage loss=0.124408\tDev F1=45.24\n",
      "[SparseLogisticRegression] Epoch 10 (12.60s)\tAverage loss=0.063676\tDev F1=46.19\n",
      "[SparseLogisticRegression] Epoch 15 (18.24s)\tAverage loss=0.037204\tDev F1=46.34\n",
      "[SparseLogisticRegression] Epoch 20 (23.81s)\tAverage loss=0.023072\tDev F1=46.57\n",
      "[SparseLogisticRegression] Epoch 25 (29.57s)\tAverage loss=0.014742\tDev F1=45.48\n",
      "[SparseLogisticRegression] Epoch 30 (35.65s)\tAverage loss=0.009617\tDev F1=45.21\n",
      "[SparseLogisticRegression] Epoch 35 (41.31s)\tAverage loss=0.006422\tDev F1=44.29\n",
      "[SparseLogisticRegression] Epoch 40 (46.89s)\tAverage loss=0.004359\tDev F1=42.70\n",
      "[SparseLogisticRegression] Epoch 45 (52.41s)\tAverage loss=0.003012\tDev F1=41.79\n",
      "[SparseLogisticRegression] Epoch 50 (58.10s)\tAverage loss=0.002070\tDev F1=41.95\n",
      "[SparseLogisticRegression] Epoch 55 (63.62s)\tAverage loss=0.001437\tDev F1=41.42\n",
      "[SparseLogisticRegression] Epoch 60 (69.20s)\tAverage loss=0.001031\tDev F1=41.07\n",
      "[SparseLogisticRegression] Epoch 65 (74.75s)\tAverage loss=0.000731\tDev F1=40.49\n",
      "[SparseLogisticRegression] Epoch 70 (80.62s)\tAverage loss=0.000524\tDev F1=40.00\n",
      "[SparseLogisticRegression] Epoch 75 (86.15s)\tAverage loss=0.000389\tDev F1=39.76\n",
      "[SparseLogisticRegression] Epoch 80 (91.92s)\tAverage loss=0.000280\tDev F1=39.04\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 85 (97.94s)\tAverage loss=0.000210\tDev F1=37.84\n",
      "[SparseLogisticRegression] Epoch 90 (103.73s)\tAverage loss=0.000156\tDev F1=37.86\n",
      "[SparseLogisticRegression] Epoch 95 (109.32s)\tAverage loss=0.000115\tDev F1=37.74\n",
      "[SparseLogisticRegression] Epoch 99 (113.76s)\tAverage loss=0.000092\tDev F1=37.59\n",
      "[SparseLogisticRegression] Training done (113.98s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.390438247012\n",
      "============================================================\n",
      "[3] Testing dim = 128, dropout = 1.00e-01, batch_size = 32, n_epochs = 25, lr = 1.00e-02, rebalance = 5.00e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=2694  #epochs=25  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (0.35s)\tAverage loss=0.561693\tDev F1=39.27\n",
      "[SparseLogisticRegression] Epoch 5 (2.26s)\tAverage loss=0.030576\tDev F1=34.33\n",
      "[SparseLogisticRegression] Epoch 10 (4.18s)\tAverage loss=0.012314\tDev F1=33.60\n",
      "[SparseLogisticRegression] Epoch 15 (6.08s)\tAverage loss=0.007201\tDev F1=34.01\n",
      "[SparseLogisticRegression] Epoch 20 (8.02s)\tAverage loss=0.003539\tDev F1=33.55\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 24 (10.06s)\tAverage loss=0.002602\tDev F1=33.77\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (10.72s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.337696335079\n",
      "============================================================\n",
      "[4] Testing dim = 64, dropout = 5.00e-01, batch_size = 32, n_epochs = 100, lr = 1.00e-04, rebalance = 5.00e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=2694  #epochs=100  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (0.35s)\tAverage loss=0.769314\tDev F1=13.12\n",
      "[SparseLogisticRegression] Epoch 5 (2.23s)\tAverage loss=0.643277\tDev F1=15.14\n",
      "[SparseLogisticRegression] Epoch 10 (4.15s)\tAverage loss=0.546323\tDev F1=17.01\n",
      "[SparseLogisticRegression] Epoch 15 (6.08s)\tAverage loss=0.473980\tDev F1=18.30\n",
      "[SparseLogisticRegression] Epoch 20 (8.01s)\tAverage loss=0.415480\tDev F1=19.22\n",
      "[SparseLogisticRegression] Epoch 25 (10.00s)\tAverage loss=0.370180\tDev F1=20.59\n",
      "[SparseLogisticRegression] Epoch 30 (12.04s)\tAverage loss=0.332565\tDev F1=21.46\n",
      "[SparseLogisticRegression] Epoch 35 (13.96s)\tAverage loss=0.299313\tDev F1=21.92\n",
      "[SparseLogisticRegression] Epoch 40 (15.86s)\tAverage loss=0.271941\tDev F1=22.79\n",
      "[SparseLogisticRegression] Epoch 45 (17.76s)\tAverage loss=0.249138\tDev F1=23.18\n",
      "[SparseLogisticRegression] Epoch 50 (19.65s)\tAverage loss=0.227322\tDev F1=23.98\n",
      "[SparseLogisticRegression] Epoch 55 (21.55s)\tAverage loss=0.210955\tDev F1=25.38\n",
      "[SparseLogisticRegression] Epoch 60 (23.43s)\tAverage loss=0.197310\tDev F1=25.56\n",
      "[SparseLogisticRegression] Epoch 65 (25.32s)\tAverage loss=0.182217\tDev F1=26.13\n",
      "[SparseLogisticRegression] Epoch 70 (27.22s)\tAverage loss=0.168000\tDev F1=26.87\n",
      "[SparseLogisticRegression] Epoch 75 (29.16s)\tAverage loss=0.159892\tDev F1=27.74\n",
      "[SparseLogisticRegression] Epoch 80 (31.09s)\tAverage loss=0.146695\tDev F1=28.22\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 85 (33.45s)\tAverage loss=0.138396\tDev F1=28.72\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 90 (35.75s)\tAverage loss=0.129275\tDev F1=29.31\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 95 (38.24s)\tAverage loss=0.121352\tDev F1=29.80\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 99 (40.33s)\tAverage loss=0.116237\tDev F1=30.26\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (40.88s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.302631578947\n",
      "============================================================\n",
      "[5] Testing dim = 128, dropout = 1.00e-01, batch_size = 32, n_epochs = 25, lr = 1.00e-02, rebalance = 0.00e+00\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=22195  #epochs=25  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (3.38s)\tAverage loss=0.252906\tDev F1=39.76\n",
      "[SparseLogisticRegression] Epoch 5 (18.88s)\tAverage loss=0.033783\tDev F1=35.14\n",
      "[SparseLogisticRegression] Epoch 10 (35.64s)\tAverage loss=0.024650\tDev F1=36.21\n",
      "[SparseLogisticRegression] Epoch 15 (52.02s)\tAverage loss=0.023689\tDev F1=35.19\n",
      "[SparseLogisticRegression] Epoch 20 (68.42s)\tAverage loss=0.021734\tDev F1=34.42\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 24 (81.53s)\tAverage loss=0.010953\tDev F1=35.46\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (82.05s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.354570637119\n",
      "============================================================\n",
      "[6] Testing dim = 128, dropout = 1.00e-01, batch_size = 32, n_epochs = 50, lr = 1.00e-04, rebalance = 2.50e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5388  #epochs=50  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (0.75s)\tAverage loss=0.715098\tDev F1=9.60\n",
      "[SparseLogisticRegression] Epoch 5 (4.59s)\tAverage loss=0.507592\tDev F1=12.74\n",
      "[SparseLogisticRegression] Epoch 10 (8.46s)\tAverage loss=0.409562\tDev F1=21.12\n",
      "[SparseLogisticRegression] Epoch 15 (12.32s)\tAverage loss=0.340123\tDev F1=29.88\n",
      "[SparseLogisticRegression] Epoch 20 (16.16s)\tAverage loss=0.288058\tDev F1=36.80\n",
      "[SparseLogisticRegression] Epoch 25 (19.97s)\tAverage loss=0.249005\tDev F1=40.83\n",
      "[SparseLogisticRegression] Epoch 30 (23.93s)\tAverage loss=0.218392\tDev F1=44.89\n",
      "[SparseLogisticRegression] Epoch 35 (27.98s)\tAverage loss=0.192871\tDev F1=43.89\n",
      "[SparseLogisticRegression] Epoch 40 (31.93s)\tAverage loss=0.172321\tDev F1=45.30\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 45 (36.28s)\tAverage loss=0.155352\tDev F1=45.28\n",
      "[SparseLogisticRegression] Epoch 49 (39.44s)\tAverage loss=0.144153\tDev F1=45.16\n",
      "[SparseLogisticRegression] Training done (39.61s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.453038674033\n",
      "============================================================\n",
      "[7] Testing dim = 128, dropout = 1.00e-01, batch_size = 16, n_epochs = 50, lr = 1.00e-04, rebalance = 2.50e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5388  #epochs=50  batch size=16\n",
      "[SparseLogisticRegression] Epoch 0 (1.07s)\tAverage loss=0.744492\tDev F1=14.06\n",
      "[SparseLogisticRegression] Epoch 5 (6.67s)\tAverage loss=0.456462\tDev F1=23.98\n",
      "[SparseLogisticRegression] Epoch 10 (12.04s)\tAverage loss=0.344488\tDev F1=34.70\n",
      "[SparseLogisticRegression] Epoch 15 (17.71s)\tAverage loss=0.271575\tDev F1=42.39\n",
      "[SparseLogisticRegression] Epoch 20 (23.39s)\tAverage loss=0.221451\tDev F1=45.22\n",
      "[SparseLogisticRegression] Epoch 25 (28.91s)\tAverage loss=0.185608\tDev F1=45.81\n",
      "[SparseLogisticRegression] Epoch 30 (34.37s)\tAverage loss=0.158911\tDev F1=47.12\n",
      "[SparseLogisticRegression] Epoch 35 (39.84s)\tAverage loss=0.137996\tDev F1=46.49\n",
      "[SparseLogisticRegression] Epoch 40 (45.42s)\tAverage loss=0.121458\tDev F1=45.45\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 45 (51.61s)\tAverage loss=0.108007\tDev F1=45.91\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 49 (56.60s)\tAverage loss=0.098944\tDev F1=45.91\n",
      "[SparseLogisticRegression] Training done (56.82s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.459102902375\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression_6>\n",
      "============================================================\n",
      "[8] Testing dim = 128, dropout = 5.00e-01, batch_size = 32, n_epochs = 100, lr = 1.00e-02, rebalance = 5.00e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=2694  #epochs=100  batch size=32\n",
      "[SparseLogisticRegression] Epoch 0 (0.38s)\tAverage loss=0.599795\tDev F1=37.66\n",
      "[SparseLogisticRegression] Epoch 5 (2.50s)\tAverage loss=0.030574\tDev F1=34.73\n",
      "[SparseLogisticRegression] Epoch 10 (4.67s)\tAverage loss=0.012462\tDev F1=33.51\n",
      "[SparseLogisticRegression] Epoch 15 (6.90s)\tAverage loss=0.007113\tDev F1=34.41\n",
      "[SparseLogisticRegression] Epoch 20 (9.02s)\tAverage loss=0.003621\tDev F1=33.69\n",
      "[SparseLogisticRegression] Epoch 25 (11.10s)\tAverage loss=0.002470\tDev F1=33.73\n",
      "[SparseLogisticRegression] Epoch 30 (13.16s)\tAverage loss=0.001788\tDev F1=33.11\n",
      "[SparseLogisticRegression] Epoch 35 (15.27s)\tAverage loss=0.001307\tDev F1=32.90\n",
      "[SparseLogisticRegression] Epoch 40 (17.40s)\tAverage loss=0.000986\tDev F1=31.03\n",
      "[SparseLogisticRegression] Epoch 45 (19.49s)\tAverage loss=0.000738\tDev F1=31.41\n",
      "[SparseLogisticRegression] Epoch 50 (21.63s)\tAverage loss=0.000569\tDev F1=31.68\n",
      "[SparseLogisticRegression] Epoch 55 (23.98s)\tAverage loss=0.000447\tDev F1=31.73\n",
      "[SparseLogisticRegression] Epoch 60 (26.26s)\tAverage loss=0.000361\tDev F1=31.72\n",
      "[SparseLogisticRegression] Epoch 65 (28.51s)\tAverage loss=0.000283\tDev F1=31.67\n",
      "[SparseLogisticRegression] Epoch 70 (30.67s)\tAverage loss=0.000222\tDev F1=31.68\n",
      "[SparseLogisticRegression] Epoch 75 (32.71s)\tAverage loss=0.000184\tDev F1=31.67\n",
      "[SparseLogisticRegression] Epoch 80 (34.83s)\tAverage loss=0.000143\tDev F1=31.76\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 85 (37.25s)\tAverage loss=0.000118\tDev F1=31.64\n",
      "[SparseLogisticRegression] Epoch 90 (39.34s)\tAverage loss=0.000094\tDev F1=31.72\n",
      "[SparseLogisticRegression] Epoch 95 (41.42s)\tAverage loss=0.000076\tDev F1=31.77\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 99 (43.56s)\tAverage loss=0.000064\tDev F1=31.62\n",
      "[SparseLogisticRegression] Training done (43.73s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.317733990148\n",
      "============================================================\n",
      "[9] Testing dim = 128, dropout = 2.50e-01, batch_size = 16, n_epochs = 25, lr = 1.00e-04, rebalance = 2.50e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=5388  #epochs=25  batch size=16\n",
      "[SparseLogisticRegression] Epoch 0 (1.05s)\tAverage loss=0.767132\tDev F1=9.15\n",
      "[SparseLogisticRegression] Epoch 5 (6.59s)\tAverage loss=0.476730\tDev F1=13.73\n",
      "[SparseLogisticRegression] Epoch 10 (12.21s)\tAverage loss=0.357632\tDev F1=28.06\n",
      "[SparseLogisticRegression] Epoch 15 (17.86s)\tAverage loss=0.280579\tDev F1=35.88\n",
      "[SparseLogisticRegression] Epoch 20 (23.23s)\tAverage loss=0.228211\tDev F1=40.78\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 24 (28.27s)\tAverage loss=0.197327\tDev F1=41.64\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Training done (28.80s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.416438356164\n",
      "============================================================\n",
      "[10] Testing dim = 128, dropout = 5.00e-01, batch_size = 16, n_epochs = 50, lr = 1.00e-02, rebalance = 5.00e-01\n",
      "============================================================\n",
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=2694  #epochs=50  batch size=16\n",
      "[SparseLogisticRegression] Epoch 0 (0.52s)\tAverage loss=0.577269\tDev F1=39.00\n",
      "[SparseLogisticRegression] Epoch 5 (3.30s)\tAverage loss=0.023920\tDev F1=34.69\n",
      "[SparseLogisticRegression] Epoch 10 (6.09s)\tAverage loss=0.007342\tDev F1=33.82\n",
      "[SparseLogisticRegression] Epoch 15 (8.86s)\tAverage loss=0.008650\tDev F1=35.38\n",
      "[SparseLogisticRegression] Epoch 20 (11.80s)\tAverage loss=0.004495\tDev F1=35.20\n",
      "[SparseLogisticRegression] Epoch 25 (14.62s)\tAverage loss=0.001111\tDev F1=33.73\n",
      "[SparseLogisticRegression] Epoch 30 (17.54s)\tAverage loss=0.002329\tDev F1=31.03\n",
      "[SparseLogisticRegression] Epoch 35 (20.48s)\tAverage loss=0.000397\tDev F1=28.92\n",
      "[SparseLogisticRegression] Epoch 40 (23.46s)\tAverage loss=0.000256\tDev F1=29.08\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 45 (26.60s)\tAverage loss=0.000169\tDev F1=29.17\n",
      "[SparseLogisticRegression] Model saved as <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] Epoch 49 (29.71s)\tAverage loss=0.000122\tDev F1=29.17\n",
      "[SparseLogisticRegression] Training done (29.95s)\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression>\n",
      "[SparseLogisticRegression] F-1.0 Score: 0.291712707182\n",
      "[SparseLogisticRegression] Loaded model <SparseLogisticRegression_6>\n",
      "   dim  dropout  batch_size  n_epochs      lr  rebalance     Prec.      Rec.  \\\n",
      "6  128     0.10          16        50  0.0001       0.25  0.475410  0.443878   \n",
      "0   64     0.25          64        50  0.0010       0.25  0.440191  0.469388   \n",
      "5  128     0.10          32        50  0.0001       0.25  0.493976  0.418367   \n",
      "8  128     0.25          16        25  0.0001       0.25  0.449704  0.387755   \n",
      "1  128     0.50          16       100  0.0010       0.25  0.320261  0.500000   \n",
      "4  128     0.10          32        25  0.0100       0.00  0.387879  0.326531   \n",
      "2  128     0.10          32        25  0.0100       0.50  0.227113  0.658163   \n",
      "7  128     0.50          32       100  0.0100       0.50  0.209416  0.658163   \n",
      "3   64     0.50          32       100  0.0001       0.50  0.203901  0.586735   \n",
      "9  128     0.50          16        50  0.0100       0.50  0.186178  0.673469   \n",
      "\n",
      "      F-1.0  \n",
      "6  0.459103  \n",
      "0  0.454321  \n",
      "5  0.453039  \n",
      "8  0.416438  \n",
      "1  0.390438  \n",
      "4  0.354571  \n",
      "2  0.337696  \n",
      "7  0.317734  \n",
      "3  0.302632  \n",
      "9  0.291713  \n",
      "[SparseLogisticRegression] Model saved as <discriminative_spouse>\n",
      "### Done in 485.7s.\n",
      "\n",
      "### [7.2] Evaluate generative model (opt_b=0.5)\n",
      "### Done in 0.0s.\n",
      "\n",
      "### [7.3] Evaluate discriminative model (opt_b=0.5)\n",
      "### Done in 0.1s.\n",
      "\n",
      "      F1 Score  Precision    Recall\n",
      "Disc  0.470825   0.428571  0.522321\n",
      "Gen   0.444444   0.472362  0.419643\n",
      "CPU times: user 17min 5s, sys: 3min 38s, total: 20min 43s\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
